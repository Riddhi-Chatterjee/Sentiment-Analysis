{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Project (Harsh Comment Classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Team Name: Tech Knights\n",
    "\n",
    "Team Members: Surya Sastry (IMT2020079), Riddhi Chatterjee (IMT2020094) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Imports and analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"harsh-comment-classification/train.csv\")\n",
    "test_df = pd.read_csv(\"harsh-comment-classification/test.csv\")\n",
    "sample_df = pd.read_csv(\"harsh-comment-classification/sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>harsh</th>\n",
       "      <th>extremely_harsh</th>\n",
       "      <th>vulgar</th>\n",
       "      <th>threatening</th>\n",
       "      <th>disrespect</th>\n",
       "      <th>targeted_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a8be7c5d4527adbbf15f</td>\n",
       "      <td>\", 6 December 2007 (UTC)\\nI am interested, not...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0b7ca73f388222aad64d</td>\n",
       "      <td>I added about three missing parameters to temp...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>db934381501872ba6f38</td>\n",
       "      <td>SANDBOX?? \\n\\nI DID YOUR MADRE DID IN THE SANDBOX</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>228015c4a87c4b1f09a7</td>\n",
       "      <td>why good sir? Why? \\n\\nYou, sir, obviously do ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b18f26cfa1408b52e949</td>\n",
       "      <td>\"\\n\\n Source \\n\\nIncase I forget, or someone e...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6729341b01ab895388d7</td>\n",
       "      <td>\"\\n Neither of your arguments are persuasive. ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>a36cf2a3d3cf833492ec</td>\n",
       "      <td>I knew this was a left wing blog, and the abov...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5ce91d74cb358c22cf8c</td>\n",
       "      <td>How can you ignore truth?  You want to libel m...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>03bae982976f8795ae43</td>\n",
       "      <td>Regarding edits made during March 14 2007 (UTC...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2442ccd35237f00a9866</td>\n",
       "      <td>user Dr.K \\n\\nhello he is vandalising, edit wa...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>f554526e8552f6b0f654</td>\n",
       "      <td>Removed parts of the article that were not sou...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>e50ed1456e917a57a268</td>\n",
       "      <td>Common name\\nIt has come to my attention there...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>e249ee908e3d8b19a6ce</td>\n",
       "      <td>\"\\nBut BB2 was added first. All I am saying is...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>f1cd10252f7613418260</td>\n",
       "      <td>It has become quite clear that the Serialcomma...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>e285fc93378eb91f1b04</td>\n",
       "      <td>you know? \\n\\nI already finish the main temple...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>7146803c403215d83e40</td>\n",
       "      <td>\"\\nBy the way, it's not Chu nom, but Han tu. C...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>868839d434762cd75e3c</td>\n",
       "      <td>Wow!\\nThis bot is amazing! It's like futurama ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3b0a5f19f847ec86b0ce</td>\n",
       "      <td>Contesting my ban\\nI refuse to be banned by a ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>f234d37a5fa9bba8c626</td>\n",
       "      <td>Please leave The Page alone!Or I'll have to re...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>223ebedf0661f3db2bbf</td>\n",
       "      <td>\"Welcome!\\n\\nHello, , and welcome to Wikipedia...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>c9f62b3623cbc7c9a392</td>\n",
       "      <td>\"\\n\\nSpeedy deletion of Media Training Worldwi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                               text  \\\n",
       "0   a8be7c5d4527adbbf15f  \", 6 December 2007 (UTC)\\nI am interested, not...   \n",
       "1   0b7ca73f388222aad64d  I added about three missing parameters to temp...   \n",
       "2   db934381501872ba6f38  SANDBOX?? \\n\\nI DID YOUR MADRE DID IN THE SANDBOX   \n",
       "3   228015c4a87c4b1f09a7  why good sir? Why? \\n\\nYou, sir, obviously do ...   \n",
       "4   b18f26cfa1408b52e949  \"\\n\\n Source \\n\\nIncase I forget, or someone e...   \n",
       "5   6729341b01ab895388d7  \"\\n Neither of your arguments are persuasive. ...   \n",
       "6   a36cf2a3d3cf833492ec  I knew this was a left wing blog, and the abov...   \n",
       "7   5ce91d74cb358c22cf8c  How can you ignore truth?  You want to libel m...   \n",
       "8   03bae982976f8795ae43  Regarding edits made during March 14 2007 (UTC...   \n",
       "9   2442ccd35237f00a9866  user Dr.K \\n\\nhello he is vandalising, edit wa...   \n",
       "10  f554526e8552f6b0f654  Removed parts of the article that were not sou...   \n",
       "11  e50ed1456e917a57a268  Common name\\nIt has come to my attention there...   \n",
       "12  e249ee908e3d8b19a6ce  \"\\nBut BB2 was added first. All I am saying is...   \n",
       "13  f1cd10252f7613418260  It has become quite clear that the Serialcomma...   \n",
       "14  e285fc93378eb91f1b04  you know? \\n\\nI already finish the main temple...   \n",
       "15  7146803c403215d83e40  \"\\nBy the way, it's not Chu nom, but Han tu. C...   \n",
       "16  868839d434762cd75e3c  Wow!\\nThis bot is amazing! It's like futurama ...   \n",
       "17  3b0a5f19f847ec86b0ce  Contesting my ban\\nI refuse to be banned by a ...   \n",
       "18  f234d37a5fa9bba8c626  Please leave The Page alone!Or I'll have to re...   \n",
       "19  223ebedf0661f3db2bbf  \"Welcome!\\n\\nHello, , and welcome to Wikipedia...   \n",
       "20  c9f62b3623cbc7c9a392  \"\\n\\nSpeedy deletion of Media Training Worldwi...   \n",
       "\n",
       "    harsh  extremely_harsh  vulgar  threatening  disrespect  targeted_hate  \n",
       "0       0                0       0            0           0              0  \n",
       "1       0                0       0            0           0              0  \n",
       "2       1                0       0            0           0              0  \n",
       "3       1                0       1            1           1              0  \n",
       "4       0                0       0            0           0              0  \n",
       "5       0                0       0            0           0              0  \n",
       "6       0                0       0            0           0              0  \n",
       "7       0                0       0            0           0              0  \n",
       "8       0                0       0            0           0              0  \n",
       "9       0                0       0            0           0              0  \n",
       "10      0                0       0            0           0              0  \n",
       "11      0                0       0            0           0              0  \n",
       "12      0                0       0            0           0              0  \n",
       "13      0                0       0            0           0              0  \n",
       "14      1                0       0            0           0              0  \n",
       "15      0                0       0            0           0              0  \n",
       "16      0                0       0            0           0              0  \n",
       "17      0                0       0            0           0              0  \n",
       "18      0                0       0            0           0              0  \n",
       "19      0                0       0            0           0              0  \n",
       "20      0                0       0            0           0              0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.loc[0:20,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89359"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                 89359\n",
       "text               89359\n",
       "harsh                  2\n",
       "extremely_harsh        2\n",
       "vulgar                 2\n",
       "threatening            2\n",
       "disrespect             2\n",
       "targeted_hate          2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>harsh</th>\n",
       "      <th>extremely_harsh</th>\n",
       "      <th>vulgar</th>\n",
       "      <th>threatening</th>\n",
       "      <th>disrespect</th>\n",
       "      <th>targeted_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>89359.000000</td>\n",
       "      <td>89359.000000</td>\n",
       "      <td>89359.000000</td>\n",
       "      <td>89359.000000</td>\n",
       "      <td>89359.000000</td>\n",
       "      <td>89359.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.095782</td>\n",
       "      <td>0.010262</td>\n",
       "      <td>0.053067</td>\n",
       "      <td>0.002999</td>\n",
       "      <td>0.049150</td>\n",
       "      <td>0.008975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.294294</td>\n",
       "      <td>0.100781</td>\n",
       "      <td>0.224168</td>\n",
       "      <td>0.054683</td>\n",
       "      <td>0.216182</td>\n",
       "      <td>0.094311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              harsh  extremely_harsh        vulgar   threatening  \\\n",
       "count  89359.000000     89359.000000  89359.000000  89359.000000   \n",
       "mean       0.095782         0.010262      0.053067      0.002999   \n",
       "std        0.294294         0.100781      0.224168      0.054683   \n",
       "min        0.000000         0.000000      0.000000      0.000000   \n",
       "25%        0.000000         0.000000      0.000000      0.000000   \n",
       "50%        0.000000         0.000000      0.000000      0.000000   \n",
       "75%        0.000000         0.000000      0.000000      0.000000   \n",
       "max        1.000000         1.000000      1.000000      1.000000   \n",
       "\n",
       "         disrespect  targeted_hate  \n",
       "count  89359.000000   89359.000000  \n",
       "mean       0.049150       0.008975  \n",
       "std        0.216182       0.094311  \n",
       "min        0.000000       0.000000  \n",
       "25%        0.000000       0.000000  \n",
       "50%        0.000000       0.000000  \n",
       "75%        0.000000       0.000000  \n",
       "max        1.000000       1.000000  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the \"id\" column is useless. So lets drop the \"id\" column from train_df..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>harsh</th>\n",
       "      <th>extremely_harsh</th>\n",
       "      <th>vulgar</th>\n",
       "      <th>threatening</th>\n",
       "      <th>disrespect</th>\n",
       "      <th>targeted_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\", 6 December 2007 (UTC)\\nI am interested, not...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I added about three missing parameters to temp...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SANDBOX?? \\n\\nI DID YOUR MADRE DID IN THE SANDBOX</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>why good sir? Why? \\n\\nYou, sir, obviously do ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"\\n\\n Source \\n\\nIncase I forget, or someone e...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"\\n Neither of your arguments are persuasive. ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I knew this was a left wing blog, and the abov...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How can you ignore truth?  You want to libel m...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Regarding edits made during March 14 2007 (UTC...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>user Dr.K \\n\\nhello he is vandalising, edit wa...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Removed parts of the article that were not sou...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Common name\\nIt has come to my attention there...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>\"\\nBut BB2 was added first. All I am saying is...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>It has become quite clear that the Serialcomma...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>you know? \\n\\nI already finish the main temple...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>\"\\nBy the way, it's not Chu nom, but Han tu. C...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Wow!\\nThis bot is amazing! It's like futurama ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Contesting my ban\\nI refuse to be banned by a ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Please leave The Page alone!Or I'll have to re...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>\"Welcome!\\n\\nHello, , and welcome to Wikipedia...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>\"\\n\\nSpeedy deletion of Media Training Worldwi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  harsh  extremely_harsh  \\\n",
       "0   \", 6 December 2007 (UTC)\\nI am interested, not...      0                0   \n",
       "1   I added about three missing parameters to temp...      0                0   \n",
       "2   SANDBOX?? \\n\\nI DID YOUR MADRE DID IN THE SANDBOX      1                0   \n",
       "3   why good sir? Why? \\n\\nYou, sir, obviously do ...      1                0   \n",
       "4   \"\\n\\n Source \\n\\nIncase I forget, or someone e...      0                0   \n",
       "5   \"\\n Neither of your arguments are persuasive. ...      0                0   \n",
       "6   I knew this was a left wing blog, and the abov...      0                0   \n",
       "7   How can you ignore truth?  You want to libel m...      0                0   \n",
       "8   Regarding edits made during March 14 2007 (UTC...      0                0   \n",
       "9   user Dr.K \\n\\nhello he is vandalising, edit wa...      0                0   \n",
       "10  Removed parts of the article that were not sou...      0                0   \n",
       "11  Common name\\nIt has come to my attention there...      0                0   \n",
       "12  \"\\nBut BB2 was added first. All I am saying is...      0                0   \n",
       "13  It has become quite clear that the Serialcomma...      0                0   \n",
       "14  you know? \\n\\nI already finish the main temple...      1                0   \n",
       "15  \"\\nBy the way, it's not Chu nom, but Han tu. C...      0                0   \n",
       "16  Wow!\\nThis bot is amazing! It's like futurama ...      0                0   \n",
       "17  Contesting my ban\\nI refuse to be banned by a ...      0                0   \n",
       "18  Please leave The Page alone!Or I'll have to re...      0                0   \n",
       "19  \"Welcome!\\n\\nHello, , and welcome to Wikipedia...      0                0   \n",
       "20  \"\\n\\nSpeedy deletion of Media Training Worldwi...      0                0   \n",
       "\n",
       "    vulgar  threatening  disrespect  targeted_hate  \n",
       "0        0            0           0              0  \n",
       "1        0            0           0              0  \n",
       "2        0            0           0              0  \n",
       "3        1            1           1              0  \n",
       "4        0            0           0              0  \n",
       "5        0            0           0              0  \n",
       "6        0            0           0              0  \n",
       "7        0            0           0              0  \n",
       "8        0            0           0              0  \n",
       "9        0            0           0              0  \n",
       "10       0            0           0              0  \n",
       "11       0            0           0              0  \n",
       "12       0            0           0              0  \n",
       "13       0            0           0              0  \n",
       "14       0            0           0              0  \n",
       "15       0            0           0              0  \n",
       "16       0            0           0              0  \n",
       "17       0            0           0              0  \n",
       "18       0            0           0              0  \n",
       "19       0            0           0              0  \n",
       "20       0            0           0              0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.drop(axis=\"columns\", labels=\"id\", inplace=True)\n",
    "train_df.loc[0:20,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e0ae9d9474a5689a5791</td>\n",
       "      <td>in an interview before his execution</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b64a191301cad4f11287</td>\n",
       "      <td>He knew what he was doing. The below posts are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5e1953d9ae04bdc66408</td>\n",
       "      <td>Zzzzzzz... youre a real bore. Now go bore some...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23128f98196c8e8f7b90</td>\n",
       "      <td>\"\\n\\nYet, it remains confusion because the 910...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2d3f1254f71472bf2b78</td>\n",
       "      <td>I was referring to them losing interest in van...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>21f4f0f4812a08ea6c28</td>\n",
       "      <td>\", 5 March 2009 (UTC)\\n\\nThat wasn't an attack...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>733b43d534c67c1be948</td>\n",
       "      <td>1) You're not reading properly. I've asked you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>aad47a397f7ddc629d5d</td>\n",
       "      <td>\"\\nplease look at the discussion here and here...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>d19fcde8a3af2e472d74</td>\n",
       "      <td>2011 (UTC)\\nCall Of Duty has never made any cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7d4de482c60f1c8a79c6</td>\n",
       "      <td>You too man, take care.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>f81afe094bcd161ea6f8</td>\n",
       "      <td>\"\\n\\n  According to the world book encyclopedi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>132973e40fa63e0d07f1</td>\n",
       "      <td>I don't object to removing it as unsourced POV...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>37195759bf9104c3b488</td>\n",
       "      <td>In reply to the above modern stations are in f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0c46ec0e711469190af8</td>\n",
       "      <td>Why is fatima put as winner and anya as runner...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ecaeb00c751a61c1370e</td>\n",
       "      <td>I am sure that you are the SON of one of those...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>dfaa434123c910477026</td>\n",
       "      <td>\"\\nHow ironic, that one was one I handled. I'l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>f0e8517a1d9e1394696b</td>\n",
       "      <td>97.84.13.145 is right, it's not about whether ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>db952db7fb786093a8f7</td>\n",
       "      <td>Hmm, it sounds like we have a conflict between...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>132ad2e7621d9c132c3f</td>\n",
       "      <td>Regarding edits made during September 14 2006 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>38eef2c8994e8a980bb1</td>\n",
       "      <td>If you need help with resampling images, let m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>160a621ad6ee20736755</td>\n",
       "      <td>\"\\nAh, yes, here's the MastCell edit, through ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                               text\n",
       "0   e0ae9d9474a5689a5791               in an interview before his execution\n",
       "1   b64a191301cad4f11287  He knew what he was doing. The below posts are...\n",
       "2   5e1953d9ae04bdc66408  Zzzzzzz... youre a real bore. Now go bore some...\n",
       "3   23128f98196c8e8f7b90  \"\\n\\nYet, it remains confusion because the 910...\n",
       "4   2d3f1254f71472bf2b78  I was referring to them losing interest in van...\n",
       "5   21f4f0f4812a08ea6c28  \", 5 March 2009 (UTC)\\n\\nThat wasn't an attack...\n",
       "6   733b43d534c67c1be948  1) You're not reading properly. I've asked you...\n",
       "7   aad47a397f7ddc629d5d  \"\\nplease look at the discussion here and here...\n",
       "8   d19fcde8a3af2e472d74  2011 (UTC)\\nCall Of Duty has never made any cl...\n",
       "9   7d4de482c60f1c8a79c6                            You too man, take care.\n",
       "10  f81afe094bcd161ea6f8  \"\\n\\n  According to the world book encyclopedi...\n",
       "11  132973e40fa63e0d07f1  I don't object to removing it as unsourced POV...\n",
       "12  37195759bf9104c3b488  In reply to the above modern stations are in f...\n",
       "13  0c46ec0e711469190af8  Why is fatima put as winner and anya as runner...\n",
       "14  ecaeb00c751a61c1370e  I am sure that you are the SON of one of those...\n",
       "15  dfaa434123c910477026  \"\\nHow ironic, that one was one I handled. I'l...\n",
       "16  f0e8517a1d9e1394696b  97.84.13.145 is right, it's not about whether ...\n",
       "17  db952db7fb786093a8f7  Hmm, it sounds like we have a conflict between...\n",
       "18  132ad2e7621d9c132c3f  Regarding edits made during September 14 2006 ...\n",
       "19  38eef2c8994e8a980bb1  If you need help with resampling images, let m...\n",
       "20  160a621ad6ee20736755  \"\\nAh, yes, here's the MastCell edit, through ..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.loc[0:20,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38297"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id      38297\n",
       "text    38297\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>38297</td>\n",
       "      <td>38297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>38297</td>\n",
       "      <td>38297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>e0ae9d9474a5689a5791</td>\n",
       "      <td>in an interview before his execution</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          id                                  text\n",
       "count                  38297                                 38297\n",
       "unique                 38297                                 38297\n",
       "top     e0ae9d9474a5689a5791  in an interview before his execution\n",
       "freq                       1                                     1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>harsh</th>\n",
       "      <th>extremely_harsh</th>\n",
       "      <th>vulgar</th>\n",
       "      <th>threatening</th>\n",
       "      <th>disrespect</th>\n",
       "      <th>targeted_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e0ae9d9474a5689a5791</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b64a191301cad4f11287</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5e1953d9ae04bdc66408</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23128f98196c8e8f7b90</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2d3f1254f71472bf2b78</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>21f4f0f4812a08ea6c28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>733b43d534c67c1be948</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>aad47a397f7ddc629d5d</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>d19fcde8a3af2e472d74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7d4de482c60f1c8a79c6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>f81afe094bcd161ea6f8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>132973e40fa63e0d07f1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>37195759bf9104c3b488</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0c46ec0e711469190af8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ecaeb00c751a61c1370e</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>dfaa434123c910477026</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>f0e8517a1d9e1394696b</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>db952db7fb786093a8f7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>132ad2e7621d9c132c3f</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>38eef2c8994e8a980bb1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>160a621ad6ee20736755</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id  harsh  extremely_harsh  vulgar  threatening  \\\n",
       "0   e0ae9d9474a5689a5791      0                0       0            0   \n",
       "1   b64a191301cad4f11287      0                0       0            0   \n",
       "2   5e1953d9ae04bdc66408      0                0       0            0   \n",
       "3   23128f98196c8e8f7b90      0                0       0            0   \n",
       "4   2d3f1254f71472bf2b78      0                0       0            0   \n",
       "5   21f4f0f4812a08ea6c28      0                0       0            0   \n",
       "6   733b43d534c67c1be948      0                0       0            0   \n",
       "7   aad47a397f7ddc629d5d      0                0       0            0   \n",
       "8   d19fcde8a3af2e472d74      0                0       0            0   \n",
       "9   7d4de482c60f1c8a79c6      0                0       0            0   \n",
       "10  f81afe094bcd161ea6f8      0                0       0            0   \n",
       "11  132973e40fa63e0d07f1      0                0       0            0   \n",
       "12  37195759bf9104c3b488      0                0       0            0   \n",
       "13  0c46ec0e711469190af8      0                0       0            0   \n",
       "14  ecaeb00c751a61c1370e      0                0       0            0   \n",
       "15  dfaa434123c910477026      0                0       0            0   \n",
       "16  f0e8517a1d9e1394696b      0                0       0            0   \n",
       "17  db952db7fb786093a8f7      0                0       0            0   \n",
       "18  132ad2e7621d9c132c3f      0                0       0            0   \n",
       "19  38eef2c8994e8a980bb1      0                0       0            0   \n",
       "20  160a621ad6ee20736755      0                0       0            0   \n",
       "\n",
       "    disrespect  targeted_hate  \n",
       "0            0              0  \n",
       "1            0              0  \n",
       "2            0              0  \n",
       "3            0              0  \n",
       "4            0              0  \n",
       "5            0              0  \n",
       "6            0              0  \n",
       "7            0              0  \n",
       "8            0              0  \n",
       "9            0              0  \n",
       "10           0              0  \n",
       "11           0              0  \n",
       "12           0              0  \n",
       "13           0              0  \n",
       "14           0              0  \n",
       "15           0              0  \n",
       "16           0              0  \n",
       "17           0              0  \n",
       "18           0              0  \n",
       "19           0              0  \n",
       "20           0              0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df.loc[0:20,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>harsh</th>\n",
       "      <th>extremely_harsh</th>\n",
       "      <th>vulgar</th>\n",
       "      <th>threatening</th>\n",
       "      <th>disrespect</th>\n",
       "      <th>targeted_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>38297.0</td>\n",
       "      <td>38297.0</td>\n",
       "      <td>38297.0</td>\n",
       "      <td>38297.0</td>\n",
       "      <td>38297.0</td>\n",
       "      <td>38297.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         harsh  extremely_harsh   vulgar  threatening  disrespect  \\\n",
       "count  38297.0          38297.0  38297.0      38297.0     38297.0   \n",
       "mean       0.0              0.0      0.0          0.0         0.0   \n",
       "std        0.0              0.0      0.0          0.0         0.0   \n",
       "min        0.0              0.0      0.0          0.0         0.0   \n",
       "25%        0.0              0.0      0.0          0.0         0.0   \n",
       "50%        0.0              0.0      0.0          0.0         0.0   \n",
       "75%        0.0              0.0      0.0          0.0         0.0   \n",
       "max        0.0              0.0      0.0          0.0         0.0   \n",
       "\n",
       "       targeted_hate  \n",
       "count        38297.0  \n",
       "mean             0.0  \n",
       "std              0.0  \n",
       "min              0.0  \n",
       "25%              0.0  \n",
       "50%              0.0  \n",
       "75%              0.0  \n",
       "max              0.0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Checking for missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text               0\n",
       "harsh              0\n",
       "extremely_harsh    0\n",
       "vulgar             0\n",
       "threatening        0\n",
       "disrespect         0\n",
       "targeted_hate      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no missing values in the training dataset. Checking further..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text               0\n",
       "harsh              0\n",
       "extremely_harsh    0\n",
       "vulgar             0\n",
       "threatening        0\n",
       "disrespect         0\n",
       "targeted_hate      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_df == \"?\").sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed there are no missing values in the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id      0\n",
       "text    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No missing values in the test dataset. Checking further..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id      0\n",
       "text    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(test_df == \"?\").sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no missing values in the test dataset for sure..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Cleaning the data and performing lemmatization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pe08 to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package pe08 is already up-to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     /Users/riddhichatterjee/nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"all\")\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "#import contractions\n",
    "\n",
    "lmt = WordNetLemmatizer()\n",
    "\n",
    "#unnecessary_words = set(set(stopwords.words(\"english\"))-set(['no', 'nor', 'not', 'against']))\n",
    "\n",
    "#   NOTE: Initially, 'negative words' were not considered as unnecessary words.\n",
    "#         But our models are currently giving less ROC_AUC score if this is done.\n",
    "#         So now, negative words are also considered as unnecessary words.\n",
    "\n",
    "unnecessary_words = [set(set(stopwords.words(\"english\")))]\n",
    "\n",
    "def isAllCaps(word):\n",
    "    if word == word.upper():\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def transformText(text):\n",
    "    #text = contractions.fix(text) #Expanding contractions\n",
    "    \n",
    "    #   NOTE: Contractions like \"isn't\", \"haven't\" etc were being expanded only when we were not considering 'negative words' as unnecessary words.\n",
    "    #         Currently, it is useless to expand contractions\n",
    "    \n",
    "    text = re.sub('[^a-zA-Z]', ' ', text) #Replacing all characters except a-z and A-Z, by a whitespace character\n",
    "    text = text.split() #Extracting words from the text\n",
    "    text = [word.lower() if not isAllCaps(word) else word.lower() for word in text] #Converting all words to lowercase\n",
    "    text = [lmt.lemmatize(word) for word in text if not word in unnecessary_words] #Performing lemmatization and removing unnecessary words\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "train_df[\"text\"] = train_df[\"text\"].apply(transformText)\n",
    "test_df[\"text\"] = test_df[\"text\"].apply(transformText)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     december utc i am interested not in arguing bu...\n",
       "1     i added about three missing parameter to templ...\n",
       "2           sandbox i did your madre did in the sandbox\n",
       "3     why good sir why you sir obviously do not comp...\n",
       "4     source incase i forget or someone else want to...\n",
       "5     neither of your argument are persuasive you di...\n",
       "6     i knew this wa a left wing blog and the above ...\n",
       "7     how can you ignore truth you want to libel me ...\n",
       "8     regarding edits made during march utc to nostr...\n",
       "9     user dr k hello he is vandalising edit warring...\n",
       "10    removed part of the article that were not sour...\n",
       "11    common name it ha come to my attention there i...\n",
       "12    but bb wa added first all i am saying is that ...\n",
       "13    it ha become quite clear that the serialcomma ...\n",
       "14    you know i already finish the main temple stru...\n",
       "15    by the way it s not chu nom but han tu chu nom...\n",
       "16    wow this bot is amazing it s like futurama now...\n",
       "17    contesting my ban i refuse to be banned by a m...\n",
       "18    please leave the page alone or i ll have to re...\n",
       "19    welcome hello and welcome to wikipedia thank y...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"text\"][0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                  in an interview before his execution\n",
       "1     he knew what he wa doing the below post are no...\n",
       "2     zzzzzzz youre a real bore now go bore someone ...\n",
       "3     yet it remains confusion because the is just m...\n",
       "4     i wa referring to them losing interest in vand...\n",
       "5     march utc that wasn t an attack ad hominem tha...\n",
       "6     you re not reading properly i ve asked you wha...\n",
       "7     please look at the discussion here and here ly...\n",
       "8     utc call of duty ha never made any claim of ac...\n",
       "9                                 you too man take care\n",
       "10    according to the world book encyclopedia it sa...\n",
       "11    i don t object to removing it a unsourced pov ...\n",
       "12    in reply to the above modern station are in fa...\n",
       "13    why is fatima put a winner and anya a runner u...\n",
       "14    i am sure that you are the son of one of those...\n",
       "15    how ironic that one wa one i handled i ll merg...\n",
       "16    is right it s not about whether you try to do ...\n",
       "17    hmm it sound like we have a conflict between s...\n",
       "18    regarding edits made during september utc to a...\n",
       "19    if you need help with resampling image let me ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[\"text\"][0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Checking for duplicate rows in the training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "481"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df.drop(axis=\"rows\", labels=train_df.index[train_df.duplicated()], inplace=True) #Removing duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "481"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Performing oversampling (basic):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rare_class_df = train_df.loc[(train_df['extremely_harsh'] == 1) | (train_df['targeted_hate'] == 1)]\n",
    "#train_df = rare_class_df.append(train_df, ignore_index = True)\n",
    "#train_df = train_df.sample(frac = 1)\n",
    "\n",
    "#   NOTE: The current (basic) implementation of oversampling is not improving the ROC_AUC score.\n",
    "#         A better implementation is needed..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Structuring the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6.1 Extracting the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>harsh</th>\n",
       "      <th>extremely_harsh</th>\n",
       "      <th>vulgar</th>\n",
       "      <th>threatening</th>\n",
       "      <th>disrespect</th>\n",
       "      <th>targeted_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89354</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89355</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89356</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89357</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89358</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>89359 rows  6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       harsh  extremely_harsh  vulgar  threatening  disrespect  targeted_hate\n",
       "0          0                0       0            0           0              0\n",
       "1          0                0       0            0           0              0\n",
       "2          1                0       0            0           0              0\n",
       "3          1                0       1            1           1              0\n",
       "4          0                0       0            0           0              0\n",
       "...      ...              ...     ...          ...         ...            ...\n",
       "89354      0                0       0            0           0              0\n",
       "89355      0                0       0            0           0              0\n",
       "89356      0                0       0            0           0              0\n",
       "89357      0                0       0            0           0              0\n",
       "89358      0                0       0            0           0              0\n",
       "\n",
       "[89359 rows x 6 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = train_df.iloc[:, 1:]\n",
    "label_type = {\n",
    "    0 : 'harsh',\n",
    "    1 : 'extremely_harsh',\n",
    "    2 : 'vulgar',\n",
    "    3 : 'threatening',\n",
    "    4 : 'disrespect',\n",
    "    5 : 'targeted_hate'\n",
    "}\n",
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6.2 Creating the data matrix:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following techniques have been used to create data matrices:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.6.2.1 Using Bag of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(89359, 112864)\n",
      "(38297, 112864)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "#column_list = [train_df[\"text\"], test_df[\"text\"]]\n",
    "#combined_data = pd.concat(column_list, ignore_index=True)\n",
    "cv.fit(train_df[\"text\"]) #Fitting on the training data\n",
    "X_train_bow = cv.transform(train_df[\"text\"])\n",
    "X_test_bow = cv.transform(test_df[\"text\"])\n",
    "print(X_train_bow.shape)\n",
    "print(X_test_bow.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.6.2.2 Using TF-IDF:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF, short for term frequencyinverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.\n",
    "\n",
    "The tfidf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general.\n",
    "\n",
    "Reference: https://www.youtube.com/watch?v=D2V1okCEsiE&list=PLZoTAELRMXVMdJ5sqbCK2LiM0HhQVWNzm&index=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(89359, 112864)\n",
      "(38297, 112864)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tvVec = TfidfVectorizer()\n",
    "#column_list = [train_df[\"text\"], test_df[\"text\"]]\n",
    "#combined_data = pd.concat(column_list, ignore_index=True)\n",
    "tvVec.fit(train_df[\"text\"]) #Fitting on the training data\n",
    "X_train_tfidf = tvVec.transform(train_df[\"text\"])\n",
    "X_test_tfidf = tvVec.transform(test_df[\"text\"])\n",
    "print(X_train_tfidf.shape)\n",
    "print(X_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.6.2.3 Using Doc2vec:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doc2Vec is a Model that represents each Document as a Vector.\n",
    "\n",
    "There are two implementations:\n",
    "1) Paragraph Vector - Distributed Memory (PV-DM)\n",
    "2) Paragraph Vector - Distributed Bag of Words (PV-DBOW)\n",
    "\n",
    "\n",
    "PV-DM is analogous to Word2Vec CBOW (Continuous Bag-Of-Words). The doc-vectors are obtained by training a neural network on the synthetic task of predicting a center word based on average of both context word-vectors and the full documents doc-vector.\n",
    "\n",
    "PV-DBOW is analogous to Word2Vec SG (SkipGram). The doc-vectors are obtained by training a neural network on the synthetic task of predicting a target word just from the full documents doc-vector.\n",
    "\n",
    "\n",
    "Reference: https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html#sphx-glr-auto-examples-tutorials-run-doc2vec-lee-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_PV_DM:\n",
      "[[-0.09880176 -0.16840743  0.01133136 ...  0.06763586 -0.00141508\n",
      "   0.01831389]\n",
      " [-0.12124774 -0.01204015 -0.14991055 ...  0.04506883  0.05359776\n",
      "   0.00332597]\n",
      " [-0.01459364  0.05586129  0.02971586 ... -0.02655341 -0.02856315\n",
      "  -0.06787729]\n",
      " ...\n",
      " [-0.02005223 -0.00799155 -0.11050207 ... -0.04725276 -0.03393681\n",
      "   0.00207261]\n",
      " [-0.04008653 -0.01210959  0.01893803 ...  0.00706374 -0.00037979\n",
      "   0.01443414]\n",
      " [-0.02224958  0.00513086 -0.00214858 ...  0.01870983  0.02579492\n",
      "   0.00675796]]\n",
      "\n",
      "X_test_PV_DM:\n",
      "[[-0.00523626 -0.0121025  -0.00432658 ... -0.03078797  0.00103106\n",
      "   0.01793174]\n",
      " [-0.05240223 -0.06793734 -0.0333886  ... -0.08998971 -0.17119977\n",
      "  -0.03379881]\n",
      " [-0.01590149  0.03556761 -0.04723367 ...  0.020498    0.00435879\n",
      "   0.00837729]\n",
      " ...\n",
      " [-0.14738154 -0.03497483 -0.05204291 ... -0.06650203 -0.02392867\n",
      "   0.06161407]\n",
      " [-0.0076974   0.02071803 -0.00268616 ... -0.02418819  0.01119902\n",
      "  -0.02712341]\n",
      " [ 0.01760116  0.09750821  0.05209621 ... -0.15760416  0.00998216\n",
      "  -0.0761494 ]]\n",
      "\n",
      "X_train_PV_DBOW:\n",
      "[[-1.9587697e-02 -4.3583650e-02 -7.2778732e-02 ... -1.0434209e-01\n",
      "   3.2113761e-02  5.3915299e-02]\n",
      " [-3.6456216e-02  1.0566084e-01 -8.9167967e-02 ... -1.3296217e-01\n",
      "  -2.1430805e-02  3.1304933e-02]\n",
      " [-3.7391696e-02  1.9399269e-02  1.1614264e-02 ... -2.8101578e-02\n",
      "  -6.0297614e-03  1.2782126e-02]\n",
      " ...\n",
      " [ 4.0963661e-02  5.1007643e-03  4.1273318e-02 ... -3.4544878e-02\n",
      "  -8.1489207e-03 -6.6766642e-02]\n",
      " [ 1.4512045e-02  4.8938185e-02  9.9599073e-03 ... -3.4007996e-02\n",
      "   1.8450629e-03 -2.4791939e-02]\n",
      " [ 5.6817755e-03 -2.7424176e-03 -3.2283150e-02 ... -1.3397080e-01\n",
      "  -1.5650567e-02 -2.3807428e-05]]\n",
      "\n",
      "X_test_PV_DBOW:\n",
      "[[-0.02585446  0.05712249 -0.0725817  ... -0.00749726 -0.0077074\n",
      "   0.00683524]\n",
      " [ 0.03970557 -0.03070883 -0.00034069 ... -0.00930974 -0.0508264\n",
      "  -0.0620894 ]\n",
      " [-0.01074979  0.0264409   0.02753952 ... -0.00057782 -0.00835039\n",
      "  -0.01772656]\n",
      " ...\n",
      " [-0.03533917  0.09857389 -0.02300772 ... -0.03226328  0.06130254\n",
      "   0.05553766]\n",
      " [ 0.01295327  0.05653363  0.02545019 ...  0.00672015 -0.0237275\n",
      "   0.00356447]\n",
      " [-0.04354641 -0.00516649 -0.05239465 ... -0.13649604 -0.03843809\n",
      "  -0.02179427]]\n"
     ]
    }
   ],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "train_df_tokenized = []\n",
    "test_df_tokenized = []\n",
    "\n",
    "for text in train_df['text']:\n",
    "    train_df_tokenized.append(text.split())\n",
    "    \n",
    "for text in test_df['text']:\n",
    "    test_df_tokenized.append(text.split())\n",
    "\n",
    "my_texts = train_df_tokenized + test_df_tokenized #Consists of texts from both the training and test data\n",
    "\n",
    "#Using 'my_texts' to train the Doc2Vec models:    \n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(my_texts)]\n",
    "model_PV_DM = Doc2Vec(documents, vector_size=2000, window=2, min_count=1, workers=4, dm=1) #Paragraph Vector - Distributed Memory\n",
    "model_PV_DBOW = Doc2Vec(documents, vector_size=2000, window=2, min_count=1, workers=4, dm=0) #Paragraph Vector - Distributed Bag of Words\n",
    "\n",
    "#Obtaining data matrices for PV-DM and PV-DBOW:\n",
    "lst = []\n",
    "for word_list in train_df_tokenized:\n",
    "    lst.append(model_PV_DM.infer_vector(word_list))\n",
    "X_train_PV_DM = np.array(lst)\n",
    "\n",
    "lst = []\n",
    "for word_list in test_df_tokenized:\n",
    "    lst.append(model_PV_DM.infer_vector(word_list))\n",
    "X_test_PV_DM = np.array(lst)\n",
    "\n",
    "lst = []\n",
    "for word_list in train_df_tokenized:\n",
    "    lst.append(model_PV_DBOW.infer_vector(word_list))\n",
    "X_train_PV_DBOW = np.array(lst)\n",
    "\n",
    "lst = []\n",
    "for word_list in test_df_tokenized:\n",
    "    lst.append(model_PV_DBOW.infer_vector(word_list))\n",
    "X_test_PV_DBOW = np.array(lst)\n",
    "\n",
    "print(\"X_train_PV_DM:\")\n",
    "print(X_train_PV_DM)\n",
    "print(\"\\nX_test_PV_DM:\")\n",
    "print(X_test_PV_DM)\n",
    "print(\"\\nX_train_PV_DBOW:\")\n",
    "print(X_train_PV_DBOW)\n",
    "print(\"\\nX_test_PV_DBOW:\")\n",
    "print(X_test_PV_DBOW)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.6.2.4 Using Word2vec:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word2vec algorithm uses a neural network model to learn word associations from a large corpus of text. Once trained, such a model can detect synonymous words or suggest additional words for a partial sentence. \n",
    "\n",
    "Word2vec represents each distinct word with a vector. The vectors are chosen carefully such that they capture the semantic and syntactic qualities of words; as such, a simple mathematical function (cosine similarity) can indicate the level of semantic similarity between the words represented by those vectors.\n",
    "\n",
    "Reference: https://www.youtube.com/watch?v=8rXD5-xhemo&list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.6.2.4.1 Method 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(89359, 2000)\n",
      "(38297, 2000)\n"
     ]
    }
   ],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "my_texts = [] #Consists of texts from both the training and test data\n",
    "for text in train_df['text']:\n",
    "    my_texts.append(text.split())\n",
    "for text in test_df[\"text\"]:\n",
    "    my_texts.append(text.split())\n",
    "\n",
    "#vectorSize = int(X_train_tfidf.shape[1]/2)\n",
    "vectorSize = 1000\n",
    "model = Word2Vec(sentences=my_texts, vector_size=vectorSize, window=5, min_count=1, workers=4) #Using 'my_texts' to train the Word2Vec model\n",
    "dummyFV = np.array([0]*2*vectorSize)\n",
    "\n",
    "featureLst = []\n",
    "for text in train_df[\"text\"]:\n",
    "    if text == \"\":\n",
    "        featureLst.append(dummyFV)\n",
    "        continue\n",
    "    min = model.wv[text.split()[0]]\n",
    "    max = min\n",
    "    for word in text.split():\n",
    "        min = np.minimum(model.wv[word], min) #Obtaining the coordinate-wise minimum of the vector representations of all the words in the text\n",
    "        max = np.maximum(model.wv[word], max) #Obtaining the coordinate-wise maximum of the vector representations of all the words in the text\n",
    "    featureVec = np.concatenate((min, max)) #Concatenating min and max to obtain a feature vector for each data point\n",
    "    featureLst.append(featureVec)\n",
    "\n",
    "X_train_W2V_method_1 = np.stack(featureLst) #Obtaining the data matrix for the training data\n",
    "print(X_train_W2V_method_1.shape)\n",
    "\n",
    "\n",
    "#Performing similar steps for the test data:\n",
    "featureLst = []\n",
    "for text in test_df[\"text\"]:\n",
    "    if text == \"\":\n",
    "        featureLst.append(dummyFV)\n",
    "        continue\n",
    "    min = model.wv[text.split()[0]]\n",
    "    max = min\n",
    "    for word in text.split():\n",
    "        min = np.minimum(model.wv[word], min)\n",
    "        max = np.maximum(model.wv[word], max)\n",
    "    featureVec = np.concatenate((min, max))\n",
    "    featureLst.append(featureVec)\n",
    "\n",
    "X_test_W2V_method_1 = np.stack(featureLst)\n",
    "print(X_test_W2V_method_1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Performing standardization, outlier removal and skew removal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "from scipy.stats import skew\n",
    "\n",
    "class Standardizer: \n",
    "    def __init__(self, mean, stdev):\n",
    "        self.mean = mean \n",
    "        self.stdev = stdev\n",
    "    def scale(self, x):\n",
    "        return (x - self.mean)/self.stdev\n",
    "    \n",
    "    \n",
    "def removeRightSkew(x):\n",
    "    x = x - np.min(x) + 0.0000001\n",
    "    return np.log(x)\n",
    "def removeLeftSkew(x):\n",
    "    return np.exp(x)\n",
    "\n",
    "\n",
    "class OutlierRemoval: \n",
    "    def __init__(self, lower_quartile, upper_quartile):\n",
    "        self.lower_whisker = lower_quartile - 1.5*(upper_quartile - lower_quartile)\n",
    "        self.upper_whisker = upper_quartile + 1.5*(upper_quartile - lower_quartile)\n",
    "    def removeOutlier(self, x):\n",
    "        x[x<self.lower_whisker] = self.lower_whisker\n",
    "        x[x>self.upper_whisker] = self.upper_whisker\n",
    "        return x\n",
    "    \n",
    "\n",
    "def operate(dm):\n",
    "    for i in range(dm.shape[1]):\n",
    "        #Outlier removal:\n",
    "        outlier_remover = OutlierRemoval(pd.DataFrame(dm[:,i])[0].quantile(0.25), pd.DataFrame(dm[:,i])[0].quantile(0.75))\n",
    "        dm[:,i] = outlier_remover.removeOutlier(dm[:,i])\n",
    "\n",
    "        #Skew removal:\n",
    "        if(skew(dm[:,i]) > 0): #This means that this column has right skew\n",
    "            dm[:,i] = removeRightSkew(dm[:,i])\n",
    "        elif(skew(dm[:,i]) < 0): #This means that this column has left skew\n",
    "            dm[:,i] = removeLeftSkew(dm[:,i])    \n",
    "            \n",
    "        #Standardization:\n",
    "        dm[:,i] = Standardizer(dm[:,i].mean(), dm[:,i].std()).scale(dm[:,i])\n",
    "    return dm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Currently this is taking too much time... (wasn't stopping after 20 mins)\n",
    "\n",
    "# X_train_tfidf = X_train_tfidf.toarray()\n",
    "# X_train_tfidf = operate(X_train_tfidf)\n",
    "# X_train_tfidf = sparse.csr_matrix(X_train_tfidf)\n",
    "# print(\"Finished operating on TFI-DF training data...\")\n",
    "\n",
    "# X_test_tfidf = X_test_tfidf.toarray()\n",
    "# X_test_tfidf = operate(X_test_tfidf)\n",
    "# X_test_tfidf = sparse.csr_matrix(X_test_tfidf)\n",
    "# print(\"Finished operating on TFI-DF test data...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: Currently this is taking too much time... (wasn't stopping after 20 mins)\n",
    "\n",
    "# X_train_bow = X_train_bow.toarray()\n",
    "# X_train_bow = operate(X_train_bow)\n",
    "# X_train_bow = sparse.csr_matrix(X_train_bow)\n",
    "# print(\"Finished operating on Bag of words training data...\")\n",
    "\n",
    "# X_test_bow = X_test_bow.toarray()\n",
    "# X_test_bow = operate(X_test_bow)\n",
    "# X_test_bow = sparse.csr_matrix(X_test_bow)\n",
    "# print(\"Finished operating on Bag of words test data...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished operating on PV_DBOW training data...\n",
      "Finished operating on PV_DBOW test data...\n"
     ]
    }
   ],
   "source": [
    "X_train_PV_DBOW = operate(X_train_PV_DBOW)\n",
    "X_train_PV_DBOW = sparse.csr_matrix(X_train_PV_DBOW)\n",
    "print(\"Finished operating on PV_DBOW training data...\")\n",
    "\n",
    "X_test_PV_DBOW = operate(X_test_PV_DBOW)\n",
    "X_test_PV_DBOW = sparse.csr_matrix(X_test_PV_DBOW)\n",
    "print(\"Finished operating on PV_DBOW test data...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished operating on PV_DM training data...\n",
      "Finished operating on PV_DM test data...\n"
     ]
    }
   ],
   "source": [
    "X_train_PV_DM = operate(X_train_PV_DM)\n",
    "X_train_PV_DM = sparse.csr_matrix(X_train_PV_DM)\n",
    "print(\"Finished operating on PV_DM training data...\")\n",
    "\n",
    "X_test_PV_DM = operate(X_test_PV_DM)\n",
    "X_test_PV_DM = sparse.csr_matrix(X_test_PV_DM)\n",
    "print(\"Finished operating on PV_DM test data...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished operating on W2V_method_1 training data...\n",
      "Finished operating on W2V_method_1 test data...\n"
     ]
    }
   ],
   "source": [
    "X_train_W2V_method_1 = operate(X_train_W2V_method_1)\n",
    "X_train_W2V_method_1 = sparse.csr_matrix(X_train_W2V_method_1)\n",
    "print(\"Finished operating on W2V_method_1 training data...\")\n",
    "\n",
    "X_test_W2V_method_1 = operate(X_test_W2V_method_1)\n",
    "X_test_W2V_method_1 = sparse.csr_matrix(X_test_W2V_method_1)\n",
    "print(\"Finished operating on W2V_method_1 test data...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def performGridSearchCV(model, param_grid, XTrain, yTrain, CV=5, Scoring='f1'):\n",
    "    model_cv = GridSearchCV(model, param_grid, cv=CV, scoring = Scoring)\n",
    "    model_cv.fit(XTrain, yTrain)\n",
    "    return model_cv.best_params_, model_cv.best_score_\n",
    "\n",
    "def execute(clf, train_data, test_data, model_name, data_matrix_name, csv_filename, mode = \"8020Split\"):\n",
    "    #General function to train and test models on different data matrices\n",
    "    fname = \"ModelOutputs/\"+csv_filename\n",
    "    fname = fname[:len(fname)-9] + \".out\"\n",
    "    with open(fname, \"w\") as f:\n",
    "        f.write(\"Model: \"+model_name+\"\\n\")\n",
    "        f.write(\"Data matrix: \"+data_matrix_name+\"\\n\\n\")\n",
    "        \n",
    "        if mode == \"GS\": #Fits the model on the entire training data, after performing 5 fold grid search CV based on roc_auc score\n",
    "            data = train_data\n",
    "            CVScores, CVParams = clf.fit(data, y_train, 'roc_auc', 5)\n",
    "    \n",
    "            f.write(\"Tuned parameters after 5 fold grid search CV:\\n\")\n",
    "            for i in range(6):\n",
    "                f.write(\"Parameters for \"+label_type[i]+\": \"+str(CVParams[label_type[i]])+\"\\n\")\n",
    "            f.write(\"\\n\")\n",
    "            f.write(\"Best roc_auc score after 5 fold grid search CV:\\n\")\n",
    "            for i in range(6):\n",
    "                f.write(\"roc_auc score for \"+label_type[i]+\": \"+str(CVScores[label_type[i]])+\"\\n\")\n",
    "            f.write(\"\\n\")\n",
    "                \n",
    "        elif mode == \"WGS\": #Fits the model on the entire training data, without performing grid search CV\n",
    "            data = train_data\n",
    "            clf.fitWithoutGridSearch(data, y_train)\n",
    "            \n",
    "        elif mode == \"8020Split\":\n",
    "            #Performs a 80-20 split of the training data\n",
    "            #Fits the model on the train split, without performing grid search CV\n",
    "            #Validates the model on the validation/test split\n",
    "            \n",
    "            data = train_data\n",
    "            X_train_new, X_val, y_train_new, y_val = train_test_split(data, y_train, test_size=0.2, shuffle=False)\n",
    "            # clf.fit(X_train_new, y_train_new, 'roc_auc', 5)\n",
    "            clf.fitWithoutGridSearch(X_train_new, y_train_new)\n",
    "            \n",
    "            probs = clf.predictProbabilities(X_val)\n",
    "            f.write(\"ROC_AUC scores for the test split:\\n\")\n",
    "            mean_ROC_AUC_score = 0\n",
    "            for i in range(6):\n",
    "                labels = y_val[label_type[i]]\n",
    "                ROC_AUC_score = roc_auc_score(labels, probs[label_type[i]])\n",
    "                f.write(\"ROC_AUC score for \"+label_type[i]+\": \"+str(ROC_AUC_score)+\"\\n\")\n",
    "                mean_ROC_AUC_score += ROC_AUC_score\n",
    "            mean_ROC_AUC_score = mean_ROC_AUC_score/6\n",
    "            f.write(\"\\nMean ROC_AUC_score: \"+str(mean_ROC_AUC_score)+\"\\n\")\n",
    "            f.write(\"\\n\")\n",
    "        \n",
    "        #Calculating accuracy and f1 scores for the entire training data:\n",
    "        data = train_data\n",
    "        predictions = clf.predict(data)\n",
    "        f.write(\"Accuracy and f1 scores for the entire training data:\\n\")\n",
    "        for i in range(6):\n",
    "            labels = y_train[label_type[i]]\n",
    "            f.write(\"Accuracy score for \"+label_type[i]+\": \"+str(accuracy_score(labels, predictions[label_type[i]]))+\"\\n\")\n",
    "            f.write(\"F1 score for \"+label_type[i]+\": \"+str(f1_score(labels, predictions[label_type[i]]))+\"\\n\")\n",
    "        f.write(\"\\n\")\n",
    "    \n",
    "        #Calculating final class probabilities for training data:\n",
    "        probs = clf.predictProbabilities(data)\n",
    "        df=pd.DataFrame([])\n",
    "        column_values = ['harsh', 'extremely_harsh', 'vulgar', 'threatening', 'disrespect', 'targeted_hate']\n",
    "        for column_type in column_values:\n",
    "            df[column_type] = probs[column_type]\n",
    "        print(\"Final class probabilities for first 20 samples of training data:\")\n",
    "        print(df.loc[0:20,:])\n",
    "    \n",
    "        #Calculating final class probabilities for test data:\n",
    "        data = test_data\n",
    "        probs = clf.predictProbabilities(data)\n",
    "        df=pd.DataFrame([])\n",
    "        df['id'] = test_df[\"id\"]\n",
    "        column_values = ['harsh', 'extremely_harsh', 'vulgar', 'threatening', 'disrespect', 'targeted_hate']\n",
    "        for column_type in column_values:\n",
    "            df[column_type] = probs[column_type]\n",
    "        print(\"Final class probabilities for first 20 samples of test data:\")\n",
    "        print(df.loc[0:20,:])\n",
    "        df.to_csv(\"TestClassProbs/\"+csv_filename, index = False) #Storing final class probabilities for the test data in a csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.01 Support Vector Machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "class SVMClassifier(object): #Pack of 6 binary SVM classifiers\n",
    "    def __init__(self):\n",
    "        self.param_grid = {\n",
    "            'C' : [1.0], #default=1.0\n",
    "            'gamma' : ['auto'], #default='scale'\n",
    "        }\n",
    "        self.models = []\n",
    "        \n",
    "        for i in range(6): #Appending 6 binary SVM classifiers (one for each class)\n",
    "            self.models.append(svm.SVC(kernel = 'poly', degree = 5, max_iter=1000, probability=True))\n",
    "    \n",
    "    def fit(self, XTrain, yTrain, CVScoreType, numFolds): #Fits the models and returns the best CVScores and CVParams after performing grid search CV\n",
    "        CVScores = {}\n",
    "        CVParams = {}\n",
    "        \n",
    "        for i, model in enumerate(self.models):\n",
    "            labels = yTrain[label_type[i]]\n",
    "            \n",
    "            bestParams, bestScore = performGridSearchCV(model, self.param_grid, XTrain, labels, numFolds, CVScoreType)\n",
    "            CVParams[label_type[i]] = bestParams\n",
    "            CVScores[label_type[i]] = bestScore\n",
    "            \n",
    "            self.models[i] = svm.SVC(C = bestParams['C'], gamma = bestParams['gamma'], kernel = 'poly', degree = 5, max_iter=1000, probability=True)\n",
    "            self.models[i].fit(XTrain, labels)\n",
    "            print(\"Fitted model \"+str(i))\n",
    "            \n",
    "        return CVScores, CVParams\n",
    "    \n",
    "    def fitWithoutGridSearch(self, XTrain, yTrain): #Fits the models without performing grid search CV\n",
    "        for i, model in enumerate(self.models):\n",
    "            labels = yTrain[label_type[i]]\n",
    "            self.models[i].fit(XTrain, labels)\n",
    "            print(\"Fitted model \"+str(i))\n",
    "    \n",
    "    def predict(self, X): #Returns the binary classification results for each of the models\n",
    "        predictions = {}\n",
    "        for i, model in enumerate(self.models):\n",
    "            predictions[label_type[i]] = model.predict(X)\n",
    "        return predictions\n",
    "    \n",
    "    def predictProbabilities(self, X): #Returns the predicted class probabilities for each of the classes\n",
    "        probabilities = {}\n",
    "        for i, model in enumerate(self.models):\n",
    "            classes = model.classes_\n",
    "            index = np.where(classes == 1)[0][0] #Getting the index of class 1\n",
    "            probabilities[label_type[i]] = model.predict_proba(X)[:, index] #Extracting the probabilities of class 1\n",
    "        return probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now testing out the SVM classifier on various data matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/riddhichatterjee/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted model 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/riddhichatterjee/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted model 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/riddhichatterjee/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted model 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/riddhichatterjee/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted model 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/riddhichatterjee/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted model 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/riddhichatterjee/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted model 5\n",
      "Final class probabilities for first 20 samples of training data:\n",
      "       harsh  extremely_harsh    vulgar  threatening  disrespect  \\\n",
      "0   0.097865         0.015560  0.060066     0.003146     0.05096   \n",
      "1   0.097865         0.015560  0.060066     0.003146     0.05096   \n",
      "2   0.097865         0.015560  0.060066     0.003146     0.05096   \n",
      "3   0.097865         0.015556  0.060066     0.003146     0.05096   \n",
      "4   0.097865         0.015560  0.060066     0.003146     0.05096   \n",
      "5   0.097865         0.015560  0.060066     0.003146     0.05096   \n",
      "6   0.097865         0.015560  0.060066     0.003146     0.05096   \n",
      "7   0.097865         0.015560  0.060066     0.003146     0.05096   \n",
      "8   0.097865         0.015560  0.060066     0.003146     0.05096   \n",
      "9   0.097865         0.015560  0.060066     0.003146     0.05096   \n",
      "10  0.097865         0.015560  0.060066     0.003146     0.05096   \n",
      "11  0.097865         0.015560  0.060066     0.003146     0.05096   \n",
      "12  0.097865         0.015560  0.060066     0.003146     0.05096   \n",
      "13  0.097865         0.015560  0.060066     0.003146     0.05096   \n",
      "14  0.097865         0.015560  0.060066     0.003146     0.05096   \n",
      "15  0.097865         0.015560  0.060066     0.003146     0.05096   \n",
      "16  0.097865         0.015560  0.060066     0.003146     0.05096   \n",
      "17  0.097865         0.015560  0.060066     0.003146     0.05096   \n",
      "18  0.097865         0.015560  0.060066     0.003146     0.05096   \n",
      "19  0.097865         0.015560  0.060066     0.003146     0.05096   \n",
      "20  0.097865         0.015560  0.060066     0.003146     0.05096   \n",
      "\n",
      "    targeted_hate  \n",
      "0        0.009006  \n",
      "1        0.009006  \n",
      "2        0.009006  \n",
      "3        0.009006  \n",
      "4        0.009006  \n",
      "5        0.009006  \n",
      "6        0.009006  \n",
      "7        0.009006  \n",
      "8        0.009006  \n",
      "9        0.009006  \n",
      "10       0.009006  \n",
      "11       0.009006  \n",
      "12       0.009006  \n",
      "13       0.009006  \n",
      "14       0.009006  \n",
      "15       0.009006  \n",
      "16       0.009006  \n",
      "17       0.009006  \n",
      "18       0.009006  \n",
      "19       0.009006  \n",
      "20       0.009006  \n",
      "Final class probabilities for first 20 samples of test data:\n",
      "                      id     harsh  extremely_harsh    vulgar  threatening  \\\n",
      "0   e0ae9d9474a5689a5791  0.097865         0.015560  0.060066     0.003146   \n",
      "1   b64a191301cad4f11287  0.097865         0.015560  0.060066     0.003146   \n",
      "2   5e1953d9ae04bdc66408  0.097865         0.015560  0.060066     0.003146   \n",
      "3   23128f98196c8e8f7b90  0.097865         0.015560  0.060066     0.003146   \n",
      "4   2d3f1254f71472bf2b78  0.097865         0.015560  0.060066     0.003146   \n",
      "5   21f4f0f4812a08ea6c28  0.097865         0.015557  0.060066     0.003146   \n",
      "6   733b43d534c67c1be948  0.097865         0.015560  0.060066     0.003146   \n",
      "7   aad47a397f7ddc629d5d  0.097865         0.015560  0.060066     0.003146   \n",
      "8   d19fcde8a3af2e472d74  0.097865         0.015560  0.060066     0.003146   \n",
      "9   7d4de482c60f1c8a79c6  0.097865         0.015560  0.060066     0.003146   \n",
      "10  f81afe094bcd161ea6f8  0.097865         0.015560  0.060066     0.003146   \n",
      "11  132973e40fa63e0d07f1  0.097865         0.015560  0.060066     0.003146   \n",
      "12  37195759bf9104c3b488  0.097865         0.015560  0.060066     0.003146   \n",
      "13  0c46ec0e711469190af8  0.097865         0.015560  0.060066     0.003146   \n",
      "14  ecaeb00c751a61c1370e  0.097865         0.015560  0.060066     0.003146   \n",
      "15  dfaa434123c910477026  0.097865         0.015560  0.060066     0.003146   \n",
      "16  f0e8517a1d9e1394696b  0.097865         0.015560  0.060066     0.003146   \n",
      "17  db952db7fb786093a8f7  0.097865         0.015560  0.060066     0.003146   \n",
      "18  132ad2e7621d9c132c3f  0.097865         0.015560  0.060066     0.003146   \n",
      "19  38eef2c8994e8a980bb1  0.097865         0.015560  0.060066     0.003146   \n",
      "20  160a621ad6ee20736755  0.097865         0.015560  0.060066     0.003146   \n",
      "\n",
      "    disrespect  targeted_hate  \n",
      "0      0.05096       0.009006  \n",
      "1      0.05096       0.009006  \n",
      "2      0.05096       0.009006  \n",
      "3      0.05096       0.009006  \n",
      "4      0.05096       0.009006  \n",
      "5      0.05096       0.009006  \n",
      "6      0.05096       0.009006  \n",
      "7      0.05096       0.009006  \n",
      "8      0.05096       0.009006  \n",
      "9      0.05096       0.009006  \n",
      "10     0.05096       0.009006  \n",
      "11     0.05096       0.009006  \n",
      "12     0.05096       0.009006  \n",
      "13     0.05096       0.009006  \n",
      "14     0.05096       0.009006  \n",
      "15     0.05096       0.009006  \n",
      "16     0.05096       0.009006  \n",
      "17     0.05096       0.009006  \n",
      "18     0.05096       0.009006  \n",
      "19     0.05096       0.009006  \n",
      "20     0.05096       0.009006  \n"
     ]
    }
   ],
   "source": [
    "train_data = X_train_bow\n",
    "test_data = X_test_bow\n",
    "clf = SVMClassifier()\n",
    "csv_filename = \"SVMC_bow_Prob.csv\"\n",
    "model_name = \"SVM Classifier\"\n",
    "data_matrix_name = \"Bag of Words\"\n",
    "execute(clf, train_data, test_data, model_name, data_matrix_name, csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/riddhichatterjee/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted model 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/riddhichatterjee/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted model 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/riddhichatterjee/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted model 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/riddhichatterjee/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted model 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/riddhichatterjee/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted model 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/riddhichatterjee/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted model 5\n",
      "Final class probabilities for first 20 samples of training data:\n",
      "       harsh  extremely_harsh    vulgar  threatening  disrespect  \\\n",
      "0   0.094183         0.017207  0.070617     0.004166    0.060836   \n",
      "1   0.093980         0.017111  0.067789     0.004161    0.059897   \n",
      "2   0.360445         0.000459  0.063469     0.004160    0.060415   \n",
      "3   0.094819         0.017437  1.000000     0.876399    0.061706   \n",
      "4   0.094243         0.017203  0.069431     0.004163    0.058944   \n",
      "5   0.094197         0.017183  0.072801     0.004162    0.060872   \n",
      "6   0.094301         0.017222  0.070629     0.004170    0.059903   \n",
      "7   0.093714         0.016859  0.058086     0.004171    0.055343   \n",
      "8   0.093188         0.016965  0.061435     0.004097    0.051480   \n",
      "9   0.094145         0.017107  0.067073     0.004158    0.060828   \n",
      "10  0.094212         0.017173  0.068886     0.004150    0.060756   \n",
      "11  0.094202         0.017157  0.062300     0.004163    0.060353   \n",
      "12  0.094290         0.017216  0.071221     0.004168    0.061070   \n",
      "13  0.094297         0.017212  0.071323     0.004169    0.061050   \n",
      "14  0.094198         0.017169  0.070320     0.004166    0.060581   \n",
      "15  0.094294         0.017186  0.071368     0.004163    0.061013   \n",
      "16  0.094262         0.017210  0.067585     0.004164    0.060823   \n",
      "17  0.094179         0.017183  0.070862     0.004166    0.060084   \n",
      "18  0.091501         0.016447  0.056860     0.003977    0.055274   \n",
      "19  0.090327         0.016989  0.021654     0.004136    0.042313   \n",
      "20  0.092950         0.017040  0.059787     0.004152    0.057869   \n",
      "\n",
      "    targeted_hate  \n",
      "0        0.014907  \n",
      "1        0.014857  \n",
      "2        0.013870  \n",
      "3        0.014832  \n",
      "4        0.014890  \n",
      "5        0.014841  \n",
      "6        0.014806  \n",
      "7        0.014807  \n",
      "8        0.013781  \n",
      "9        0.014843  \n",
      "10       0.014875  \n",
      "11       0.014791  \n",
      "12       0.014931  \n",
      "13       0.014931  \n",
      "14       0.014939  \n",
      "15       0.014938  \n",
      "16       0.014934  \n",
      "17       0.014905  \n",
      "18       0.014463  \n",
      "19       0.014865  \n",
      "20       0.014652  \n",
      "Final class probabilities for first 20 samples of test data:\n",
      "                      id     harsh  extremely_harsh    vulgar  threatening  \\\n",
      "0   e0ae9d9474a5689a5791  0.094305         0.017223  0.071307     0.004222   \n",
      "1   b64a191301cad4f11287  0.092875         0.016980  0.032409     0.004145   \n",
      "2   5e1953d9ae04bdc66408  0.094266         0.017218  0.055049     0.004172   \n",
      "3   23128f98196c8e8f7b90  0.094286         0.017217  0.071356     0.004168   \n",
      "4   2d3f1254f71472bf2b78  0.092259         0.016781  0.043391     0.004157   \n",
      "5   21f4f0f4812a08ea6c28  0.093679         0.016767  0.061827     0.004110   \n",
      "6   733b43d534c67c1be948  0.094250         0.017162  0.072134     0.004158   \n",
      "7   aad47a397f7ddc629d5d  0.094214         0.017195  0.070699     0.004168   \n",
      "8   d19fcde8a3af2e472d74  0.094303         0.017221  0.071330     0.004169   \n",
      "9   7d4de482c60f1c8a79c6  0.094063         0.016767  0.060988     0.004166   \n",
      "10  f81afe094bcd161ea6f8  0.094212         0.017174  0.070579     0.004158   \n",
      "11  132973e40fa63e0d07f1  0.094204         0.017173  0.069273     0.004162   \n",
      "12  37195759bf9104c3b488  0.094189         0.017188  0.070604     0.004160   \n",
      "13  0c46ec0e711469190af8  0.094279         0.017213  0.069138     0.004167   \n",
      "14  ecaeb00c751a61c1370e  0.094311         0.017222  0.071613     0.004168   \n",
      "15  dfaa434123c910477026  0.094266         0.017195  0.071326     0.004165   \n",
      "16  f0e8517a1d9e1394696b  0.094358         0.017171  0.071167     0.004165   \n",
      "17  db952db7fb786093a8f7  0.094285         0.017203  0.071275     0.004128   \n",
      "18  132ad2e7621d9c132c3f  0.093780         0.017084  0.069170     0.004115   \n",
      "19  38eef2c8994e8a980bb1  0.093568         0.017168  0.060292     0.004061   \n",
      "20  160a621ad6ee20736755  0.094160         0.017206  0.069470     0.004166   \n",
      "\n",
      "    disrespect  targeted_hate  \n",
      "0     0.061053       0.014836  \n",
      "1     0.050552       0.014148  \n",
      "2     0.060936       0.014947  \n",
      "3     0.061069       0.014934  \n",
      "4     0.057597       0.014759  \n",
      "5     0.056609       0.014417  \n",
      "6     0.061020       0.014857  \n",
      "7     0.059152       0.014904  \n",
      "8     0.061131       0.014941  \n",
      "9     0.059606       0.014871  \n",
      "10    0.061016       0.014870  \n",
      "11    0.060755       0.014883  \n",
      "12    0.061022       0.014846  \n",
      "13    0.059860       0.014937  \n",
      "14    0.061118       0.014932  \n",
      "15    0.061098       0.014902  \n",
      "16    0.060927       0.014892  \n",
      "17    0.060389       0.014897  \n",
      "18    0.049905       0.013226  \n",
      "19    0.059979       0.014801  \n",
      "20    0.060931       0.014890  \n"
     ]
    }
   ],
   "source": [
    "train_data = X_train_tfidf\n",
    "test_data = X_test_tfidf\n",
    "clf = SVMClassifier()\n",
    "csv_filename = \"SVMC_tfidf_Prob.csv\"\n",
    "model_name = \"SVM Classifier\"\n",
    "data_matrix_name = \"Tf-idf\"\n",
    "execute(clf, train_data, test_data, model_name, data_matrix_name, csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/riddhichatterjee/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted model 0\n"
     ]
    }
   ],
   "source": [
    "train_data = X_train_PV_DM\n",
    "test_data = X_test_PV_DM\n",
    "clf = SVMClassifier()\n",
    "csv_filename = \"SVMC_PV_DM_Prob.csv\"\n",
    "model_name = \"SVM Classifier\"\n",
    "data_matrix_name = \"Paragraph Vector - Distributed Memory\"\n",
    "execute(clf, train_data, test_data, model_name, data_matrix_name, csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = X_train_PV_DBOW\n",
    "test_data = X_test_PV_DBOW\n",
    "clf = SVMClassifier()\n",
    "csv_filename = \"SVMC_PV_DBOW_Prob.csv\"\n",
    "model_name = \"SVM Classifier\"\n",
    "data_matrix_name = \"Paragraph Vector - Distributed Bag Of Words\"\n",
    "execute(clf, train_data, test_data, model_name, data_matrix_name, csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/riddhichatterjee/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted model 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/riddhichatterjee/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted model 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/riddhichatterjee/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted model 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/riddhichatterjee/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted model 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/riddhichatterjee/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted model 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/riddhichatterjee/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted model 5\n",
      "Final class probabilities for first 20 samples of training data:\n",
      "       harsh  extremely_harsh    vulgar   threatening  disrespect  \\\n",
      "0   0.032304         0.000532  0.036614  2.436623e-07    0.017614   \n",
      "1   0.031338         0.002451  0.036168  7.024986e-06    0.016812   \n",
      "2   0.026541         0.482743  0.032732  1.509382e-03    0.013008   \n",
      "3   0.032684         0.000033  0.037418  1.000000e+00    0.018026   \n",
      "4   0.027578         0.103335  0.033476  1.886796e-04    0.013738   \n",
      "5   0.029259         0.016926  0.034948  1.649827e-03    0.014903   \n",
      "6   0.027307         0.283729  0.033330  1.348854e-02    0.013596   \n",
      "7   0.027578         0.165183  0.033576  5.222490e-04    0.013812   \n",
      "8   0.032424         0.000048  0.037025  3.271377e-07    0.017690   \n",
      "9   0.028382         0.075166  0.034014  6.349090e-03    0.014192   \n",
      "10  0.029536         0.015016  0.034923  1.533146e-06    0.015209   \n",
      "11  0.027700         0.128374  0.033531  7.143536e-05    0.013719   \n",
      "12  0.028885         0.056313  0.034568  1.120676e-04    0.014838   \n",
      "13  0.028419         0.024275  0.033872  1.417859e-04    0.014147   \n",
      "14  0.026443         0.456951  0.032719  1.628008e-03    0.012915   \n",
      "15  0.027290         0.233007  0.033307  5.210664e-05    0.013525   \n",
      "16  0.026979         0.371411  0.033072  8.871342e-04    0.013251   \n",
      "17  0.027135         0.437285  0.033296  8.028477e-03    0.013428   \n",
      "18  0.027263         0.328921  0.033281  1.461428e-03    0.013497   \n",
      "19  0.031354         0.000320  0.036076  6.231289e-07    0.016545   \n",
      "20  0.036447         0.000001  0.039170  2.290243e-07    0.020471   \n",
      "\n",
      "    targeted_hate  \n",
      "0        0.000652  \n",
      "1        0.002476  \n",
      "2        0.192746  \n",
      "3        0.000357  \n",
      "4        0.092119  \n",
      "5        0.041400  \n",
      "6        0.104600  \n",
      "7        0.111583  \n",
      "8        0.000308  \n",
      "9        0.086414  \n",
      "10       0.042567  \n",
      "11       0.102478  \n",
      "12       0.097334  \n",
      "13       0.050278  \n",
      "14       0.183818  \n",
      "15       0.130287  \n",
      "16       0.174537  \n",
      "17       0.151759  \n",
      "18       0.094248  \n",
      "19       0.001928  \n",
      "20       0.000015  \n",
      "Final class probabilities for first 20 samples of test data:\n",
      "                      id     harsh  extremely_harsh    vulgar   threatening  \\\n",
      "0   e0ae9d9474a5689a5791  0.026447     4.657995e-01  0.032696  1.279176e-03   \n",
      "1   b64a191301cad4f11287  0.029650     4.960428e-03  0.034867  1.928931e-04   \n",
      "2   5e1953d9ae04bdc66408  0.026391     4.412498e-01  0.032696  1.478508e-03   \n",
      "3   23128f98196c8e8f7b90  0.028178     2.786881e-02  0.033944  1.058767e-04   \n",
      "4   2d3f1254f71472bf2b78  0.026862     3.642360e-01  0.032928  1.981992e-03   \n",
      "5   21f4f0f4812a08ea6c28  0.040053     1.000000e-07  0.042188  4.863421e-05   \n",
      "6   733b43d534c67c1be948  0.029434     4.960930e-03  0.034705  1.367317e-06   \n",
      "7   aad47a397f7ddc629d5d  0.027175     2.121530e-01  0.033183  1.197557e-04   \n",
      "8   d19fcde8a3af2e472d74  0.028374     3.759405e-02  0.034186  9.861984e-06   \n",
      "9   7d4de482c60f1c8a79c6  0.026282     4.781158e-01  0.032633  2.222531e-03   \n",
      "10  f81afe094bcd161ea6f8  0.029281     5.309371e-03  0.034631  1.198283e-04   \n",
      "11  132973e40fa63e0d07f1  0.031123     6.307838e-04  0.036036  5.360513e-07   \n",
      "12  37195759bf9104c3b488  0.030664     8.754157e-04  0.035571  2.617179e-07   \n",
      "13  0c46ec0e711469190af8  0.027422     1.379144e-01  0.033229  7.079329e-04   \n",
      "14  ecaeb00c751a61c1370e  0.028051     6.911410e-02  0.033911  2.532819e-03   \n",
      "15  dfaa434123c910477026  0.026885     4.384200e-01  0.033014  1.570729e-03   \n",
      "16  f0e8517a1d9e1394696b  0.028549     4.982571e-02  0.034090  1.815366e-04   \n",
      "17  db952db7fb786093a8f7  0.027572     2.664502e-01  0.033500  9.991135e-05   \n",
      "18  132ad2e7621d9c132c3f  0.030063     3.268886e-03  0.035393  2.018086e-06   \n",
      "19  38eef2c8994e8a980bb1  0.027883     8.850306e-02  0.033727  1.411068e-04   \n",
      "20  160a621ad6ee20736755  0.028405     4.130020e-02  0.034227  8.909185e-05   \n",
      "\n",
      "    disrespect  targeted_hate  \n",
      "0     0.012896   1.793011e-01  \n",
      "1     0.015166   3.565323e-02  \n",
      "2     0.012806   1.838147e-01  \n",
      "3     0.014215   7.412237e-02  \n",
      "4     0.013248   1.445685e-01  \n",
      "5     0.024536   9.021683e-07  \n",
      "6     0.015286   6.535572e-03  \n",
      "7     0.013397   1.490522e-01  \n",
      "8     0.014343   3.018151e-02  \n",
      "9     0.012784   1.882834e-01  \n",
      "10    0.014975   2.054294e-02  \n",
      "11    0.016315   2.581138e-03  \n",
      "12    0.016098   7.988823e-03  \n",
      "13    0.013580   9.751988e-02  \n",
      "14    0.014331   8.051220e-02  \n",
      "15    0.013298   1.550023e-01  \n",
      "16    0.014469   2.649148e-02  \n",
      "17    0.013735   8.529752e-02  \n",
      "18    0.015891   3.715024e-03  \n",
      "19    0.013824   1.151127e-01  \n",
      "20    0.014414   6.225296e-02  \n"
     ]
    }
   ],
   "source": [
    "train_data = X_train_W2V_method_1\n",
    "test_data = X_test_W2V_method_1\n",
    "clf = SVMClassifier()\n",
    "csv_filename = \"SVMC_W2V_method_1_Prob.csv\"\n",
    "model_name = \"SVM Classifier\"\n",
    "data_matrix_name = \"Word2Vec Method_1\"\n",
    "execute(clf, train_data, test_data, model_name, data_matrix_name, csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: Currently this can't be done as dimensions of X_train_PV_DBOW and X_train_tfidf don't match\n",
    "\n",
    "# train_data = X_train_PV_DBOW + X_train_tfidf\n",
    "# test_data = X_test_PV_DBOW + X_test_tfidf\n",
    "# clf = SVMClassifier()\n",
    "# csv_filename = \"SVMC_PV_DBOW_tfidf_Prob.csv\"\n",
    "# model_name = \"SVM Classifier\"\n",
    "# data_matrix_name = \"Combination of PV_DBOW and TF-IDF\"\n",
    "# execute(clf, train_data, test_data, model_name, data_matrix_name, csv_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.02 Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "class LogisticRegClassifier(object): #Pack of 6 binary Logistic Regression classifiers\n",
    "    def __init__(self, initial_params):\n",
    "        self.param_grid = {\n",
    "            'solver' : ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "            }\n",
    "        self.models = []\n",
    "\n",
    "        for i in range(6): #Appending 6 binary Logistic Regression classifiers (one for each class)\n",
    "            self.models.append(LogisticRegression(max_iter=initial_params[i]['max_iter'], class_weight=initial_params[i]['class_weight'], penalty=initial_params[i]['penalty'], solver=initial_params[i]['solver'], C=initial_params[i]['C'], tol=1e-9))\n",
    "\n",
    "    def fit(self, XTrain, yTrain, CVScoreType, numFolds): #Fits the models and returns the best CVScores and CVParams after performing grid search CV\n",
    "        CVScores = {}\n",
    "        CVParams = {}\n",
    "\n",
    "        for i, model in enumerate(self.models):\n",
    "            labels = yTrain[label_type[i]]\n",
    "\n",
    "            bestParams, bestScore = performGridSearchCV(model, self.param_grid, XTrain, labels, numFolds, CVScoreType)\n",
    "            CVParams[label_type[i]] = bestParams\n",
    "            CVScores[label_type[i]] = bestScore\n",
    "\n",
    "            self.models[i] = LogisticRegression(solver=bestParams['solver'],penalty='l2', max_iter=100000, class_weight='balanced')\n",
    "            self.models[i].fit(XTrain, labels)\n",
    "            print(\"Fitted model \"+str(i))\n",
    "\n",
    "        return CVScores, CVParams\n",
    "\n",
    "    def fitWithoutGridSearch(self, XTrain, yTrain): #Fits the models without performing grid search CV\n",
    "        for i, model in enumerate(self.models):\n",
    "            labels = yTrain[label_type[i]]\n",
    "            self.models[i].fit(XTrain, labels)   \n",
    "            print(\"Fitted model \"+str(i))\n",
    "    \n",
    "    def predict(self, X): #Returns the binary classification results for each of the models\n",
    "        predictions = {}\n",
    "        for i, model in enumerate(self.models):\n",
    "            predictions[label_type[i]] = model.predict(X)\n",
    "        return predictions\n",
    "\n",
    "    def predictProbabilities(self, X): #Returns the predicted class probabilities for each of the classes\n",
    "        probabilities = {}\n",
    "        for i, model in enumerate(self.models):\n",
    "            classes = model.classes_\n",
    "            index = np.where(classes == 1)[0][0] #Getting the index of class 1\n",
    "            probabilities[label_type[i]] = model.predict_proba(X)[:, index] #Extracting the probabilities of class 1\n",
    "        return probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now testing out the Logistic regression classifier on various data matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IP = {\n",
    "            0 : {'max_iter':100000, 'class_weight':'balanced', 'penalty':'l2', 'solver':'lbfgs', 'C':1.9},\n",
    "            1 : {'max_iter':100000, 'class_weight':'balanced', 'penalty':'l2', 'solver':'liblinear', 'C':0.175},\n",
    "            2 : {'max_iter':100000, 'class_weight':'balanced', 'penalty':'l2', 'solver':'liblinear', 'C':1.8},\n",
    "            3 : {'max_iter':100000, 'class_weight':'balanced', 'penalty':'l2', 'solver':'liblinear', 'C':1.6},\n",
    "            4 : {'max_iter':100000, 'class_weight':'balanced', 'penalty':'l2', 'solver':'liblinear', 'C':1.55},\n",
    "            5 : {'max_iter':100000, 'class_weight':'balanced', 'penalty':'l2', 'solver':'liblinear', 'C':2}\n",
    "        } #Initial parameters for logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: Currently this can't be done as dimensions of X_train_tfidf and X_train_bow don't match\n",
    "\n",
    "# train_data = X_train_tfidf + X_train_bow\n",
    "# test_data = X_test_tfidf + X_test_bow\n",
    "# clf = LogisticRegClassifier(IP)\n",
    "# csv_filename = \"LogisticReg_tfidf_bow_Prob.csv\"\n",
    "# model_name = \"Logistic Regression Classifier\"\n",
    "# data_matrix_name = \"Combination of TF-IDF and Bag of Words\"\n",
    "# execute(clf, train_data, test_data, model_name, data_matrix_name, csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Logistic Regression Classifier\n",
      "Data matrix: Bag of Words\n",
      "\n",
      "Fitted model 0\n",
      "Fitted model 1\n",
      "Fitted model 2\n",
      "Fitted model 3\n",
      "Fitted model 4\n"
     ]
    }
   ],
   "source": [
    "train_data = X_train_bow\n",
    "test_data = X_test_bow\n",
    "clf = LogisticRegClassifier(IP)\n",
    "csv_filename = \"LogisticReg_bow_Prob.csv\"\n",
    "model_name = \"Logistic Regression Classifier\"\n",
    "data_matrix_name = \"Bag of Words\"\n",
    "execute(clf, train_data, test_data, model_name, data_matrix_name, csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted model 0\n",
      "Fitted model 1\n",
      "Fitted model 2\n",
      "Fitted model 3\n",
      "Fitted model 4\n",
      "Fitted model 5\n",
      "Final class probabilities for first 20 samples of training data:\n",
      "       harsh  extremely_harsh    vulgar  threatening  disrespect  \\\n",
      "0   0.001456         0.005400  0.003846     0.003429    0.000865   \n",
      "1   0.000411         0.007325  0.001985     0.000588    0.000744   \n",
      "2   0.566746         0.212618  0.037427     0.004115    0.037875   \n",
      "3   0.998809         0.385708  0.988862     0.983151    0.970557   \n",
      "4   0.017835         0.015218  0.021477     0.000926    0.006009   \n",
      "5   0.070877         0.016505  0.027173     0.007373    0.024287   \n",
      "6   0.171855         0.085784  0.122289     0.024312    0.063687   \n",
      "7   0.520046         0.469583  0.211048     0.160340    0.233146   \n",
      "8   0.002963         0.002910  0.007363     0.000542    0.001619   \n",
      "9   0.103944         0.071557  0.023658     0.095975    0.099213   \n",
      "10  0.008613         0.017254  0.003800     0.002048    0.001591   \n",
      "11  0.030165         0.023461  0.025947     0.001437    0.022650   \n",
      "12  0.013072         0.022257  0.011142     0.002046    0.016138   \n",
      "13  0.049626         0.031662  0.033111     0.005497    0.031321   \n",
      "14  0.847292         0.157257  0.151897     0.011228    0.258776   \n",
      "15  0.048902         0.030923  0.026131     0.000853    0.033492   \n",
      "16  0.186282         0.063901  0.142495     0.022656    0.086124   \n",
      "17  0.351252         0.078841  0.059300     0.027468    0.124318   \n",
      "18  0.170611         0.016387  0.059255     0.002740    0.026174   \n",
      "19  0.020719         0.018954  0.027778     0.013681    0.046012   \n",
      "20  0.001097         0.001805  0.002547     0.000099    0.001617   \n",
      "\n",
      "    targeted_hate  \n",
      "0        0.000302  \n",
      "1        0.000315  \n",
      "2        0.009754  \n",
      "3        0.219366  \n",
      "4        0.000688  \n",
      "5        0.005946  \n",
      "6        0.013454  \n",
      "7        0.068176  \n",
      "8        0.000839  \n",
      "9        0.021685  \n",
      "10       0.000972  \n",
      "11       0.008834  \n",
      "12       0.004791  \n",
      "13       0.007136  \n",
      "14       0.013676  \n",
      "15       0.003700  \n",
      "16       0.024992  \n",
      "17       0.010744  \n",
      "18       0.005794  \n",
      "19       0.004771  \n",
      "20       0.000124  \n",
      "Final class probabilities for first 20 samples of test data:\n",
      "                      id     harsh  extremely_harsh    vulgar  threatening  \\\n",
      "0   e0ae9d9474a5689a5791  0.114300         0.111054  0.030741     0.009576   \n",
      "1   b64a191301cad4f11287  0.878644         0.021330  0.150911     0.001674   \n",
      "2   5e1953d9ae04bdc66408  0.752585         0.198251  0.289238     0.065422   \n",
      "3   23128f98196c8e8f7b90  0.036049         0.028677  0.035463     0.000881   \n",
      "4   2d3f1254f71472bf2b78  0.115389         0.053234  0.044174     0.002050   \n",
      "5   21f4f0f4812a08ea6c28  0.558266         0.009102  0.034602     0.001070   \n",
      "6   733b43d534c67c1be948  0.106373         0.021971  0.037154     0.000454   \n",
      "7   aad47a397f7ddc629d5d  0.022829         0.026193  0.020738     0.003228   \n",
      "8   d19fcde8a3af2e472d74  0.464352         0.070358  0.105931     0.003805   \n",
      "9   7d4de482c60f1c8a79c6  0.672760         0.535509  0.352830     0.038707   \n",
      "10  f81afe094bcd161ea6f8  0.043915         0.010272  0.016847     0.003256   \n",
      "11  132973e40fa63e0d07f1  0.015502         0.003486  0.005803     0.000461   \n",
      "12  37195759bf9104c3b488  0.020488         0.008977  0.014582     0.001783   \n",
      "13  0c46ec0e711469190af8  0.152299         0.094188  0.032304     0.006053   \n",
      "14  ecaeb00c751a61c1370e  0.271059         0.091548  0.605779     0.019628   \n",
      "15  dfaa434123c910477026  0.097490         0.038250  0.018307     0.006098   \n",
      "16  f0e8517a1d9e1394696b  0.081352         0.010242  0.052217     0.000636   \n",
      "17  db952db7fb786093a8f7  0.043268         0.052909  0.026080     0.007796   \n",
      "18  132ad2e7621d9c132c3f  0.004787         0.010355  0.006518     0.001107   \n",
      "19  38eef2c8994e8a980bb1  0.081012         0.097264  0.048699     0.001955   \n",
      "20  160a621ad6ee20736755  0.016757         0.006183  0.010362     0.000982   \n",
      "\n",
      "    disrespect  targeted_hate  \n",
      "0     0.046500       0.006895  \n",
      "1     0.375677       0.039255  \n",
      "2     0.472428       0.046706  \n",
      "3     0.020924       0.009934  \n",
      "4     0.028749       0.003067  \n",
      "5     0.060113       0.003419  \n",
      "6     0.063591       0.004335  \n",
      "7     0.011762       0.007854  \n",
      "8     0.097840       0.004345  \n",
      "9     0.297967       0.187389  \n",
      "10    0.035711       0.007265  \n",
      "11    0.006704       0.001483  \n",
      "12    0.021027       0.001756  \n",
      "13    0.087746       0.010778  \n",
      "14    0.129728       0.257691  \n",
      "15    0.010586       0.005370  \n",
      "16    0.020887       0.007805  \n",
      "17    0.025682       0.007920  \n",
      "18    0.008531       0.000867  \n",
      "19    0.082024       0.011408  \n",
      "20    0.011809       0.003837  \n"
     ]
    }
   ],
   "source": [
    "train_data = X_train_tfidf\n",
    "test_data = X_test_tfidf\n",
    "clf = LogisticRegClassifier(IP)\n",
    "csv_filename = \"LogisticReg_tfidf_Prob.csv\"\n",
    "model_name = \"Logistic Regression Classifier\"\n",
    "data_matrix_name = \"TF-IDF\"\n",
    "#execute(clf, train_data, test_data, model_name, data_matrix_name, csv_filename, \"WGS\")\n",
    "execute(clf, train_data, test_data, model_name, data_matrix_name, csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted model 0\n",
      "Fitted model 1\n",
      "Fitted model 2\n",
      "Fitted model 3\n",
      "Fitted model 4\n",
      "Fitted model 5\n",
      "Final class probabilities for first 20 samples of training data:\n",
      "           harsh  extremely_harsh        vulgar   threatening    disrespect  \\\n",
      "0   3.956165e-06     6.740222e-09  4.062802e-11  7.794363e-13  8.284751e-11   \n",
      "1   5.647224e-10     9.753947e-14  3.291531e-12  7.142455e-21  3.495606e-12   \n",
      "2   2.186373e-01     7.636043e-02  2.130762e-01  7.379742e-08  6.987392e-02   \n",
      "3   9.400171e-01     7.776725e-15  9.973338e-01  9.997194e-01  9.999984e-01   \n",
      "4   1.824230e-02     2.493654e-03  4.421334e-04  3.078966e-07  1.255613e-04   \n",
      "5   5.218986e-02     5.760664e-09  9.107303e-06  6.533016e-12  1.037154e-05   \n",
      "6   4.713945e-01     1.474039e-03  1.382751e-03  2.377995e-05  3.069225e-03   \n",
      "7   3.515649e-01     1.009173e-02  2.323673e-01  1.419913e-06  8.213935e-01   \n",
      "8   9.486263e-06     3.284787e-09  1.212063e-06  1.353371e-10  3.797951e-10   \n",
      "9   2.693962e-02     5.313875e-03  2.414825e-05  4.181402e-06  6.875073e-02   \n",
      "10  1.966367e-04     3.201485e-09  5.997992e-08  4.978359e-10  2.599720e-09   \n",
      "11  8.067554e-03     1.453654e-04  2.949078e-03  6.905717e-12  1.374945e-04   \n",
      "12  1.609932e-04     1.698026e-06  6.712296e-06  1.157013e-13  1.625271e-03   \n",
      "13  2.071894e-02     2.627869e-03  1.936094e-03  3.094228e-06  1.946748e-03   \n",
      "14  5.121190e-01     7.194499e-04  5.413419e-02  3.331571e-02  6.085903e-02   \n",
      "15  5.841688e-03     2.186723e-05  3.569003e-05  1.342960e-19  4.974810e-04   \n",
      "16  3.169357e-01     3.435766e-04  8.583367e-03  5.826229e-08  3.541301e-02   \n",
      "17  2.987338e-01     2.805190e-04  1.976348e-02  1.778680e-01  1.864774e-03   \n",
      "18  4.773991e-01     4.449868e-03  2.956687e-01  3.193352e-06  2.016884e-02   \n",
      "19  2.689795e-03     2.065721e-07  2.532309e-04  1.844622e-07  2.041798e-06   \n",
      "20  3.488568e-06     2.589310e-10  3.075411e-13  9.665960e-25  3.112949e-11   \n",
      "\n",
      "    targeted_hate  \n",
      "0    1.127364e-18  \n",
      "1    1.582610e-27  \n",
      "2    5.523098e-05  \n",
      "3    1.445016e-37  \n",
      "4    7.850417e-12  \n",
      "5    2.561723e-14  \n",
      "6    1.709444e-07  \n",
      "7    1.030220e-06  \n",
      "8    2.757001e-27  \n",
      "9    3.007961e-09  \n",
      "10   3.776616e-19  \n",
      "11   3.875794e-10  \n",
      "12   5.011112e-17  \n",
      "13   1.755945e-07  \n",
      "14   1.526897e-06  \n",
      "15   2.965786e-02  \n",
      "16   1.029823e-04  \n",
      "17   2.658823e-08  \n",
      "18   1.326411e-06  \n",
      "19   6.666952e-12  \n",
      "20   4.005640e-37  \n",
      "Final class probabilities for first 20 samples of test data:\n",
      "                      id         harsh  extremely_harsh        vulgar  \\\n",
      "0   e0ae9d9474a5689a5791  2.258154e-01     5.407466e-04  8.450670e-03   \n",
      "1   b64a191301cad4f11287  9.072759e-01     2.902256e-08  1.011447e-02   \n",
      "2   5e1953d9ae04bdc66408  4.903136e-01     1.264848e-02  2.697833e-01   \n",
      "3   23128f98196c8e8f7b90  7.006414e-06     1.834286e-06  3.343018e-07   \n",
      "4   2d3f1254f71472bf2b78  7.282759e-02     1.098243e-03  3.495018e-02   \n",
      "5   21f4f0f4812a08ea6c28  9.995499e-01     1.903282e-06  9.758325e-05   \n",
      "6   733b43d534c67c1be948  9.830837e-01     1.219510e-05  9.723982e-02   \n",
      "7   aad47a397f7ddc629d5d  1.124210e-01     3.223535e-02  6.822249e-03   \n",
      "8   d19fcde8a3af2e472d74  2.615043e-04     2.244179e-06  4.139375e-11   \n",
      "9   7d4de482c60f1c8a79c6  8.271715e-01     1.276995e-02  2.252435e-02   \n",
      "10  f81afe094bcd161ea6f8  2.776613e-03     7.802049e-06  3.022408e-10   \n",
      "11  132973e40fa63e0d07f1  2.774002e-02     9.378457e-12  7.349821e-08   \n",
      "12  37195759bf9104c3b488  7.689967e-09     3.460335e-11  1.313636e-14   \n",
      "13  0c46ec0e711469190af8  2.466818e-01     6.925579e-06  6.475516e-02   \n",
      "14  ecaeb00c751a61c1370e  5.651668e-01     7.326166e-07  8.169026e-04   \n",
      "15  dfaa434123c910477026  5.953861e-02     8.081579e-04  5.965722e-04   \n",
      "16  f0e8517a1d9e1394696b  9.685915e-01     1.697150e-04  1.766873e-02   \n",
      "17  db952db7fb786093a8f7  6.018634e-03     1.378661e-03  1.595438e-06   \n",
      "18  132ad2e7621d9c132c3f  1.960048e-03     2.989196e-04  1.148745e-07   \n",
      "19  38eef2c8994e8a980bb1  4.763559e-03     4.556616e-04  8.932126e-04   \n",
      "20  160a621ad6ee20736755  9.396463e-05     1.938272e-05  1.572766e-11   \n",
      "\n",
      "     threatening    disrespect  targeted_hate  \n",
      "0   2.300652e-05  4.066559e-02   1.089473e-03  \n",
      "1   5.976012e-30  3.753299e-03   4.913882e-07  \n",
      "2   2.964963e-07  9.281119e-01   2.508455e-05  \n",
      "3   3.413929e-19  6.166604e-11   3.430151e-22  \n",
      "4   6.829296e-04  3.827018e-03   9.997253e-05  \n",
      "5   3.335423e-17  1.025918e-01   3.504967e-13  \n",
      "6   2.845198e-22  5.867450e-02   3.520863e-08  \n",
      "7   2.831698e-08  2.770989e-02   3.435159e-03  \n",
      "8   4.036106e-11  8.079644e-13   5.455771e-30  \n",
      "9   7.586075e-05  9.529627e-02   5.045052e-03  \n",
      "10  1.922590e-07  7.405746e-08   3.144114e-21  \n",
      "11  9.241474e-20  2.346573e-05   4.867690e-27  \n",
      "12  2.274727e-08  2.779703e-15   1.923035e-66  \n",
      "13  3.364106e-08  5.503200e-02   1.573282e-09  \n",
      "14  5.677539e-01  4.697427e-01   3.034336e-01  \n",
      "15  8.380411e-05  1.317213e-04   1.708686e-06  \n",
      "16  9.543272e-14  4.388316e-01   5.786859e-11  \n",
      "17  1.305949e-07  1.963724e-05   3.380379e-10  \n",
      "18  1.005884e-13  5.590820e-09   2.246689e-10  \n",
      "19  7.994859e-09  2.968552e-04   2.847857e-07  \n",
      "20  1.392713e-20  5.932023e-11   1.783443e-22  \n"
     ]
    }
   ],
   "source": [
    "train_data = X_train_PV_DBOW\n",
    "test_data = X_test_PV_DBOW\n",
    "clf = LogisticRegClassifier(IP)\n",
    "csv_filename = \"LogisticReg_PV_DBOW_Prob.csv\"\n",
    "model_name = \"Logistic Regression Classifier\"\n",
    "data_matrix_name = \"Paragraph Vector - Distributed Bag Of Words\"\n",
    "execute(clf, train_data, test_data, model_name, data_matrix_name, csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted model 0\n",
      "Fitted model 1\n",
      "Fitted model 2\n",
      "Fitted model 3\n",
      "Fitted model 4\n",
      "Fitted model 5\n",
      "Final class probabilities for first 20 samples of training data:\n",
      "           harsh  extremely_harsh        vulgar   threatening    disrespect  \\\n",
      "0   9.378488e-03     3.978390e-12  5.306889e-07  3.003100e-14  2.035493e-09   \n",
      "1   2.481777e-08     2.460236e-27  2.110361e-22  1.106482e-56  6.781289e-18   \n",
      "2   6.863067e-01     3.690981e-01  1.220478e-01  1.036114e-08  1.595909e-01   \n",
      "3   9.998975e-01     1.903694e-11  9.999564e-01  9.993975e-01  9.999871e-01   \n",
      "4   1.559914e-02     8.459822e-06  1.729012e-02  3.137675e-07  1.237170e-04   \n",
      "5   2.124660e-03     3.719278e-10  3.801067e-07  3.000145e-04  2.069032e-05   \n",
      "6   7.876087e-01     8.534242e-02  5.248414e-01  4.790839e-03  8.525118e-01   \n",
      "7   5.933820e-01     4.700247e-02  1.486934e-01  1.717517e-02  1.897644e-01   \n",
      "8   2.037172e-02     2.217997e-04  6.088564e-03  3.100562e-07  4.092175e-03   \n",
      "9   8.180046e-01     1.246910e-02  1.744899e-01  6.882833e-02  5.290611e-02   \n",
      "10  5.866584e-02     8.552549e-09  2.717535e-01  7.181625e-17  9.727988e-05   \n",
      "11  2.705852e-01     7.337333e-01  2.534021e-01  2.027638e-02  2.582314e-01   \n",
      "12  4.897891e-03     8.749055e-03  1.691828e-04  4.701730e-15  3.089905e-04   \n",
      "13  6.042211e-03     1.554299e-01  5.608546e-03  1.141898e-05  5.064992e-04   \n",
      "14  6.511210e-01     4.294564e-01  4.882264e-01  8.987885e-05  5.147680e-01   \n",
      "15  8.088559e-03     5.606748e-05  1.654897e-03  9.061677e-12  3.591546e-03   \n",
      "16  7.305818e-01     3.580399e-04  3.705210e-02  6.598663e-07  5.088748e-02   \n",
      "17  9.480548e-01     2.897742e-02  8.454592e-01  1.140207e-01  5.612669e-01   \n",
      "18  3.177022e-01     4.725181e-02  5.903713e-02  1.350101e-01  1.176936e-01   \n",
      "19  1.546776e-01     6.449063e-03  2.600027e-02  4.759818e-05  5.658740e-03   \n",
      "20  8.967458e-03     6.356659e-07  6.265966e-04  3.257444e-16  9.861711e-07   \n",
      "\n",
      "    targeted_hate  \n",
      "0    7.894924e-43  \n",
      "1    4.494595e-72  \n",
      "2    7.189805e-09  \n",
      "3    1.839716e-38  \n",
      "4    7.946798e-09  \n",
      "5    2.746839e-17  \n",
      "6    8.487090e-03  \n",
      "7    1.727342e-07  \n",
      "8    3.787730e-05  \n",
      "9    4.253687e-07  \n",
      "10   3.621348e-11  \n",
      "11   1.427869e-03  \n",
      "12   4.039814e-05  \n",
      "13   2.777835e-11  \n",
      "14   3.763284e-02  \n",
      "15   1.203973e-15  \n",
      "16   3.970743e-06  \n",
      "17   2.708341e-02  \n",
      "18   1.585969e-02  \n",
      "19   1.464112e-06  \n",
      "20   1.841081e-17  \n",
      "Final class probabilities for first 20 samples of test data:\n",
      "                      id         harsh  extremely_harsh        vulgar  \\\n",
      "0   e0ae9d9474a5689a5791  5.827372e-02     1.238197e-04  2.414113e-03   \n",
      "1   b64a191301cad4f11287  5.090689e-01     1.193602e-06  5.219962e-04   \n",
      "2   5e1953d9ae04bdc66408  4.256005e-01     5.002904e-02  7.197113e-01   \n",
      "3   23128f98196c8e8f7b90  5.002817e-04     8.180782e-11  4.203244e-08   \n",
      "4   2d3f1254f71472bf2b78  1.291892e-01     2.754452e-03  3.265618e-03   \n",
      "5   21f4f0f4812a08ea6c28  3.346792e-03     8.816337e-17  6.330358e-14   \n",
      "6   733b43d534c67c1be948  3.469262e-02     7.304058e-10  7.009578e-03   \n",
      "7   aad47a397f7ddc629d5d  3.675804e-02     1.831864e-03  3.478485e-03   \n",
      "8   d19fcde8a3af2e472d74  7.886834e-01     1.614875e-03  3.039462e-01   \n",
      "9   7d4de482c60f1c8a79c6  3.555340e-01     1.990754e-03  6.026928e-02   \n",
      "10  f81afe094bcd161ea6f8  2.752971e-03     1.569889e-06  3.630479e-02   \n",
      "11  132973e40fa63e0d07f1  9.007568e-03     1.040176e-10  1.047475e-04   \n",
      "12  37195759bf9104c3b488  2.628981e-09     2.971770e-20  4.332900e-12   \n",
      "13  0c46ec0e711469190af8  9.942108e-02     1.837244e-04  1.353957e-03   \n",
      "14  ecaeb00c751a61c1370e  9.096482e-01     8.122272e-02  9.975064e-01   \n",
      "15  dfaa434123c910477026  6.045901e-02     2.174695e-03  4.439989e-03   \n",
      "16  f0e8517a1d9e1394696b  6.062190e-04     2.537854e-13  2.492536e-05   \n",
      "17  db952db7fb786093a8f7  4.291943e-02     1.400785e-03  2.444449e-03   \n",
      "18  132ad2e7621d9c132c3f  1.996010e-01     2.926866e-03  6.731003e-02   \n",
      "19  38eef2c8994e8a980bb1  3.916389e-03     4.924522e-04  1.178300e-04   \n",
      "20  160a621ad6ee20736755  2.276023e-04     1.061903e-10  1.398748e-09   \n",
      "\n",
      "     threatening    disrespect  targeted_hate  \n",
      "0   9.837164e-12  3.228213e-04   9.938903e-10  \n",
      "1   9.355703e-24  1.140733e-03   5.900166e-08  \n",
      "2   5.147349e-08  3.159886e-01   1.592146e-05  \n",
      "3   4.566576e-31  2.305053e-10   9.440427e-30  \n",
      "4   2.060887e-12  1.628823e-03   1.176713e-05  \n",
      "5   2.922182e-63  2.532999e-19   1.717908e-23  \n",
      "6   1.706635e-15  1.170474e-05   7.184179e-18  \n",
      "7   1.328338e-09  1.424590e-04   1.748923e-08  \n",
      "8   1.905285e-19  2.278251e-01   6.783339e-16  \n",
      "9   2.636779e-08  1.812716e-03   7.692525e-05  \n",
      "10  3.849227e-17  2.360675e-03   7.562491e-12  \n",
      "11  4.443363e-20  3.544092e-12   2.086195e-17  \n",
      "12  8.535864e-23  1.855255e-15   8.788396e-65  \n",
      "13  3.333658e-13  2.739477e-05   8.291720e-09  \n",
      "14  4.947915e-10  6.821087e-01   9.998991e-01  \n",
      "15  9.440032e-10  1.802012e-04   2.363734e-10  \n",
      "16  4.939103e-21  7.386713e-09   1.419674e-17  \n",
      "17  1.363079e-10  3.585932e-05   1.008457e-09  \n",
      "18  6.785153e-10  1.086755e-03   6.248532e-09  \n",
      "19  1.937597e-13  7.571559e-04   1.077359e-04  \n",
      "20  3.023429e-18  6.131206e-11   9.364026e-17  \n"
     ]
    }
   ],
   "source": [
    "train_data = X_train_PV_DM\n",
    "test_data = X_test_PV_DM\n",
    "clf = LogisticRegClassifier(IP)\n",
    "csv_filename = \"LogisticReg_PV_DM_Prob.csv\"\n",
    "model_name = \"Logistic Regression Classifier\"\n",
    "data_matrix_name = \"Paragraph Vector - Distributed Memory\"\n",
    "execute(clf, train_data, test_data, model_name, data_matrix_name, csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Logistic Regression Classifier\n",
      "Data matrix: Word2Vec Method_1\n",
      "\n",
      "Fitted model 0\n",
      "Fitted model 1\n",
      "Fitted model 2\n",
      "Fitted model 3\n",
      "Fitted model 4\n",
      "Fitted model 5\n",
      "ROC_AUC scores for the test split:\n",
      "ROC_AUC score for harsh: 0.9363846406347226\n",
      "ROC_AUC score for extremely_harsh: 0.9466765490110498\n",
      "ROC_AUC score for vulgar: 0.9481783261118517\n",
      "ROC_AUC score for threatening: 0.9171357458872262\n",
      "ROC_AUC score for disrespect: 0.9359044178138796\n",
      "ROC_AUC score for targeted_hate: 0.8881044325277389\n",
      "\n",
      "Mean ROC_AUC_score: 0.9287306853310783\n",
      "\n",
      "Accuracy and f1 scores for the entire training data:\n",
      "Accuracy score for harsh: 0.8975293641150264\n",
      "F1 score for harsh: 0.6263537906137184\n",
      "Accuracy score for extremely_harsh: 0.9812114666306647\n",
      "F1 score for extremely_harsh: 0.5038621509209745\n",
      "Accuracy score for vulgar: 0.9383353584447145\n",
      "F1 score for vulgar: 0.6184476157326836\n",
      "Accuracy score for threatening: 0.9978623824310338\n",
      "F1 score for threatening: 0.7155688622754491\n",
      "Accuracy score for disrespect: 0.9261509383016066\n",
      "F1 score for disrespect: 0.5555254604550379\n",
      "Accuracy score for targeted_hate: 0.9837766077134242\n",
      "F1 score for targeted_hate: 0.49261083743842365\n",
      "\n",
      "Final class probabilities for first 20 samples of training data:\n",
      "       harsh  extremely_harsh        vulgar   threatening  disrespect  \\\n",
      "0   0.000834     1.521812e-07  1.794704e-03  3.708409e-14    0.000006   \n",
      "1   0.000065     5.154544e-04  2.182977e-03  2.756504e-02    0.001723   \n",
      "2   0.959278     1.230168e-01  7.511596e-01  2.614448e-07    0.730923   \n",
      "3   0.674811     1.182624e-03  8.777568e-01  9.991777e-01    0.818664   \n",
      "4   0.020572     1.358460e-08  1.768028e-03  7.177258e-03    0.000582   \n",
      "5   0.022735     1.017645e-08  2.481118e-03  9.769812e-08    0.019991   \n",
      "6   0.842255     1.336328e-03  7.461985e-01  3.738932e-06    0.127446   \n",
      "7   0.093014     4.836120e-06  1.677453e-02  1.915580e-08    0.014556   \n",
      "8   0.005322     5.004737e-08  1.885218e-01  3.365399e-12    0.009335   \n",
      "9   0.189443     4.593267e-08  8.682203e-03  6.771502e-08    0.238998   \n",
      "10  0.018634     2.626533e-06  6.154423e-04  1.947600e-06    0.000252   \n",
      "11  0.634556     1.433959e-01  5.088266e-01  1.580919e-08    0.796119   \n",
      "12  0.023547     3.104699e-05  5.560583e-05  4.524725e-13    0.000216   \n",
      "13  0.122296     2.236535e-07  9.459493e-05  1.648786e-07    0.001430   \n",
      "14  0.633615     8.586324e-05  1.137225e-02  2.046186e-08    0.351860   \n",
      "15  0.014819     2.896984e-10  4.058468e-03  1.264498e-09    0.000333   \n",
      "16  0.409309     6.805062e-06  2.478320e-01  7.153814e-06    0.019323   \n",
      "17  0.564187     8.013701e-03  3.469681e-01  1.683201e-02    0.226446   \n",
      "18  0.546113     3.908202e-03  9.695642e-02  1.064189e-05    0.019864   \n",
      "19  0.003976     6.716015e-04  1.076870e-03  6.023155e-05    0.013844   \n",
      "20  0.000091     6.288774e-05  6.490573e-07  2.956781e-05    0.000005   \n",
      "\n",
      "    targeted_hate  \n",
      "0    3.278655e-23  \n",
      "1    3.811635e-19  \n",
      "2    3.797810e-01  \n",
      "3    6.683461e-21  \n",
      "4    3.331667e-14  \n",
      "5    1.145766e-05  \n",
      "6    1.025846e-05  \n",
      "7    5.460389e-08  \n",
      "8    1.139188e-12  \n",
      "9    1.646115e-10  \n",
      "10   1.129709e-17  \n",
      "11   4.948762e-08  \n",
      "12   1.290446e-06  \n",
      "13   2.124547e-11  \n",
      "14   8.996657e-06  \n",
      "15   1.064110e-13  \n",
      "16   6.538690e-03  \n",
      "17   2.502258e-03  \n",
      "18   1.022506e-13  \n",
      "19   8.615637e-17  \n",
      "20   8.492884e-34  \n",
      "Final class probabilities for first 20 samples of test data:\n",
      "                      id     harsh  extremely_harsh    vulgar   threatening  \\\n",
      "0   e0ae9d9474a5689a5791  0.056032     7.398014e-06  0.000004  2.130659e-16   \n",
      "1   b64a191301cad4f11287  0.646172     2.298883e-05  0.000601  1.811738e-08   \n",
      "2   5e1953d9ae04bdc66408  0.336983     2.858642e-02  0.117879  1.987363e-04   \n",
      "3   23128f98196c8e8f7b90  0.007239     1.434312e-07  0.000026  3.002831e-11   \n",
      "4   2d3f1254f71472bf2b78  0.086707     5.409595e-07  0.000118  2.000775e-07   \n",
      "5   21f4f0f4812a08ea6c28  0.393838     7.739344e-07  0.000093  6.019437e-14   \n",
      "6   733b43d534c67c1be948  0.199027     2.209461e-06  0.049362  1.715276e-08   \n",
      "7   aad47a397f7ddc629d5d  0.003021     1.825266e-08  0.052786  7.129586e-13   \n",
      "8   d19fcde8a3af2e472d74  0.003922     1.235565e-06  0.000275  8.753799e-19   \n",
      "9   7d4de482c60f1c8a79c6  0.998795     9.004679e-01  0.939906  1.633266e-10   \n",
      "10  f81afe094bcd161ea6f8  0.370457     2.823355e-05  0.028890  1.734561e-06   \n",
      "11  132973e40fa63e0d07f1  0.003347     1.499435e-08  0.001034  1.537810e-06   \n",
      "12  37195759bf9104c3b488  0.069880     1.120658e-05  0.000025  2.071795e-09   \n",
      "13  0c46ec0e711469190af8  0.213903     1.299266e-04  0.132525  1.589858e-09   \n",
      "14  ecaeb00c751a61c1370e  0.429327     1.556571e-06  0.000242  1.645906e-07   \n",
      "15  dfaa434123c910477026  0.021059     4.696609e-08  0.000016  2.777254e-12   \n",
      "16  f0e8517a1d9e1394696b  0.289617     3.402487e-05  0.446912  4.186497e-07   \n",
      "17  db952db7fb786093a8f7  0.018805     7.395874e-02  0.000732  2.636153e-08   \n",
      "18  132ad2e7621d9c132c3f  0.000253     8.530022e-07  0.005166  1.688322e-08   \n",
      "19  38eef2c8994e8a980bb1  0.021253     2.730986e-03  0.012544  1.029282e-07   \n",
      "20  160a621ad6ee20736755  0.008256     1.783271e-05  0.000897  1.840849e-08   \n",
      "\n",
      "    disrespect  targeted_hate  \n",
      "0     0.002915   6.630954e-20  \n",
      "1     0.145762   1.488819e-03  \n",
      "2     0.180756   1.653474e-04  \n",
      "3     0.000010   1.805688e-13  \n",
      "4     0.000013   4.596011e-16  \n",
      "5     0.003320   1.654636e-17  \n",
      "6     0.021198   4.045763e-20  \n",
      "7     0.000051   1.936105e-21  \n",
      "8     0.000405   1.303130e-23  \n",
      "9     0.999878   4.572246e-13  \n",
      "10    0.003095   1.229174e-10  \n",
      "11    0.000020   2.343512e-13  \n",
      "12    0.000482   7.897027e-11  \n",
      "13    0.002334   9.563640e-11  \n",
      "14    0.020149   3.184268e-07  \n",
      "15    0.000057   6.011731e-13  \n",
      "16    0.151656   6.387138e-12  \n",
      "17    0.000884   1.746464e-13  \n",
      "18    0.011052   5.323289e-09  \n",
      "19    0.007510   3.304599e-09  \n",
      "20    0.000038   9.008729e-05  \n"
     ]
    }
   ],
   "source": [
    "train_data = X_train_W2V_method_1\n",
    "test_data = X_test_W2V_method_1\n",
    "clf = LogisticRegClassifier(IP)\n",
    "csv_filename = \"LogisticReg_W2V_method_1_Prob.csv\"\n",
    "model_name = \"Logistic Regression Classifier\"\n",
    "data_matrix_name = \"Word2Vec Method_1\"\n",
    "execute(clf, train_data, test_data, model_name, data_matrix_name, csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: Currently this can't be done as dimensions of X_train_PV_DBOW and X_train_tfidf don't match\n",
    "\n",
    "# train_data = X_train_PV_DBOW + X_train_tfidf\n",
    "# test_data = X_test_PV_DBOW + X_test_tfidf\n",
    "# clf = LogisticRegClassifier(IP)\n",
    "# csv_filename = \"LogisticReg_PV_DBOW_tfidf_Prob.csv\"\n",
    "# model_name = \"Logistic Regression Classifier\"\n",
    "# data_matrix_name = \"Combination of PV_DBOW and TF-IDF\"\n",
    "# execute(clf, train_data, test_data, model_name, data_matrix_name, csv_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.03 Gaussian Naive Bayes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "class GNBClassifier(object): #Pack of 6 binary Gaussian naive bayes classifiers\n",
    "    def __init__(self):\n",
    "        self.param_grid = {\n",
    "            'var_smoothing' : [1e-9] #default=1e-9\n",
    "        }\n",
    "        self.models = []\n",
    "        \n",
    "        for i in range(6): #Appending 6 binary Gaussian naive bayes classifiers (one for each class)\n",
    "            self.models.append(GaussianNB())\n",
    "    \n",
    "    def fit(self, XTrain, yTrain, CVScoreType, numFolds): #Fits the models and returns the best CVScores and CVParams after performing grid search CV\n",
    "        CVScores = {}\n",
    "        CVParams = {}\n",
    "        \n",
    "        for i, model in enumerate(self.models):\n",
    "            labels = yTrain[label_type[i]]\n",
    "            \n",
    "            bestParams, bestScore = performGridSearchCV(model, self.param_grid, XTrain, labels, numFolds, CVScoreType)\n",
    "            CVParams[label_type[i]] = bestParams\n",
    "            CVScores[label_type[i]] = bestScore\n",
    "            \n",
    "            self.models[i] = GaussianNB(var_smoothing=bestParams['var_smoothing'])\n",
    "            self.models[i].fit(XTrain, labels)\n",
    "            print(\"Fitted model \"+str(i))\n",
    "            \n",
    "        return CVScores, CVParams\n",
    "    \n",
    "    def fitWithoutGridSearch(self, XTrain, yTrain): #Fits the models without performing grid search CV\n",
    "        for i, model in enumerate(self.models):\n",
    "            labels = yTrain[label_type[i]]\n",
    "            self.models[i].fit(XTrain, labels)\n",
    "            print(\"Fitted model \"+str(i))\n",
    "    \n",
    "    def predict(self, X): #Returns the binary classification results for each of the models\n",
    "        predictions = {}\n",
    "        for i, model in enumerate(self.models):\n",
    "            predictions[label_type[i]] = model.predict(X)\n",
    "        return predictions\n",
    "    \n",
    "    def predictProbabilities(self, X): #Returns the predicted class probabilities for each of the classes\n",
    "        probabilities = {}\n",
    "        for i, model in enumerate(self.models):\n",
    "            classes = model.classes_\n",
    "            index = np.where(classes == 1)[0][0] #Getting the index of class 1\n",
    "            probabilities[label_type[i]] = model.predict_proba(X)[:, index] #Extracting the probabilities of class 1\n",
    "        return probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now testing out the Gaussian naive bayes classifier on various data matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: Currently this can't be done... Resulting in kernel crash due to large dimensions\n",
    "\n",
    "# train_data = X_train_tfidf.toarray()\n",
    "# test_data = X_test_tfidf.toarray()\n",
    "# clf = GNBClassifier()\n",
    "# csv_filename = \"GNBC_tfidf_Prob.csv\"\n",
    "# model_name = \"Gaussian Naive Bayes Classifier\"\n",
    "# data_matrix_name = \"TF-IDF\"\n",
    "# execute(clf, train_data, test_data, model_name, data_matrix_name, csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: Currently this can't be done... Resulting in kernel crash due to large dimensions\n",
    "\n",
    "# train_data = X_train_bow.toarray()\n",
    "# test_data = X_test_bow.toarray()\n",
    "# clf = GNBClassifier()\n",
    "# csv_filename = \"GNBC_bow_Prob.csv\"\n",
    "# model_name = \"Gaussian Naive Bayes Classifier\"\n",
    "# data_matrix_name = \"Bag of Words\"\n",
    "# execute(clf, train_data, test_data, model_name, data_matrix_name, csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted model 0\n",
      "Fitted model 1\n",
      "Fitted model 2\n",
      "Fitted model 3\n",
      "Fitted model 4\n",
      "Fitted model 5\n",
      "Final class probabilities for first 20 samples of training data:\n",
      "            harsh  extremely_harsh         vulgar    threatening  \\\n",
      "0    0.000000e+00     0.000000e+00   0.000000e+00   0.000000e+00   \n",
      "1    0.000000e+00     0.000000e+00   0.000000e+00   0.000000e+00   \n",
      "2    1.000000e+00     1.000000e+00   1.000000e+00   1.000000e+00   \n",
      "3    0.000000e+00     0.000000e+00   0.000000e+00   2.572441e-23   \n",
      "4    1.000000e+00     1.000000e+00   1.000000e+00   4.854296e-43   \n",
      "5   2.034221e-153     0.000000e+00  1.789087e-209   0.000000e+00   \n",
      "6    1.000000e+00     1.000000e+00   1.000000e+00   1.000000e+00   \n",
      "7    1.000000e+00     1.000000e+00   1.000000e+00   1.000000e+00   \n",
      "8    1.000000e+00     1.000000e+00   1.000000e+00   1.000000e+00   \n",
      "9    1.000000e+00     1.000000e+00   1.000000e+00   1.000000e+00   \n",
      "10  1.088240e-184    5.591118e-184  1.273466e-189   0.000000e+00   \n",
      "11   1.000000e+00     1.000000e+00   1.000000e+00   1.000000e+00   \n",
      "12   9.213107e-09     1.544722e-60   7.390639e-23  4.940656e-324   \n",
      "13   1.000000e+00     1.000000e+00   1.000000e+00   1.000000e+00   \n",
      "14   1.000000e+00     1.000000e+00   1.000000e+00   1.000000e+00   \n",
      "15   1.000000e+00     1.632051e-50   1.000000e+00   4.726077e-02   \n",
      "16   1.000000e+00     1.000000e+00   1.000000e+00   1.000000e+00   \n",
      "17   1.000000e+00     1.000000e+00   1.000000e+00   1.000000e+00   \n",
      "18   1.000000e+00     1.000000e+00   1.000000e+00   1.000000e+00   \n",
      "19   1.000000e+00     1.000000e+00   1.000000e+00   1.000000e+00   \n",
      "20   1.533241e-62     3.297496e-36   4.081298e-69   0.000000e+00   \n",
      "\n",
      "       disrespect  targeted_hate  \n",
      "0    0.000000e+00   0.000000e+00  \n",
      "1    0.000000e+00   0.000000e+00  \n",
      "2    1.000000e+00   1.000000e+00  \n",
      "3    0.000000e+00   0.000000e+00  \n",
      "4    1.000000e+00   1.000000e+00  \n",
      "5   2.170233e-220  1.419260e-158  \n",
      "6    1.000000e+00   1.000000e+00  \n",
      "7    1.000000e+00   1.000000e+00  \n",
      "8    1.000000e+00   1.000000e+00  \n",
      "9    1.000000e+00   1.000000e+00  \n",
      "10  4.179038e-247  5.507632e-150  \n",
      "11   1.000000e+00   1.000000e+00  \n",
      "12   1.574493e-26   1.134485e-89  \n",
      "13   1.000000e+00   1.000000e+00  \n",
      "14   1.000000e+00   1.000000e+00  \n",
      "15   1.000000e+00   0.000000e+00  \n",
      "16   1.000000e+00   1.000000e+00  \n",
      "17   1.000000e+00   1.000000e+00  \n",
      "18   1.000000e+00   1.000000e+00  \n",
      "19   1.000000e+00   1.000000e+00  \n",
      "20   1.806386e-73   9.647471e-78  \n",
      "Final class probabilities for first 20 samples of test data:\n",
      "                      id          harsh  extremely_harsh         vulgar  \\\n",
      "0   e0ae9d9474a5689a5791   1.000000e+00     1.000000e+00   1.000000e+00   \n",
      "1   b64a191301cad4f11287   2.587127e-96    6.446818e-183  8.062228e-137   \n",
      "2   5e1953d9ae04bdc66408   1.000000e+00     1.000000e+00   1.000000e+00   \n",
      "3   23128f98196c8e8f7b90  4.270022e-279     0.000000e+00   0.000000e+00   \n",
      "4   2d3f1254f71472bf2b78   1.000000e+00     1.000000e+00   1.000000e+00   \n",
      "5   21f4f0f4812a08ea6c28   0.000000e+00     0.000000e+00   0.000000e+00   \n",
      "6   733b43d534c67c1be948  1.968695e-192     0.000000e+00  7.742122e-245   \n",
      "7   aad47a397f7ddc629d5d   1.000000e+00     1.000000e+00   1.000000e+00   \n",
      "8   d19fcde8a3af2e472d74   3.773924e-05     1.830241e-85   1.727206e-26   \n",
      "9   7d4de482c60f1c8a79c6   1.000000e+00     1.000000e+00   1.000000e+00   \n",
      "10  f81afe094bcd161ea6f8   1.000000e+00     1.000000e+00   1.000000e+00   \n",
      "11  132973e40fa63e0d07f1  3.836827e-223     0.000000e+00  1.705029e-283   \n",
      "12  37195759bf9104c3b488   0.000000e+00     0.000000e+00   0.000000e+00   \n",
      "13  0c46ec0e711469190af8   1.000000e+00     1.000000e+00   1.000000e+00   \n",
      "14  ecaeb00c751a61c1370e   1.000000e+00     1.000000e+00   1.000000e+00   \n",
      "15  dfaa434123c910477026   1.000000e+00     1.000000e+00   1.000000e+00   \n",
      "16  f0e8517a1d9e1394696b   0.000000e+00     0.000000e+00   0.000000e+00   \n",
      "17  db952db7fb786093a8f7   1.000000e+00     1.000000e+00   1.000000e+00   \n",
      "18  132ad2e7621d9c132c3f   1.000000e+00     1.000000e+00   1.000000e+00   \n",
      "19  38eef2c8994e8a980bb1   1.000000e+00    2.338815e-247   1.000000e+00   \n",
      "20  160a621ad6ee20736755   5.369672e-31     7.708928e-77   9.991353e-56   \n",
      "\n",
      "      threatening     disrespect  targeted_hate  \n",
      "0    1.000000e+00   1.000000e+00   1.000000e+00  \n",
      "1    0.000000e+00  1.226547e-159   1.518408e-74  \n",
      "2    1.000000e+00   1.000000e+00   1.000000e+00  \n",
      "3    0.000000e+00   0.000000e+00   0.000000e+00  \n",
      "4    1.000000e+00   1.000000e+00   1.000000e+00  \n",
      "5    0.000000e+00   0.000000e+00   0.000000e+00  \n",
      "6    0.000000e+00  2.026437e-268   0.000000e+00  \n",
      "7    1.000000e+00   1.000000e+00   1.000000e+00  \n",
      "8   4.205388e-227   9.333855e-33   3.230480e-13  \n",
      "9    1.000000e+00   1.000000e+00   1.000000e+00  \n",
      "10   9.283491e-01   1.000000e+00   1.000000e+00  \n",
      "11   0.000000e+00  4.249312e-306   0.000000e+00  \n",
      "12   0.000000e+00   0.000000e+00   0.000000e+00  \n",
      "13   1.000000e+00   1.000000e+00   1.000000e+00  \n",
      "14   4.154082e-19   1.000000e+00   1.000000e+00  \n",
      "15   1.000000e+00   1.000000e+00   1.000000e+00  \n",
      "16   0.000000e+00   0.000000e+00   0.000000e+00  \n",
      "17   1.000000e+00   1.000000e+00   1.000000e+00  \n",
      "18   1.000000e+00   1.000000e+00   1.000000e+00  \n",
      "19   9.999998e-01   1.000000e+00   1.000000e+00  \n",
      "20   0.000000e+00   1.869206e-54   4.626601e-67  \n"
     ]
    }
   ],
   "source": [
    "train_data = X_train_PV_DM.toarray()\n",
    "test_data = X_test_PV_DM.toarray()\n",
    "clf = GNBClassifier()\n",
    "csv_filename = \"GNBC_PV_DM_Prob.csv\"\n",
    "model_name = \"Gaussian Naive Bayes Classifier\"\n",
    "data_matrix_name = \"Paragraph Vector - Distributed Memory\"\n",
    "execute(clf, train_data, test_data, model_name, data_matrix_name, csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted model 0\n",
      "Fitted model 1\n",
      "Fitted model 2\n",
      "Fitted model 3\n",
      "Fitted model 4\n",
      "Fitted model 5\n",
      "Final class probabilities for first 20 samples of training data:\n",
      "            harsh  extremely_harsh         vulgar    threatening  \\\n",
      "0    0.000000e+00     0.000000e+00   0.000000e+00   0.000000e+00   \n",
      "1    0.000000e+00     0.000000e+00   0.000000e+00   0.000000e+00   \n",
      "2    1.000000e+00     1.000000e+00   1.000000e+00   1.000000e+00   \n",
      "3   6.104054e-188     0.000000e+00  2.687508e-316   1.000000e+00   \n",
      "4    1.000000e+00     1.000000e+00   1.000000e+00   1.000000e+00   \n",
      "5   3.007471e-183    1.204468e-295  1.502833e-229   0.000000e+00   \n",
      "6    1.000000e+00     9.999909e-01   1.000000e+00   1.000000e+00   \n",
      "7    1.000000e+00     1.000000e+00   1.000000e+00   1.000000e+00   \n",
      "8   1.513726e-269    4.511501e-315  1.412330e-304   0.000000e+00   \n",
      "9    1.000000e+00     3.161197e-27   1.000000e+00  1.295772e-280   \n",
      "10  3.224498e-212     0.000000e+00  8.121834e-277   0.000000e+00   \n",
      "11   3.377494e-65    8.418707e-169   4.239763e-87   0.000000e+00   \n",
      "12   2.563125e-51     0.000000e+00   6.099316e-82   0.000000e+00   \n",
      "13   6.711426e-57    3.697286e-106   1.499938e-65   0.000000e+00   \n",
      "14   1.000000e+00     1.000000e+00   1.000000e+00   1.000000e+00   \n",
      "15   2.154205e-36     9.875239e-25   2.726397e-28   0.000000e+00   \n",
      "16   1.000000e+00     1.000000e+00   1.000000e+00   1.000000e+00   \n",
      "17   1.000000e+00     1.000000e+00   1.000000e+00   1.000000e+00   \n",
      "18   1.000000e+00     1.000000e+00   1.000000e+00   1.000000e+00   \n",
      "19   8.625387e-89     5.352960e-83   4.529495e-65  2.996568e-129   \n",
      "20   0.000000e+00     0.000000e+00   0.000000e+00   0.000000e+00   \n",
      "\n",
      "       disrespect  targeted_hate  \n",
      "0    0.000000e+00   0.000000e+00  \n",
      "1    0.000000e+00   0.000000e+00  \n",
      "2    1.000000e+00   1.000000e+00  \n",
      "3    0.000000e+00   0.000000e+00  \n",
      "4    1.000000e+00   9.992075e-01  \n",
      "5   5.819925e-294   0.000000e+00  \n",
      "6    1.000000e+00   1.000000e+00  \n",
      "7    1.000000e+00   1.000000e+00  \n",
      "8    0.000000e+00   0.000000e+00  \n",
      "9    1.000000e+00   9.315850e-21  \n",
      "10  1.093994e-284  2.483312e-318  \n",
      "11   1.098230e-98   0.000000e+00  \n",
      "12   4.367651e-70  3.247043e-145  \n",
      "13   8.095148e-75   2.419333e-84  \n",
      "14   1.000000e+00   1.000000e+00  \n",
      "15   9.478054e-37   8.335360e-57  \n",
      "16   1.000000e+00   1.000000e+00  \n",
      "17   1.000000e+00   1.000000e+00  \n",
      "18   1.000000e+00   1.000000e+00  \n",
      "19   9.073667e-98  1.858357e-182  \n",
      "20   0.000000e+00   0.000000e+00  \n",
      "Final class probabilities for first 20 samples of test data:\n",
      "                      id          harsh  extremely_harsh         vulgar  \\\n",
      "0   e0ae9d9474a5689a5791   1.000000e+00     1.000000e+00   1.000000e+00   \n",
      "1   b64a191301cad4f11287   1.000000e+00    4.423344e-105   6.757453e-02   \n",
      "2   5e1953d9ae04bdc66408   1.000000e+00     1.000000e+00   1.000000e+00   \n",
      "3   23128f98196c8e8f7b90  1.825818e-241     0.000000e+00  1.991502e-299   \n",
      "4   2d3f1254f71472bf2b78   1.000000e+00     1.000000e+00   1.000000e+00   \n",
      "5   21f4f0f4812a08ea6c28   0.000000e+00     0.000000e+00   0.000000e+00   \n",
      "6   733b43d534c67c1be948   9.947273e-85    7.145006e-300  7.568695e-138   \n",
      "7   aad47a397f7ddc629d5d   1.000000e+00     1.000000e+00   1.000000e+00   \n",
      "8   d19fcde8a3af2e472d74  3.822214e-181     0.000000e+00  1.352549e-250   \n",
      "9   7d4de482c60f1c8a79c6   1.000000e+00     1.000000e+00   1.000000e+00   \n",
      "10  f81afe094bcd161ea6f8   3.891540e-79    3.411434e-256  1.402547e-130   \n",
      "11  132973e40fa63e0d07f1   0.000000e+00     0.000000e+00   0.000000e+00   \n",
      "12  37195759bf9104c3b488   0.000000e+00     0.000000e+00   0.000000e+00   \n",
      "13  0c46ec0e711469190af8   1.000000e+00     1.000000e+00   1.000000e+00   \n",
      "14  ecaeb00c751a61c1370e   5.980181e-65    1.577296e-288  1.356498e-130   \n",
      "15  dfaa434123c910477026   1.000000e+00     2.337515e-07   1.000000e+00   \n",
      "16  f0e8517a1d9e1394696b   1.353307e-25    3.755392e-150   9.418513e-63   \n",
      "17  db952db7fb786093a8f7   1.000000e+00     2.951824e-14   1.000000e+00   \n",
      "18  132ad2e7621d9c132c3f  5.481201e-233    5.848760e-133  2.304201e-297   \n",
      "19  38eef2c8994e8a980bb1   1.000000e+00     1.000000e+00   1.000000e+00   \n",
      "20  160a621ad6ee20736755  2.532144e-228     0.000000e+00  2.162819e-302   \n",
      "\n",
      "      threatening     disrespect  targeted_hate  \n",
      "0    1.000000e+00   1.000000e+00   1.000000e+00  \n",
      "1    0.000000e+00   9.773552e-09   4.449074e-57  \n",
      "2    1.000000e+00   1.000000e+00   1.000000e+00  \n",
      "3    0.000000e+00  4.701469e-312  8.360374e-316  \n",
      "4    1.000000e+00   1.000000e+00   1.000000e+00  \n",
      "5    0.000000e+00   0.000000e+00   0.000000e+00  \n",
      "6    0.000000e+00  5.713173e-167  4.439269e-175  \n",
      "7    1.000000e+00   1.000000e+00   1.000000e+00  \n",
      "8    0.000000e+00  6.268247e-309   0.000000e+00  \n",
      "9    1.000000e+00   1.000000e+00   1.000000e+00  \n",
      "10   0.000000e+00  2.549883e-135  6.098963e-152  \n",
      "11   0.000000e+00   0.000000e+00   0.000000e+00  \n",
      "12   0.000000e+00   0.000000e+00   0.000000e+00  \n",
      "13   1.000000e+00   1.000000e+00   1.000000e+00  \n",
      "14  3.427946e-268  3.156254e-118   1.000000e+00  \n",
      "15   1.000000e+00   1.000000e+00   1.000000e+00  \n",
      "16  2.071091e-287   1.402016e-69   9.028046e-63  \n",
      "17   1.000000e+00   1.000000e+00   1.000000e+00  \n",
      "18   0.000000e+00   0.000000e+00   0.000000e+00  \n",
      "19   1.000000e+00   1.000000e+00   1.000000e+00  \n",
      "20   0.000000e+00   0.000000e+00   0.000000e+00  \n"
     ]
    }
   ],
   "source": [
    "train_data = X_train_PV_DBOW.toarray()\n",
    "test_data = X_test_PV_DBOW.toarray()\n",
    "clf = GNBClassifier()\n",
    "csv_filename = \"GNBC_PV_DBOW_Prob.csv\"\n",
    "model_name = \"Gaussian Naive Bayes Classifier\"\n",
    "data_matrix_name = \"Paragraph Vector - Distributed Bag of Words\"\n",
    "execute(clf, train_data, test_data, model_name, data_matrix_name, csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: Currently this can't be done as dimensions of X_train_PV_DBOW and X_train_tfidf don't match\n",
    "\n",
    "# train_data = (X_train_PV_DBOW + X_train_tfidf).toarray()\n",
    "# test_data = (X_test_PV_DBOW + X_test_tfidf).toarray()\n",
    "# clf = GNBClassifier()\n",
    "# csv_filename = \"GNBC_PV_DBOW_tfidf_Prob.csv\"\n",
    "# model_name = \"Gaussian Naive Bayes Classifier\"\n",
    "# data_matrix_name = \"Combination of PV-DBOW and tfidf\"\n",
    "# execute(clf, train_data, test_data, model_name, data_matrix_name, csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted model 0\n",
      "Fitted model 1\n",
      "Fitted model 2\n",
      "Fitted model 3\n",
      "Fitted model 4\n",
      "Fitted model 5\n",
      "Final class probabilities for first 20 samples of training data:\n",
      "            harsh  extremely_harsh         vulgar    threatening  \\\n",
      "0    0.000000e+00     0.000000e+00   0.000000e+00   0.000000e+00   \n",
      "1    0.000000e+00     0.000000e+00   0.000000e+00   0.000000e+00   \n",
      "2    8.057051e-01     1.000000e+00   1.000000e+00   0.000000e+00   \n",
      "3    0.000000e+00     0.000000e+00   0.000000e+00   0.000000e+00   \n",
      "4    1.000000e+00     1.801689e-78   1.000000e+00   0.000000e+00   \n",
      "5   1.398824e-202     0.000000e+00  1.067358e-257   0.000000e+00   \n",
      "6    9.999989e-01     7.278383e-57   1.901527e-09   1.000000e+00   \n",
      "7    1.000000e+00     2.375757e-26   1.000000e+00   0.000000e+00   \n",
      "8    0.000000e+00     0.000000e+00   0.000000e+00   0.000000e+00   \n",
      "9   3.212388e-190     0.000000e+00  4.967359e-237  2.587740e-245   \n",
      "10  4.940656e-324     0.000000e+00   0.000000e+00   0.000000e+00   \n",
      "11   3.030508e-15    1.317446e-148   2.223578e-38  2.865787e-230   \n",
      "12  3.503362e-154     0.000000e+00  2.514620e-183   2.460512e-51   \n",
      "13   1.600615e-45    7.390676e-259   1.171228e-68   1.384971e-73   \n",
      "14   1.000000e+00     1.000000e+00   1.000000e+00   1.000000e+00   \n",
      "15   1.000000e+00     1.000000e+00   1.000000e+00   1.000000e+00   \n",
      "16   1.000000e+00     1.000000e+00   1.000000e+00   1.000000e+00   \n",
      "17   1.000000e+00     1.000000e+00   1.000000e+00   1.000000e+00   \n",
      "18   1.000000e+00     1.000000e+00   1.000000e+00   1.000000e+00   \n",
      "19   0.000000e+00     0.000000e+00   0.000000e+00   0.000000e+00   \n",
      "20   0.000000e+00     0.000000e+00   0.000000e+00   0.000000e+00   \n",
      "\n",
      "       disrespect  targeted_hate  \n",
      "0    0.000000e+00   0.000000e+00  \n",
      "1    0.000000e+00   0.000000e+00  \n",
      "2    1.000000e+00   9.999701e-01  \n",
      "3    0.000000e+00   0.000000e+00  \n",
      "4    1.000000e+00   8.094502e-19  \n",
      "5   4.113712e-269   0.000000e+00  \n",
      "6    7.177037e-04   1.093794e-13  \n",
      "7    1.000000e+00   1.000000e+00  \n",
      "8    0.000000e+00   0.000000e+00  \n",
      "9   1.704956e-204  4.382201e-281  \n",
      "10   0.000000e+00   0.000000e+00  \n",
      "11   3.356415e-27   7.575476e-96  \n",
      "12  2.360624e-175  7.077234e-159  \n",
      "13   2.261805e-63  3.244101e-153  \n",
      "14   1.000000e+00   1.000000e+00  \n",
      "15   1.000000e+00   1.000000e+00  \n",
      "16   1.000000e+00   1.000000e+00  \n",
      "17   1.000000e+00   1.000000e+00  \n",
      "18   1.000000e+00   1.000000e+00  \n",
      "19   0.000000e+00   0.000000e+00  \n",
      "20   0.000000e+00   0.000000e+00  \n",
      "Final class probabilities for first 20 samples of test data:\n",
      "                      id          harsh  extremely_harsh         vulgar  \\\n",
      "0   e0ae9d9474a5689a5791   1.000000e+00     1.000000e+00   1.000000e+00   \n",
      "1   b64a191301cad4f11287   3.090546e-87     0.000000e+00  3.632005e-123   \n",
      "2   5e1953d9ae04bdc66408   1.000000e+00     1.000000e+00   1.000000e+00   \n",
      "3   23128f98196c8e8f7b90   1.346169e-84     0.000000e+00  5.018515e-112   \n",
      "4   2d3f1254f71472bf2b78   1.000000e+00     1.000000e+00   1.000000e+00   \n",
      "5   21f4f0f4812a08ea6c28   0.000000e+00     0.000000e+00   0.000000e+00   \n",
      "6   733b43d534c67c1be948  1.199340e-244     0.000000e+00  1.916709e-306   \n",
      "7   aad47a397f7ddc629d5d   1.000000e+00     3.267581e-12   1.000000e+00   \n",
      "8   d19fcde8a3af2e472d74   9.671290e-76     0.000000e+00  1.116020e-115   \n",
      "9   7d4de482c60f1c8a79c6   1.000000e+00     1.000000e+00   1.000000e+00   \n",
      "10  f81afe094bcd161ea6f8  3.252499e-132     0.000000e+00  4.073771e-165   \n",
      "11  132973e40fa63e0d07f1  1.217312e-213     0.000000e+00  2.718057e-282   \n",
      "12  37195759bf9104c3b488  1.856314e-159     0.000000e+00  5.867508e-196   \n",
      "13  0c46ec0e711469190af8   1.000000e+00     1.000000e+00   1.000000e+00   \n",
      "14  ecaeb00c751a61c1370e   9.988119e-01    2.469955e-135   7.084932e-20   \n",
      "15  dfaa434123c910477026   1.000000e+00     1.000000e+00   1.000000e+00   \n",
      "16  f0e8517a1d9e1394696b   9.998904e-01    2.381561e-132   1.382604e-17   \n",
      "17  db952db7fb786093a8f7   2.159239e-30    8.776807e-216   2.316465e-49   \n",
      "18  132ad2e7621d9c132c3f   0.000000e+00     0.000000e+00   0.000000e+00   \n",
      "19  38eef2c8994e8a980bb1   1.464872e-04    3.510298e-132   2.486925e-22   \n",
      "20  160a621ad6ee20736755   4.791718e-25    1.502153e-180   1.750960e-56   \n",
      "\n",
      "      threatening     disrespect  targeted_hate  \n",
      "0    3.687791e-71   2.658365e-53   1.000000e+00  \n",
      "1    2.882834e-53  1.585059e-112  8.396469e-105  \n",
      "2    1.000000e+00   1.000000e+00   1.000000e+00  \n",
      "3    3.004205e-76  3.063206e-125  3.539543e-118  \n",
      "4    1.000000e+00   1.000000e+00   1.000000e+00  \n",
      "5    0.000000e+00   0.000000e+00   0.000000e+00  \n",
      "6    0.000000e+00   0.000000e+00   0.000000e+00  \n",
      "7    7.789871e-31   1.000000e+00   1.000000e+00  \n",
      "8    0.000000e+00   4.295993e-95  2.278036e-193  \n",
      "9    0.000000e+00   1.000000e+00   1.000000e+00  \n",
      "10   0.000000e+00  6.448049e-178   0.000000e+00  \n",
      "11   0.000000e+00  1.379890e-282  1.904063e-272  \n",
      "12  8.474880e-265  4.976174e-189  3.501481e-221  \n",
      "13   1.000000e+00   1.000000e+00   1.000000e+00  \n",
      "14   1.000000e+00   3.833370e-11   2.360983e-29  \n",
      "15   1.000000e+00   1.000000e+00   1.000000e+00  \n",
      "16   2.844199e-05   3.223826e-05   9.460509e-12  \n",
      "17   0.000000e+00   5.369154e-60   2.618863e-59  \n",
      "18   0.000000e+00   0.000000e+00   0.000000e+00  \n",
      "19   7.983070e-80   2.452223e-13   9.308725e-43  \n",
      "20   1.000000e+00   8.825840e-44  5.094445e-104  \n"
     ]
    }
   ],
   "source": [
    "train_data = X_train_W2V_method_1.toarray()\n",
    "test_data = X_test_W2V_method_1.toarray()\n",
    "clf = GNBClassifier()\n",
    "csv_filename = \"GNBC_W2V_method_1_Prob.csv\"\n",
    "model_name = \"Gaussian Naive Bayes Classifier\"\n",
    "data_matrix_name = \"Word2Vec Method_1\"\n",
    "execute(clf, train_data, test_data, model_name, data_matrix_name, csv_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.04 Multinomial Naive Bayes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "class MNBClassifier(object): #Pack of 6 binary Multinomial naive bayes classifiers\n",
    "    def __init__(self):\n",
    "        self.param_grid = {\n",
    "            'alpha' : [1.0] #default=1.0\n",
    "        }\n",
    "        self.models = []\n",
    "        \n",
    "        for i in range(6): #Appending 6 binary Multinomial naive bayes classifiers (one for each class)\n",
    "            self.models.append(MultinomialNB())\n",
    "    \n",
    "    def fit(self, XTrain, yTrain, CVScoreType, numFolds): #Fits the models and returns the best CVScores and CVParams after performing grid search CV\n",
    "        CVScores = {}\n",
    "        CVParams = {}\n",
    "        \n",
    "        for i, model in enumerate(self.models):\n",
    "            labels = yTrain[label_type[i]]\n",
    "            \n",
    "            bestParams, bestScore = performGridSearchCV(model, self.param_grid, XTrain, labels, numFolds, CVScoreType)\n",
    "            CVParams[label_type[i]] = bestParams\n",
    "            CVScores[label_type[i]] = bestScore\n",
    "            \n",
    "            self.models[i] = MultinomialNB(alpha=bestParams['alpha'])\n",
    "            self.models[i].fit(XTrain, labels)\n",
    "            print(\"Fitted model \"+str(i))\n",
    "            \n",
    "        return CVScores, CVParams\n",
    "\n",
    "    def fitWithoutGridSearch(self, XTrain, yTrain): #Fits the models without performing grid search CV\n",
    "        for i, model in enumerate(self.models):\n",
    "            labels = yTrain[label_type[i]]\n",
    "            self.models[i].fit(XTrain, labels) \n",
    "    \n",
    "    def predict(self, X): #Returns the binary classification results for each of the models\n",
    "        predictions = {}\n",
    "        for i, model in enumerate(self.models):\n",
    "            predictions[label_type[i]] = model.predict(X)\n",
    "        return predictions\n",
    "    \n",
    "    def predictProbabilities(self, X): #Returns the predicted class probabilities for each of the classes\n",
    "        probabilities = {}\n",
    "        for i, model in enumerate(self.models):\n",
    "            classes = model.classes_\n",
    "            index = np.where(classes == 1)[0][0] #Getting the index of class 1\n",
    "            probabilities[label_type[i]] = model.predict_proba(X)[:, index] #Extracting the probabilities of class 1\n",
    "        return probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now testing out the Multinomial naive bayes classifier on various data matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Multinomial Naive Bayes Classifier\n",
      "Data matrix: TF-IDF\n",
      "\n",
      "ROC_AUC scores for the test split:\n",
      "ROC_AUC score for harsh: 0.8503997639390523\n",
      "ROC_AUC score for extremely_harsh: 0.8671244863132967\n",
      "ROC_AUC score for vulgar: 0.8528247567987389\n",
      "ROC_AUC score for threatening: 0.7673325574718537\n",
      "ROC_AUC score for disrespect: 0.8384486498508045\n",
      "ROC_AUC score for targeted_hate: 0.8024000859998701\n",
      "\n",
      "Mean ROC_AUC_score: 0.8297550500622694\n",
      "\n",
      "Accuracy and f1 scores for the entire training data:\n",
      "Accuracy score for harsh: 0.9189450707711695\n",
      "F1 score for harsh: 0.2729107791683488\n",
      "Accuracy score for extremely_harsh: 0.9896487319696663\n",
      "F1 score for extremely_harsh: 0.0\n",
      "Accuracy score for vulgar: 0.9514503026620761\n",
      "F1 score for vulgar: 0.16650569828085765\n",
      "Accuracy score for threatening: 0.9969733792389568\n",
      "F1 score for threatening: 0.0\n",
      "Accuracy score for disrespect: 0.9522266477643512\n",
      "F1 score for disrespect: 0.06558098591549295\n",
      "Accuracy score for targeted_hate: 0.9909876459866334\n",
      "F1 score for targeted_hate: 0.0024906600249066002\n",
      "\n",
      "Final class probabilities for first 20 samples of training data:\n",
      "           harsh  extremely_harsh        vulgar   threatening    disrespect  \\\n",
      "0   1.110132e-06     1.739983e-11  4.660479e-08  1.670868e-12  2.877773e-08   \n",
      "1   3.878851e-07     5.712147e-12  1.948768e-08  3.006830e-13  8.175219e-09   \n",
      "2   2.570386e-02     2.092119e-04  4.234330e-03  1.800206e-05  3.374471e-03   \n",
      "3   6.414228e-04     2.316466e-11  5.187183e-06  6.901255e-13  2.690079e-06   \n",
      "4   8.954317e-04     1.531984e-07  8.272564e-05  1.468860e-08  5.365694e-05   \n",
      "5   1.026567e-05     1.406209e-10  4.335211e-07  1.176392e-11  2.296566e-07   \n",
      "6   9.296965e-04     8.317160e-08  5.402694e-05  1.136522e-08  3.893371e-05   \n",
      "7   2.717125e-03     1.872409e-07  1.854724e-04  1.094344e-08  1.044331e-04   \n",
      "8   6.654893e-07     1.532524e-13  1.067946e-08  4.753230e-15  4.772863e-09   \n",
      "9   4.422511e-04     7.242122e-08  3.023776e-05  8.162453e-09  2.671722e-05   \n",
      "10  1.378478e-05     2.486156e-10  5.052578e-07  1.882992e-11  2.605373e-07   \n",
      "11  1.935659e-04     1.519362e-08  1.751426e-05  1.197603e-09  1.118788e-05   \n",
      "12  1.926167e-04     1.606013e-08  1.399183e-05  1.760439e-09  1.006624e-05   \n",
      "13  3.005253e-03     2.594341e-06  4.200799e-04  3.143774e-07  3.042877e-04   \n",
      "14  6.696662e-03     5.022462e-06  5.118897e-04  6.473255e-07  4.378960e-04   \n",
      "15  5.427848e-03     1.281397e-05  1.085109e-03  1.731709e-06  9.639747e-04   \n",
      "16  8.214013e-03     6.363699e-06  1.188723e-03  1.037657e-06  8.292974e-04   \n",
      "17  2.978988e-03     1.213667e-06  2.495514e-04  1.308296e-07  2.097900e-04   \n",
      "18  1.070219e-03     4.692439e-08  7.064795e-05  1.908481e-09  4.766547e-05   \n",
      "19  1.917379e-07     1.395756e-13  4.005773e-09  4.307948e-15  2.423741e-09   \n",
      "20  2.487060e-08     4.627429e-14  5.903702e-10  1.544654e-15  3.246974e-10   \n",
      "\n",
      "    targeted_hate  \n",
      "0    2.355528e-11  \n",
      "1    5.942983e-12  \n",
      "2    8.438843e-05  \n",
      "3    1.562307e-11  \n",
      "4    1.708823e-07  \n",
      "5    2.580734e-10  \n",
      "6    8.118115e-08  \n",
      "7    1.320167e-07  \n",
      "8    2.328315e-13  \n",
      "9    6.650192e-08  \n",
      "10   3.089616e-10  \n",
      "11   2.627731e-08  \n",
      "12   2.818684e-08  \n",
      "13   2.442739e-06  \n",
      "14   5.178176e-06  \n",
      "15   1.428017e-05  \n",
      "16   6.914953e-06  \n",
      "17   1.313680e-06  \n",
      "18   3.165315e-08  \n",
      "19   1.161073e-13  \n",
      "20   5.066080e-14  \n",
      "Final class probabilities for first 20 samples of test data:\n",
      "                      id         harsh  extremely_harsh        vulgar  \\\n",
      "0   e0ae9d9474a5689a5791  8.991998e-03     6.701560e-05  1.486807e-03   \n",
      "1   b64a191301cad4f11287  1.795273e-04     5.351864e-11  2.682111e-06   \n",
      "2   5e1953d9ae04bdc66408  8.075448e-02     4.246322e-04  1.646060e-02   \n",
      "3   23128f98196c8e8f7b90  1.222903e-03     9.833653e-07  1.911108e-04   \n",
      "4   2d3f1254f71472bf2b78  2.763509e-03     3.079767e-06  3.683494e-04   \n",
      "5   21f4f0f4812a08ea6c28  1.152359e-07     6.042924e-17  1.464632e-10   \n",
      "6   733b43d534c67c1be948  5.102836e-05     6.711462e-10  1.767531e-06   \n",
      "7   aad47a397f7ddc629d5d  5.844656e-03     9.668910e-06  1.176434e-03   \n",
      "8   d19fcde8a3af2e472d74  1.942979e-04     1.021423e-08  9.460584e-06   \n",
      "9   7d4de482c60f1c8a79c6  2.577796e-02     3.182652e-05  3.927147e-03   \n",
      "10  f81afe094bcd161ea6f8  5.687264e-05     7.405171e-10  1.881241e-06   \n",
      "11  132973e40fa63e0d07f1  1.259080e-05     1.152203e-10  4.526107e-07   \n",
      "12  37195759bf9104c3b488  1.867909e-06     3.616152e-12  4.258637e-08   \n",
      "13  0c46ec0e711469190af8  7.008335e-03     9.895279e-06  1.078420e-03   \n",
      "14  ecaeb00c751a61c1370e  1.983343e-03     9.007377e-08  1.576278e-04   \n",
      "15  dfaa434123c910477026  1.518414e-03     3.050963e-06  2.459077e-04   \n",
      "16  f0e8517a1d9e1394696b  5.506950e-04     3.483574e-08  3.843400e-05   \n",
      "17  db952db7fb786093a8f7  6.381113e-04     2.537438e-07  5.892001e-05   \n",
      "18  132ad2e7621d9c132c3f  1.226434e-05     9.455465e-11  5.346400e-07   \n",
      "19  38eef2c8994e8a980bb1  8.324437e-04     1.721776e-07  9.966035e-05   \n",
      "20  160a621ad6ee20736755  3.421263e-05     2.315703e-09  2.362054e-06   \n",
      "\n",
      "     threatening    disrespect  targeted_hate  \n",
      "0   1.504649e-05  1.299071e-03   6.119801e-05  \n",
      "1   1.254841e-12  1.816124e-06   1.299442e-10  \n",
      "2   1.013169e-04  1.688992e-02   4.102367e-04  \n",
      "3   1.084832e-07  1.419465e-04   1.425139e-06  \n",
      "4   3.913868e-07  2.820877e-04   2.796425e-06  \n",
      "5   5.197472e-19  7.523185e-11   1.044762e-16  \n",
      "6   3.138892e-11  1.307618e-06   8.529347e-10  \n",
      "7   1.567359e-06  9.596222e-04   1.212825e-05  \n",
      "8   7.820883e-10  5.850628e-06   1.014777e-08  \n",
      "9   2.017300e-06  3.028004e-03   2.467332e-05  \n",
      "10  5.606976e-11  1.617349e-06   1.445856e-09  \n",
      "11  8.631569e-12  3.383615e-07   2.371322e-10  \n",
      "12  2.034545e-13  2.139756e-08   6.285199e-12  \n",
      "13  1.367529e-06  9.287989e-04   1.015680e-05  \n",
      "14  8.668452e-09  5.745839e-05   1.814669e-07  \n",
      "15  5.226180e-07  1.908117e-04   3.064611e-06  \n",
      "16  2.886339e-09  2.467068e-05   6.689914e-08  \n",
      "17  3.038580e-08  4.600351e-05   3.176847e-07  \n",
      "18  4.062550e-12  3.632147e-07   8.935544e-11  \n",
      "19  1.167730e-08  7.324016e-05   1.783555e-07  \n",
      "20  2.459262e-10  1.674963e-06   3.573954e-09  \n"
     ]
    }
   ],
   "source": [
    "train_data = X_train_tfidf\n",
    "test_data = X_test_tfidf\n",
    "clf = MNBClassifier()\n",
    "csv_filename = \"MNBC_tfidf_Prob.csv\"\n",
    "model_name = \"Multinomial Naive Bayes Classifier\"\n",
    "data_matrix_name = \"TF-IDF\"\n",
    "execute(clf, train_data, test_data, model_name, data_matrix_name, csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Multinomial Naive Bayes Classifier\n",
      "Data matrix: Bag of Words\n",
      "\n",
      "ROC_AUC scores for the test split:\n",
      "ROC_AUC score for harsh: 0.883152888848902\n",
      "ROC_AUC score for extremely_harsh: 0.8943696164472614\n",
      "ROC_AUC score for vulgar: 0.886351963650596\n",
      "ROC_AUC score for threatening: 0.8042413048828004\n",
      "ROC_AUC score for disrespect: 0.8781685281595838\n",
      "ROC_AUC score for targeted_hate: 0.8003261504510936\n",
      "\n",
      "Mean ROC_AUC_score: 0.8577684087400396\n",
      "\n",
      "Accuracy and f1 scores for the entire training data:\n",
      "Accuracy score for harsh: 0.9505839465334504\n",
      "F1 score for harsh: 0.7188220230473752\n",
      "Accuracy score for extremely_harsh: 0.987049663583789\n",
      "F1 score for extremely_harsh: 0.4360607545320921\n",
      "Accuracy score for vulgar: 0.9679223204842593\n",
      "F1 score for vulgar: 0.6831870207800866\n",
      "Accuracy score for threatening: 0.9955894597088143\n",
      "F1 score for threatening: 0.09259259259259259\n",
      "Accuracy score for disrespect: 0.9657057989603727\n",
      "F1 score for disrespect: 0.6347951114306255\n",
      "Accuracy score for targeted_hate: 0.9882760638178176\n",
      "F1 score for targeted_hate: 0.21654135338345865\n",
      "\n",
      "Final class probabilities for first 20 samples of training data:\n",
      "           harsh  extremely_harsh         vulgar    threatening  \\\n",
      "0   5.425996e-45    2.289520e-107   9.793043e-55  4.305789e-122   \n",
      "1   5.634449e-38     9.580974e-82   1.171138e-43  1.901845e-106   \n",
      "2   8.937221e-03     3.103611e-07   7.460055e-04   1.977454e-10   \n",
      "3   1.000000e+00    5.519056e-151   6.678737e-13  1.427176e-180   \n",
      "4   3.077500e-06     5.975135e-19   1.840405e-08   2.897791e-26   \n",
      "5   1.200243e-20     3.593304e-61   2.905169e-27   7.494006e-73   \n",
      "6   1.091094e-05     1.581723e-21   9.853981e-09   2.337922e-26   \n",
      "7   6.703286e-02     6.415207e-12   3.164618e-04   2.006570e-19   \n",
      "8   3.193785e-22     3.424548e-71   1.259193e-30   5.372926e-93   \n",
      "9   9.582250e-07     2.337670e-22   9.234124e-10   1.768547e-27   \n",
      "10  8.081210e-19     2.013033e-51   1.029711e-24   1.777907e-65   \n",
      "11  1.642355e-07     3.993088e-19   4.398187e-09   3.194008e-26   \n",
      "12  3.152629e-11     9.275074e-36   1.518493e-15   4.739629e-44   \n",
      "13  1.707103e-10     2.177293e-34   4.717422e-14   8.633407e-44   \n",
      "14  2.569984e-03     4.573631e-12   2.180154e-05   1.252886e-14   \n",
      "15  2.213111e-09     3.287455e-30   3.057041e-12   2.361568e-37   \n",
      "16  1.196307e-02     1.958033e-14   9.770319e-05   1.266793e-18   \n",
      "17  2.855188e-04     8.978970e-17   1.947707e-07   1.633220e-21   \n",
      "18  6.554396e-05     3.946775e-17   8.608673e-07   1.167467e-22   \n",
      "19  2.765863e-37     3.081135e-94   8.149166e-46  1.215728e-116   \n",
      "20  6.291435e-85    2.422119e-195  1.288817e-101  2.886436e-233   \n",
      "\n",
      "       disrespect  targeted_hate  \n",
      "0    8.308445e-59  2.632861e-102  \n",
      "1    1.184749e-48   1.069740e-85  \n",
      "2    2.739082e-04   3.325006e-08  \n",
      "3    6.399788e-18  1.742037e-156  \n",
      "4    2.255750e-09   3.478366e-19  \n",
      "5    3.656611e-31   1.092450e-55  \n",
      "6    1.168171e-09   6.425593e-22  \n",
      "7    6.783086e-05   7.226227e-14  \n",
      "8    6.435495e-34   5.432754e-72  \n",
      "9    2.016641e-10   5.929946e-22  \n",
      "10   2.261258e-27   4.160415e-47  \n",
      "11   5.053340e-10   8.080935e-19  \n",
      "12   1.526687e-16   7.415485e-31  \n",
      "13   1.465112e-15   2.180126e-33  \n",
      "14   1.155905e-05   3.687650e-11  \n",
      "15   9.444146e-13   3.376253e-25  \n",
      "16   1.965278e-05   2.841526e-13  \n",
      "17   1.195622e-07   2.711469e-16  \n",
      "18   2.758603e-07   5.868783e-18  \n",
      "19   3.171699e-48   2.102467e-96  \n",
      "20  1.202342e-107  4.459872e-189  \n",
      "Final class probabilities for first 20 samples of test data:\n",
      "                      id         harsh  extremely_harsh         vulgar  \\\n",
      "0   e0ae9d9474a5689a5791  2.715314e-03     1.699000e-06   1.152306e-04   \n",
      "1   b64a191301cad4f11287  1.157644e-08     2.794749e-55   3.441803e-17   \n",
      "2   5e1953d9ae04bdc66408  5.955335e-01     5.653573e-06   1.905944e-01   \n",
      "3   23128f98196c8e8f7b90  6.795273e-16     7.416377e-43   1.894691e-19   \n",
      "4   2d3f1254f71472bf2b78  3.769109e-04     1.921596e-12   7.412260e-06   \n",
      "5   21f4f0f4812a08ea6c28  6.511597e-70    4.889713e-306  6.157189e-122   \n",
      "6   733b43d534c67c1be948  1.772956e-17     5.203929e-60   5.707029e-26   \n",
      "7   aad47a397f7ddc629d5d  1.788561e-04     1.348419e-12   5.752334e-06   \n",
      "8   d19fcde8a3af2e472d74  2.565778e-13     1.726584e-43   2.267534e-18   \n",
      "9   7d4de482c60f1c8a79c6  4.382043e-01     2.604352e-04   1.194916e-01   \n",
      "10  f81afe094bcd161ea6f8  3.218162e-14     2.936149e-48   3.490276e-21   \n",
      "11  132973e40fa63e0d07f1  3.173012e-29     2.804044e-83   8.090518e-38   \n",
      "12  37195759bf9104c3b488  4.351744e-32     5.685394e-95   1.266744e-43   \n",
      "13  0c46ec0e711469190af8  4.715619e-03     1.390263e-13   8.793848e-05   \n",
      "14  ecaeb00c751a61c1370e  4.156265e-07     8.659137e-42   9.831220e-12   \n",
      "15  dfaa434123c910477026  2.958542e-05     1.143498e-12   4.340280e-07   \n",
      "16  f0e8517a1d9e1394696b  3.853086e-15     3.527437e-55   8.721834e-22   \n",
      "17  db952db7fb786093a8f7  4.099805e-05     1.804646e-14   5.564793e-07   \n",
      "18  132ad2e7621d9c132c3f  1.563756e-15     3.429974e-47   3.686802e-21   \n",
      "19  38eef2c8994e8a980bb1  1.453399e-04     2.539251e-13   4.992798e-06   \n",
      "20  160a621ad6ee20736755  9.974871e-22     1.617037e-55   4.084377e-27   \n",
      "\n",
      "      threatening     disrespect  targeted_hate  \n",
      "0    9.331804e-09   6.798228e-05   2.173619e-07  \n",
      "1    1.094594e-76   1.423440e-18   3.177436e-51  \n",
      "2    6.185646e-07   1.089672e-01   1.778735e-05  \n",
      "3    1.304200e-53   1.623515e-20   2.698250e-36  \n",
      "4    2.192049e-15   2.934194e-06   3.025044e-12  \n",
      "5    0.000000e+00  1.523770e-129  1.146841e-294  \n",
      "6    7.821382e-83   6.884381e-28   2.658252e-60  \n",
      "7    4.356048e-15   4.399276e-06   2.662152e-11  \n",
      "8    5.763565e-57   4.428064e-20   7.148281e-42  \n",
      "9    1.135858e-06   9.425079e-02   5.113851e-05  \n",
      "10   8.959114e-62   1.031585e-21   4.423684e-45  \n",
      "11  9.029229e-101   1.982668e-39   4.224692e-74  \n",
      "12  1.147403e-117   7.115933e-47   1.771149e-87  \n",
      "13   2.411310e-18   5.277679e-05   4.738363e-13  \n",
      "14   8.045100e-52   4.052420e-16   8.483541e-34  \n",
      "15   1.410156e-13   5.862526e-07   1.499288e-11  \n",
      "16   2.605593e-71   9.282925e-25   8.705287e-49  \n",
      "17   3.465912e-18   3.232885e-07   2.148270e-13  \n",
      "18   4.864495e-65   1.388192e-22   1.861863e-49  \n",
      "19   9.977048e-20   4.224301e-06   3.490266e-14  \n",
      "20   8.896910e-66   2.117134e-28   2.122822e-50  \n"
     ]
    }
   ],
   "source": [
    "train_data = X_train_bow\n",
    "test_data = X_test_bow\n",
    "clf = MNBClassifier()\n",
    "csv_filename = \"MNBC_bow_Prob.csv\"\n",
    "model_name = \"Multinomial Naive Bayes Classifier\"\n",
    "data_matrix_name = \"Bag of Words\"\n",
    "# execute(clf, train_data, test_data, model_name, data_matrix_name, csv_filename, \"WGS\")\n",
    "execute(clf, train_data, test_data, model_name, data_matrix_name, csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Multinomial Naive Bayes Classifier\n",
      "Data matrix: Combination of TF-IDF and Bag of Words\n",
      "\n",
      "ROC_AUC scores for the test split:\n",
      "ROC_AUC score for harsh: 0.8923194458153137\n",
      "ROC_AUC score for extremely_harsh: 0.9049849376610712\n",
      "ROC_AUC score for vulgar: 0.8958764888005428\n",
      "ROC_AUC score for threatening: 0.8181824590365646\n",
      "ROC_AUC score for disrespect: 0.8885085144654412\n",
      "ROC_AUC score for targeted_hate: 0.814608247603903\n",
      "\n",
      "Mean ROC_AUC_score: 0.8690800155638062\n",
      "\n",
      "Accuracy and f1 scores for the entire training data:\n",
      "Accuracy score for harsh: 0.9484686874142082\n",
      "F1 score for harsh: 0.7300801508722299\n",
      "Accuracy score for extremely_harsh: 0.9838992776615135\n",
      "F1 score for extremely_harsh: 0.4222850222042794\n",
      "Accuracy score for vulgar: 0.963770561893832\n",
      "F1 score for vulgar: 0.680365296803653\n",
      "Accuracy score for threatening: 0.9942055401786719\n",
      "F1 score for threatening: 0.09490333919156414\n",
      "Accuracy score for disrespect: 0.9614077724521254\n",
      "F1 score for disrespect: 0.6374207188160677\n",
      "Accuracy score for targeted_hate: 0.9857107495668219\n",
      "F1 score for targeted_hate: 0.24852071005917162\n",
      "\n",
      "Final class probabilities for first 20 samples of training data:\n",
      "           harsh  extremely_harsh         vulgar    threatening  \\\n",
      "0   5.767022e-48    2.434631e-112   6.799904e-58  1.654590e-127   \n",
      "1   3.429995e-41     3.912034e-87   5.270705e-47  3.241657e-113   \n",
      "2   2.176250e-02     2.232442e-07   6.691742e-04   1.166246e-11   \n",
      "3   1.000000e+00    3.054559e-150   1.405194e-08  8.357277e-177   \n",
      "4   9.623201e-07     2.210442e-21   2.433061e-09   1.043833e-29   \n",
      "5   2.278914e-22     3.558677e-66   2.502969e-29   3.174653e-78   \n",
      "6   6.727308e-06     1.771048e-24   2.049140e-09   7.689526e-30   \n",
      "7   8.979533e-02     1.088473e-13   1.772737e-04   3.878904e-22   \n",
      "8   5.442123e-24     1.916376e-76   1.056303e-32   1.079594e-99   \n",
      "9   2.922205e-07     2.076620e-24   1.593315e-10   9.278002e-30   \n",
      "10  1.834154e-20     1.360146e-55   9.499698e-27   5.891597e-71   \n",
      "11  7.478250e-09     2.697470e-22   2.428558e-10   1.876220e-30   \n",
      "12  3.007734e-12     1.080392e-38   1.129068e-16   2.200400e-47   \n",
      "13  6.040767e-11     3.893646e-35   2.266607e-14   1.130255e-44   \n",
      "14  1.574671e-03     3.274350e-14   2.541285e-06   2.687791e-17   \n",
      "15  1.071052e-09     2.008687e-31   2.417067e-12   9.484513e-39   \n",
      "16  1.504350e-02     1.250138e-15   7.901245e-05   8.312516e-20   \n",
      "17  1.297161e-04     9.189203e-19   2.665797e-08   7.009994e-24   \n",
      "18  3.527663e-05     1.881896e-19   2.579118e-07   3.053549e-26   \n",
      "19  1.066227e-39     1.733819e-99   2.696657e-48  6.992453e-123   \n",
      "20  6.944392e-88    2.072104e-201  1.275676e-104  3.181397e-240   \n",
      "\n",
      "       disrespect  targeted_hate  \n",
      "0    5.032155e-62  6.821278e-107  \n",
      "1    1.343332e-52   2.077174e-91  \n",
      "2    1.403558e-04   5.185638e-09  \n",
      "3    1.868529e-13  2.430316e-155  \n",
      "4    2.345027e-10   1.779230e-21  \n",
      "5    9.420119e-34   7.517293e-60  \n",
      "6    1.935370e-10   8.535250e-25  \n",
      "7    2.146780e-05   6.932218e-16  \n",
      "8    2.913320e-36   6.245623e-77  \n",
      "9    3.660163e-11   6.406217e-24  \n",
      "10   1.300602e-29   8.218898e-51  \n",
      "11   1.266184e-11   1.309568e-21  \n",
      "12   8.766796e-18   5.558211e-33  \n",
      "13   4.922611e-16   6.222290e-34  \n",
      "14   1.238289e-06   5.416692e-13  \n",
      "15   9.774632e-13   4.846650e-26  \n",
      "16   1.267612e-05   3.259278e-14  \n",
      "17   1.815205e-08   6.785664e-18  \n",
      "18   7.285182e-08   1.774465e-20  \n",
      "19   8.708861e-51  1.731391e-101  \n",
      "20  8.012153e-111  1.653730e-194  \n",
      "Final class probabilities for first 20 samples of test data:\n",
      "                      id         harsh  extremely_harsh         vulgar  \\\n",
      "0   e0ae9d9474a5689a5791  8.702856e-04     9.156685e-08   7.983407e-06   \n",
      "1   b64a191301cad4f11287  2.264602e-08     4.397554e-58   2.343121e-17   \n",
      "2   5e1953d9ae04bdc66408  8.159926e-01     1.281309e-06   3.321588e-01   \n",
      "3   23128f98196c8e8f7b90  4.213396e-17     3.281218e-44   2.164382e-20   \n",
      "4   2d3f1254f71472bf2b78  1.692505e-04     1.178144e-14   1.599088e-06   \n",
      "5   21f4f0f4812a08ea6c28  4.265498e-69    1.488988e-309  8.650325e-122   \n",
      "6   733b43d534c67c1be948  1.110858e-18     1.486951e-63   1.592791e-27   \n",
      "7   aad47a397f7ddc629d5d  1.135037e-04     3.387704e-14   2.634278e-06   \n",
      "8   d19fcde8a3af2e472d74  4.691285e-14     1.346671e-46   2.904049e-19   \n",
      "9   7d4de482c60f1c8a79c6  6.752045e-01     6.326214e-05   1.862652e-01   \n",
      "10  f81afe094bcd161ea6f8  5.516711e-15     1.033220e-51   1.141311e-22   \n",
      "11  132973e40fa63e0d07f1  5.336712e-31     1.159963e-87   9.285933e-40   \n",
      "12  37195759bf9104c3b488  9.587524e-34    7.653841e-100   1.506257e-45   \n",
      "13  0c46ec0e711469190af8  3.078933e-03     9.621742e-15   4.921951e-05   \n",
      "14  ecaeb00c751a61c1370e  1.622326e-06     9.745498e-44   4.273865e-11   \n",
      "15  dfaa434123c910477026  3.209093e-06     6.196008e-15   1.605617e-08   \n",
      "16  f0e8517a1d9e1394696b  1.073145e-15     1.389657e-57   1.389339e-22   \n",
      "17  db952db7fb786093a8f7  6.835662e-06     2.758081e-17   3.667578e-08   \n",
      "18  132ad2e7621d9c132c3f  4.120419e-17     5.713255e-51   9.267089e-23   \n",
      "19  38eef2c8994e8a980bb1  1.611262e-05     1.499710e-15   4.765870e-07   \n",
      "20  160a621ad6ee20736755  1.743648e-23     2.048810e-59   5.598710e-29   \n",
      "\n",
      "      threatening     disrespect  targeted_hate  \n",
      "0    5.380068e-10   3.329513e-06   8.597565e-09  \n",
      "1    6.338514e-81   8.861608e-19   1.198796e-53  \n",
      "2    1.755033e-07   1.989677e-01   5.957939e-06  \n",
      "3    4.577113e-55   1.820840e-21   5.771471e-37  \n",
      "4    3.476592e-18   5.793913e-07   3.257982e-14  \n",
      "5    0.000000e+00  4.013272e-129  7.330890e-297  \n",
      "6    2.136518e-87   1.524270e-29   1.189440e-63  \n",
      "7    6.531682e-17   2.193212e-06   1.279372e-12  \n",
      "8    4.459515e-61   3.039073e-21   1.037051e-44  \n",
      "9    4.261557e-08   1.369608e-01   8.688995e-06  \n",
      "10   8.561180e-66   7.259707e-23   4.133170e-48  \n",
      "11  8.638791e-106   3.133047e-41   1.033229e-77  \n",
      "12  1.632873e-123   3.567036e-49   2.352142e-91  \n",
      "13   8.745564e-20   3.414122e-05   5.896821e-14  \n",
      "14   3.051508e-54   4.143162e-16   1.659127e-34  \n",
      "15   9.188260e-16   2.757422e-08   1.831864e-13  \n",
      "16   8.244578e-74   7.920339e-26   1.707426e-50  \n",
      "17   9.529958e-22   2.278331e-08   1.251305e-15  \n",
      "18   1.799252e-70   2.281604e-24   2.235804e-53  \n",
      "19   5.829043e-23   3.985536e-07   2.121821e-16  \n",
      "20   5.035629e-70   3.220352e-30   1.000726e-53  \n"
     ]
    }
   ],
   "source": [
    "train_data = X_train_bow + X_train_tfidf\n",
    "test_data = X_test_bow + X_test_tfidf\n",
    "clf = MNBClassifier()\n",
    "csv_filename = \"MNBC_tfidf_bow_Prob.csv\"\n",
    "model_name = \"Multinomial Naive Bayes Classifier\"\n",
    "data_matrix_name = \"Combination of TF-IDF and Bag of Words\"\n",
    "execute(clf, train_data, test_data, model_name, data_matrix_name, csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final class probabilities for first 20 samples of training data:\n",
      "           harsh  extremely_harsh        vulgar   threatening    disrespect  \\\n",
      "0   3.339703e-11     3.921526e-22  4.341312e-13  1.389711e-18  8.499735e-13   \n",
      "1   1.876888e-19     1.310739e-22  3.769578e-20  7.161686e-30  5.032563e-21   \n",
      "2   8.331768e-02     2.176093e-08  2.729368e-03  9.131470e-08  4.207047e-03   \n",
      "3   1.000000e+00     1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
      "4   2.718317e-02     1.689536e-09  6.223234e-04  2.745624e-09  1.210267e-03   \n",
      "5   6.252064e-03     3.026622e-07  1.496456e-03  1.450005e-06  2.478505e-03   \n",
      "6   9.880060e-03     6.585332e-08  4.344872e-04  4.469047e-08  6.392441e-04   \n",
      "7   8.395903e-02     1.290112e-07  7.258902e-03  4.982949e-07  7.796325e-03   \n",
      "8   5.219363e-03     6.287339e-09  3.524871e-04  5.011420e-10  3.059233e-04   \n",
      "9   2.028515e-02     4.510567e-10  5.081534e-04  6.133391e-09  7.367768e-04   \n",
      "10  2.315245e-06     5.442561e-12  1.351559e-07  4.159740e-18  1.542933e-08   \n",
      "11  5.906617e-02     9.888432e-06  6.946042e-03  1.691958e-06  7.604791e-03   \n",
      "12  6.932040e-02     1.485704e-08  3.743428e-03  6.118148e-10  8.460921e-03   \n",
      "13  5.779131e-01     5.224722e-06  7.523763e-02  6.685635e-08  9.770573e-02   \n",
      "14  2.675931e-02     1.709327e-05  2.854549e-03  9.020610e-07  4.266691e-03   \n",
      "15  3.854476e-02     4.580247e-07  6.187168e-03  2.815341e-08  6.515200e-03   \n",
      "16  1.498614e-02     1.488543e-09  5.338808e-04  3.255049e-10  7.251024e-04   \n",
      "17  6.395562e-03     1.738189e-08  5.409924e-04  6.241630e-09  3.720548e-04   \n",
      "18  7.234890e-03     1.615975e-06  8.785506e-04  2.009638e-07  1.184779e-03   \n",
      "19  4.812514e-04     1.194612e-10  2.241252e-05  1.700420e-13  1.162799e-05   \n",
      "20  1.390174e-02     9.849869e-08  2.635267e-03  3.093007e-12  1.281076e-03   \n",
      "\n",
      "    targeted_hate  \n",
      "0    6.421112e-19  \n",
      "1    1.768552e-27  \n",
      "2    2.044654e-06  \n",
      "3    1.000000e+00  \n",
      "4    1.700525e-06  \n",
      "5    4.351413e-08  \n",
      "6    2.792795e-06  \n",
      "7    4.496835e-06  \n",
      "8    1.176585e-06  \n",
      "9    2.421933e-07  \n",
      "10   8.846516e-12  \n",
      "11   2.346805e-05  \n",
      "12   8.738496e-06  \n",
      "13   5.294350e-05  \n",
      "14   1.847267e-05  \n",
      "15   2.792743e-06  \n",
      "16   2.099155e-07  \n",
      "17   3.678572e-07  \n",
      "18   1.445911e-05  \n",
      "19   3.250204e-08  \n",
      "20   8.238829e-08  \n",
      "Final class probabilities for first 20 samples of test data:\n",
      "                      id         harsh  extremely_harsh        vulgar  \\\n",
      "0   e0ae9d9474a5689a5791  3.347255e-03     8.717214e-04  1.190487e-03   \n",
      "1   b64a191301cad4f11287  1.683239e-02     6.734768e-10  6.215980e-05   \n",
      "2   5e1953d9ae04bdc66408  7.699475e-03     4.366610e-05  1.584171e-03   \n",
      "3   23128f98196c8e8f7b90  2.697873e-07     7.523349e-16  1.480581e-09   \n",
      "4   2d3f1254f71472bf2b78  2.125935e-02     2.509569e-05  3.317713e-03   \n",
      "5   21f4f0f4812a08ea6c28  9.999978e-01     1.000000e+00  9.999306e-01   \n",
      "6   733b43d534c67c1be948  5.664694e-04     9.787919e-11  7.875808e-06   \n",
      "7   aad47a397f7ddc629d5d  6.564082e-03     1.447356e-05  1.485373e-03   \n",
      "8   d19fcde8a3af2e472d74  2.107096e-01     5.415169e-11  2.359435e-03   \n",
      "9   7d4de482c60f1c8a79c6  8.210010e-03     1.724684e-03  4.261114e-03   \n",
      "10  f81afe094bcd161ea6f8  1.014260e-01     1.131789e-07  1.696151e-03   \n",
      "11  132973e40fa63e0d07f1  1.128046e-05     1.018380e-12  1.167687e-07   \n",
      "12  37195759bf9104c3b488  2.906194e-15     1.517903e-22  2.204581e-16   \n",
      "13  0c46ec0e711469190af8  2.188927e-03     4.963904e-08  2.065412e-04   \n",
      "14  ecaeb00c751a61c1370e  2.738216e-02     6.961498e-09  3.287639e-04   \n",
      "15  dfaa434123c910477026  1.457015e-02     3.804495e-07  8.673007e-04   \n",
      "16  f0e8517a1d9e1394696b  2.560122e-07     1.083178e-17  6.973428e-08   \n",
      "17  db952db7fb786093a8f7  1.401796e-02     3.339625e-06  1.644185e-03   \n",
      "18  132ad2e7621d9c132c3f  1.465716e-04     3.514864e-09  1.594747e-05   \n",
      "19  38eef2c8994e8a980bb1  9.529769e-03     1.275653e-07  3.563481e-04   \n",
      "20  160a621ad6ee20736755  1.362955e-02     5.458349e-09  2.582976e-04   \n",
      "\n",
      "     threatening    disrespect  targeted_hate  \n",
      "0   2.406686e-06  9.010151e-04   3.078128e-05  \n",
      "1   1.512733e-13  3.457421e-05   3.295033e-05  \n",
      "2   7.045569e-08  1.357957e-03   8.164265e-06  \n",
      "3   1.282159e-17  1.504781e-09   3.376712e-14  \n",
      "4   3.319089e-07  3.332817e-03   2.168003e-05  \n",
      "5   4.574183e-18  9.985196e-01   3.177685e-05  \n",
      "6   5.560804e-14  1.669338e-05   4.548808e-11  \n",
      "7   3.926586e-07  1.223019e-03   4.135728e-05  \n",
      "8   5.618317e-14  4.623411e-03   5.699720e-06  \n",
      "9   4.759753e-06  3.037243e-03   8.636228e-05  \n",
      "10  3.668154e-09  5.166861e-03   5.993373e-07  \n",
      "11  4.899583e-14  3.291561e-07   1.864667e-11  \n",
      "12  4.438585e-21  1.097096e-17   1.100007e-24  \n",
      "13  2.470987e-09  2.105590e-04   9.000456e-08  \n",
      "14  5.997828e-11  7.460661e-04   1.665741e-04  \n",
      "15  8.922227e-09  1.278144e-03   5.256889e-06  \n",
      "16  3.065973e-18  1.037913e-09   1.943728e-12  \n",
      "17  9.330706e-08  1.371936e-03   5.500049e-06  \n",
      "18  5.886686e-12  1.305319e-05   2.534004e-08  \n",
      "19  1.029268e-08  7.029736e-04   1.433495e-05  \n",
      "20  3.595734e-10  7.194103e-04   1.206951e-06  \n"
     ]
    }
   ],
   "source": [
    "train_data = np.abs(X_train_PV_DM) #Taking absolute value as Multinomial Naive Bayes Classifier requires data matrix to have positive entries\n",
    "test_data = np.abs(X_test_PV_DM) #Taking absolute value as Multinomial Naive Bayes Classifier requires data matrix to have positive entries\n",
    "clf = MNBClassifier()\n",
    "csv_filename = \"MNBC_PV_DM_Prob.csv\"\n",
    "model_name = \"Multinomial Naive Bayes Classifier\"\n",
    "data_matrix_name = \"Paragraph Vector - Distributed Memory\"\n",
    "execute(clf, train_data, test_data, model_name, data_matrix_name, csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final class probabilities for first 20 samples of training data:\n",
      "           harsh  extremely_harsh        vulgar   threatening    disrespect  \\\n",
      "0   1.145447e-03     6.421058e-20  9.392470e-06  3.108905e-16  1.225155e-05   \n",
      "1   1.389412e-14     5.995153e-29  2.794160e-18  1.233128e-29  9.977739e-18   \n",
      "2   1.388325e-02     3.938387e-10  6.776839e-04  5.109616e-09  1.177681e-03   \n",
      "3   9.999904e-01     5.094963e-07  9.296185e-01  1.000000e+00  7.766697e-01   \n",
      "4   6.087962e-04     1.414294e-12  9.704256e-06  2.188325e-11  1.698754e-05   \n",
      "5   2.524724e-07     1.556370e-20  1.174980e-10  1.016947e-18  4.449715e-11   \n",
      "6   1.274237e-03     1.464199e-13  9.130911e-06  9.573572e-12  1.955250e-05   \n",
      "7   8.247911e-03     2.220147e-11  2.442139e-04  9.397391e-09  8.908754e-04   \n",
      "8   3.637223e-10     6.023946e-25  1.172934e-14  8.405588e-24  2.687005e-14   \n",
      "9   2.796757e-04     8.217566e-17  2.018925e-07  1.427603e-12  5.204466e-06   \n",
      "10  1.564126e-09     2.453529e-23  4.440689e-13  2.231356e-20  8.983062e-13   \n",
      "11  3.539208e-06     2.381941e-16  3.767327e-08  6.291339e-13  8.034033e-08   \n",
      "12  1.215684e-03     2.960454e-14  5.885822e-06  1.770119e-09  1.389696e-05   \n",
      "13  4.162998e-05     1.677756e-17  1.606479e-07  3.160754e-13  6.338757e-07   \n",
      "14  2.757925e-04     2.127161e-13  4.117824e-06  6.245526e-11  5.970699e-06   \n",
      "15  2.431665e-02     3.092935e-10  6.698465e-04  6.781352e-13  5.056535e-04   \n",
      "16  2.904296e-01     3.344394e-09  3.625914e-02  3.940267e-10  4.565750e-02   \n",
      "17  2.965344e-03     3.769264e-13  5.472782e-05  1.332418e-07  1.033621e-04   \n",
      "18  2.494670e-03     3.498469e-12  3.948921e-05  1.455208e-09  7.698460e-05   \n",
      "19  2.013604e-07     6.705181e-17  2.621938e-09  1.102269e-18  2.074494e-10   \n",
      "20  5.566877e-08     2.904512e-16  8.424921e-10  3.451066e-20  6.756091e-10   \n",
      "\n",
      "    targeted_hate  \n",
      "0    5.987569e-08  \n",
      "1    5.258685e-25  \n",
      "2    1.374238e-06  \n",
      "3    8.370512e-01  \n",
      "4    2.536544e-08  \n",
      "5    4.091111e-13  \n",
      "6    1.220646e-09  \n",
      "7    4.291273e-07  \n",
      "8    8.952785e-21  \n",
      "9    1.696261e-11  \n",
      "10   1.541156e-16  \n",
      "11   9.229532e-12  \n",
      "12   1.469714e-09  \n",
      "13   4.916658e-10  \n",
      "14   4.817726e-10  \n",
      "15   2.554266e-05  \n",
      "16   5.989285e-06  \n",
      "17   7.466678e-09  \n",
      "18   6.337772e-09  \n",
      "19   1.339724e-17  \n",
      "20   5.609011e-17  \n",
      "Final class probabilities for first 20 samples of test data:\n",
      "                      id         harsh  extremely_harsh        vulgar  \\\n",
      "0   e0ae9d9474a5689a5791  1.238776e-02     4.973405e-10  1.828224e-04   \n",
      "1   b64a191301cad4f11287  7.773093e-01     2.596565e-20  5.375885e-03   \n",
      "2   5e1953d9ae04bdc66408  9.997046e-01     4.487405e-02  9.997938e-01   \n",
      "3   23128f98196c8e8f7b90  2.352417e-05     7.530135e-18  1.377013e-07   \n",
      "4   2d3f1254f71472bf2b78  1.778304e-02     4.322067e-10  4.281198e-04   \n",
      "5   21f4f0f4812a08ea6c28  9.406947e-15     1.248163e-36  7.942455e-21   \n",
      "6   733b43d534c67c1be948  2.455271e-04     1.728175e-20  1.013674e-07   \n",
      "7   aad47a397f7ddc629d5d  1.713361e-02     9.946061e-10  7.307880e-04   \n",
      "8   d19fcde8a3af2e472d74  5.057222e-07     5.112433e-20  2.344219e-09   \n",
      "9   7d4de482c60f1c8a79c6  9.633000e-01     4.222286e-04  8.917367e-01   \n",
      "10  f81afe094bcd161ea6f8  4.884309e-05     8.549100e-16  8.230291e-08   \n",
      "11  132973e40fa63e0d07f1  6.840584e-11     1.585616e-31  2.621243e-15   \n",
      "12  37195759bf9104c3b488  1.297452e-16     1.026103e-33  1.933924e-20   \n",
      "13  0c46ec0e711469190af8  3.741237e-01     2.058276e-09  1.974771e-02   \n",
      "14  ecaeb00c751a61c1370e  1.505196e-07     9.652743e-23  1.582868e-11   \n",
      "15  dfaa434123c910477026  3.774439e-04     1.388593e-13  2.566250e-06   \n",
      "16  f0e8517a1d9e1394696b  1.108851e-04     3.165830e-17  3.114853e-08   \n",
      "17  db952db7fb786093a8f7  4.428548e-04     1.101474e-12  3.164681e-06   \n",
      "18  132ad2e7621d9c132c3f  3.262286e-09     9.719040e-18  1.235708e-12   \n",
      "19  38eef2c8994e8a980bb1  1.525662e-03     7.325174e-13  1.504919e-05   \n",
      "20  160a621ad6ee20736755  2.264846e-06     3.931027e-17  1.093588e-08   \n",
      "\n",
      "     threatening    disrespect  targeted_hate  \n",
      "0   9.617835e-08  4.577040e-04   1.249457e-06  \n",
      "1   1.192489e-12  8.979671e-03   8.115354e-09  \n",
      "2   4.488450e-06  9.997254e-01   2.664025e-01  \n",
      "3   3.169118e-16  5.891487e-07   3.740210e-11  \n",
      "4   5.178987e-09  9.258615e-04   7.232621e-07  \n",
      "5   2.864364e-38  3.054388e-21   9.962853e-27  \n",
      "6   5.790923e-21  2.482192e-07   4.517381e-12  \n",
      "7   1.305506e-08  1.817432e-03   4.251128e-06  \n",
      "8   7.463251e-15  2.253329e-09   1.525760e-14  \n",
      "9   3.331680e-05  9.357054e-01   1.634742e-02  \n",
      "10  6.398739e-16  8.738708e-08   2.451740e-12  \n",
      "11  8.518752e-28  3.407194e-15   5.027331e-21  \n",
      "12  1.111802e-30  1.726130e-20   2.077641e-27  \n",
      "13  4.054392e-10  2.376629e-02   1.064928e-05  \n",
      "14  1.008771e-03  9.319161e-11   1.731301e-06  \n",
      "15  2.018298e-12  8.675053e-06   4.705497e-09  \n",
      "16  1.305095e-13  2.289428e-08   1.229312e-10  \n",
      "17  1.054155e-09  5.122499e-06   4.617200e-09  \n",
      "18  3.802607e-21  7.258421e-13   6.196742e-20  \n",
      "19  8.199423e-10  4.197444e-05   6.258433e-08  \n",
      "20  2.414957e-19  6.500726e-08   3.379326e-12  \n"
     ]
    }
   ],
   "source": [
    "train_data = np.abs(X_train_PV_DBOW) #Taking absolute value as Multinomial Naive Bayes Classifier requires data matrix to have positive entries\n",
    "test_data = np.abs(X_test_PV_DBOW) #Taking absolute value as Multinomial Naive Bayes Classifier requires data matrix to have positive entries\n",
    "clf = MNBClassifier()\n",
    "csv_filename = \"MNBC_PV_DBOW_Prob.csv\"\n",
    "model_name = \"Multinomial Naive Bayes Classifier\"\n",
    "data_matrix_name = \"Paragraph Vector - Distributed Bag of Words\"\n",
    "execute(clf, train_data, test_data, model_name, data_matrix_name, csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final class probabilities for first 20 samples of training data:\n",
      "           harsh  extremely_harsh        vulgar   threatening    disrespect  \\\n",
      "0   3.218719e-22     9.188210e-38  1.244205e-25  9.338553e-20  4.026766e-26   \n",
      "1   1.046032e-22     4.763716e-37  4.240795e-25  2.191823e-27  8.963876e-25   \n",
      "2   5.088033e-01     4.476396e-09  1.625422e-03  2.903842e-04  5.980104e-04   \n",
      "3   4.235804e-35     7.831738e-61  4.018011e-40  2.228009e-14  3.907753e-39   \n",
      "4   3.158409e-09     3.941851e-24  1.451419e-12  1.008892e-10  1.946812e-12   \n",
      "5   2.306006e-15     1.527910e-32  2.418108e-19  2.617270e-14  5.147941e-19   \n",
      "6   5.516983e-11     1.176658e-25  2.034312e-14  5.282023e-03  1.588812e-13   \n",
      "7   6.561227e-02     5.769492e-12  2.938073e-04  8.426003e-03  2.302116e-03   \n",
      "8   9.851351e-21     4.205506e-41  6.945596e-25  1.949204e-30  3.508098e-23   \n",
      "9   3.984628e-15     1.828550e-34  5.211690e-20  7.459755e-17  2.111940e-17   \n",
      "10  5.992175e-20     8.221611e-36  5.554762e-24  6.500568e-22  3.221128e-23   \n",
      "11  3.112486e-10     1.137245e-24  1.341508e-13  6.384496e-16  4.881552e-13   \n",
      "12  1.073427e-15     2.956198e-33  3.128128e-20  6.848945e-11  4.775443e-20   \n",
      "13  1.268698e-08     4.460516e-22  3.717667e-11  1.499995e-08  7.467479e-11   \n",
      "14  9.999752e-01     3.400127e-04  9.993353e-01  9.542129e-01  9.998200e-01   \n",
      "15  4.557327e-10     4.485570e-26  2.719549e-14  2.866168e-16  2.264602e-13   \n",
      "16  1.940720e-06     1.331043e-21  1.455521e-09  1.088843e-11  6.091445e-09   \n",
      "17  4.241280e-07     5.707740e-21  1.597403e-10  2.469117e-02  4.530821e-10   \n",
      "18  8.917614e-07     1.068507e-18  9.043120e-10  2.626067e-02  2.383259e-09   \n",
      "19  4.770565e-36     8.222582e-62  5.555383e-41  2.960353e-22  6.355992e-38   \n",
      "20  2.052883e-74     1.677750e-97  1.175350e-78  3.931896e-68  5.809182e-75   \n",
      "\n",
      "    targeted_hate  \n",
      "0    8.080197e-27  \n",
      "1    1.716151e-28  \n",
      "2    1.097628e-07  \n",
      "3    1.219802e-40  \n",
      "4    1.030340e-16  \n",
      "5    2.883858e-17  \n",
      "6    4.063102e-16  \n",
      "7    1.893111e-08  \n",
      "8    5.107496e-25  \n",
      "9    7.261346e-20  \n",
      "10   3.112191e-23  \n",
      "11   2.693997e-13  \n",
      "12   1.482812e-18  \n",
      "13   8.374293e-15  \n",
      "14   1.323400e-01  \n",
      "15   3.840941e-14  \n",
      "16   6.811114e-10  \n",
      "17   2.248598e-11  \n",
      "18   7.896875e-13  \n",
      "19   7.435936e-43  \n",
      "20   3.823965e-73  \n",
      "Final class probabilities for first 20 samples of test data:\n",
      "                      id         harsh  extremely_harsh        vulgar  \\\n",
      "0   e0ae9d9474a5689a5791  2.843745e-01     9.298879e-21  2.561010e-07   \n",
      "1   b64a191301cad4f11287  4.300992e-07     2.641548e-26  1.649524e-11   \n",
      "2   5e1953d9ae04bdc66408  9.993392e-01     1.136255e-08  8.976645e-01   \n",
      "3   23128f98196c8e8f7b90  1.034778e-10     1.085201e-25  3.806185e-13   \n",
      "4   2d3f1254f71472bf2b78  2.637216e-07     7.132746e-21  1.671214e-10   \n",
      "5   21f4f0f4812a08ea6c28  2.829107e-43     1.365907e-92  5.881728e-55   \n",
      "6   733b43d534c67c1be948  4.262810e-07     1.596597e-24  3.760962e-10   \n",
      "7   aad47a397f7ddc629d5d  1.523514e-10     8.255099e-29  6.127208e-15   \n",
      "8   d19fcde8a3af2e472d74  2.939463e-06     6.380456e-19  2.993920e-09   \n",
      "9   7d4de482c60f1c8a79c6  1.000000e+00     1.000000e+00  1.000000e+00   \n",
      "10  f81afe094bcd161ea6f8  8.294169e-10     4.141281e-24  3.250056e-13   \n",
      "11  132973e40fa63e0d07f1  5.860397e-12     2.000677e-31  1.225790e-16   \n",
      "12  37195759bf9104c3b488  1.510467e-10     3.490294e-27  6.075940e-14   \n",
      "13  0c46ec0e711469190af8  1.128194e-01     3.029054e-19  5.039879e-06   \n",
      "14  ecaeb00c751a61c1370e  2.142056e-03     3.625915e-18  2.064682e-07   \n",
      "15  dfaa434123c910477026  8.551542e-06     1.795900e-19  4.453977e-09   \n",
      "16  f0e8517a1d9e1394696b  1.684044e-03     3.369059e-15  3.484278e-06   \n",
      "17  db952db7fb786093a8f7  1.630617e-09     4.337168e-22  5.211327e-12   \n",
      "18  132ad2e7621d9c132c3f  1.493967e-12     1.145940e-28  5.071615e-16   \n",
      "19  38eef2c8994e8a980bb1  2.242273e-06     3.499312e-20  1.247561e-09   \n",
      "20  160a621ad6ee20736755  1.020625e-09     1.189784e-24  3.112941e-13   \n",
      "\n",
      "     threatening    disrespect  targeted_hate  \n",
      "0   6.194150e-23  3.246659e-07   1.012962e-03  \n",
      "1   1.516668e-12  1.956757e-10   2.464710e-10  \n",
      "2   2.219689e-06  9.619326e-01   6.194901e-02  \n",
      "3   1.342220e-15  3.600748e-13   4.095525e-15  \n",
      "4   8.583635e-07  1.923391e-10   1.547711e-13  \n",
      "5   5.213492e-49  4.623534e-52   4.541755e-57  \n",
      "6   1.804847e-07  1.139537e-10   2.115725e-19  \n",
      "7   4.208042e-20  9.229554e-15   9.074750e-16  \n",
      "8   1.409735e-13  5.529367e-08   1.832665e-11  \n",
      "9   1.000000e+00  1.000000e+00   1.000000e+00  \n",
      "10  1.158244e-12  2.721588e-12   8.073238e-13  \n",
      "11  2.380094e-17  4.262949e-16   1.215870e-15  \n",
      "12  4.173668e-13  3.890340e-13   5.406862e-15  \n",
      "13  5.814010e-08  3.681165e-05   4.886391e-08  \n",
      "14  9.439106e-01  5.553771e-07   5.159306e-09  \n",
      "15  1.660300e-04  3.958601e-09   2.489597e-12  \n",
      "16  4.071687e-09  8.508590e-06   1.049744e-07  \n",
      "17  2.667152e-08  7.729091e-12   1.246901e-13  \n",
      "18  8.474918e-24  4.457580e-14   1.912673e-18  \n",
      "19  1.689706e-09  9.574835e-09   1.121504e-11  \n",
      "20  5.549748e-12  1.634553e-12   2.363944e-12  \n"
     ]
    }
   ],
   "source": [
    "train_data = np.abs(X_train_W2V_method_1) #Taking absolute value as Multinomial Naive Bayes Classifier requires data matrix to have positive entries\n",
    "test_data = np.abs(X_test_W2V_method_1) #Taking absolute value as Multinomial Naive Bayes Classifier requires data matrix to have positive entries\n",
    "clf = MNBClassifier()\n",
    "csv_filename = \"MNBC_W2V_method_1_Prob.csv\"\n",
    "model_name = \"Multinomial Naive Bayes Classifier\"\n",
    "data_matrix_name = \"Word2Vec Method_1\"\n",
    "execute(clf, train_data, test_data, model_name, data_matrix_name, csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: Currently this can't be done as dimensions of X_train_PV_DBOW and X_train_tfidf don't match\n",
    "\n",
    "# train_data = np.abs(X_train_PV_DBOW + X_train_tfidf) #Taking absolute value as Multinomial Naive Bayes Classifier requires data matrix to have positive entries\n",
    "# test_data = np.abs(X_test_PV_DBOW + X_test_tfidf) #Taking absolute value as Multinomial Naive Bayes Classifier requires data matrix to have positive entries\n",
    "# clf = MNBClassifier()\n",
    "# csv_filename = \"MNBC_PV_DBOW_tfidf_Prob.csv\"\n",
    "# model_name = \"Multinomial Naive Bayes Classifier\"\n",
    "# data_matrix_name = \"Combination of PV-DBOW and tfidf\"\n",
    "# execute(clf, train_data, test_data, model_name, data_matrix_name, csv_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.05 Random Forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "class RFClassifier(object): #Pack of 6 binary Random forest classifiers\n",
    "    def __init__(self, max_depth=850, max_features=5000, max_samples=None, min_samples_leaf=100):\n",
    "        self.max_depth = max_depth\n",
    "        self.max_features = max_features\n",
    "        self.max_samples = max_samples\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        \n",
    "        self.param_grid = {\n",
    "            'n_estimators' : [100, 200, 300], #default=100\n",
    "            'criterion' : ['gini', 'entropy'], #default=gini\n",
    "        }\n",
    "        self.models = []\n",
    "        \n",
    "        for i in range(6): #Appending 6 binary Random forest classifiers (one for each class)\n",
    "            self.models.append(RandomForestClassifier(max_depth=self.max_depth, max_features=self.max_features, max_samples=self.max_samples, min_samples_leaf = self.min_samples_leaf))\n",
    "    \n",
    "    def fit(self, XTrain, yTrain, CVScoreType, numFolds): #Fits the models and returns the best CVScores and CVParams after performing grid search CV\n",
    "        CVScores = {}\n",
    "        CVParams = {}\n",
    "        \n",
    "        for i, model in enumerate(self.models):\n",
    "            labels = yTrain[label_type[i]]\n",
    "            \n",
    "            bestParams, bestScore = performGridSearchCV(model, self.param_grid, XTrain, labels, numFolds, CVScoreType)\n",
    "            CVParams[label_type[i]] = bestParams\n",
    "            CVScores[label_type[i]] = bestScore\n",
    "            \n",
    "            self.models[i] = RandomForestClassifier(n_estimators = bestParams['n_estimators'], criterion = bestParams['criterion'], max_depth=self.max_depth, max_features=self.max_features, max_samples=self.max_samples, min_samples_leaf = self.min_samples_leaf)\n",
    "            self.models[i].fit(XTrain, labels)\n",
    "            print(\"Fitted Model \"+str(i))\n",
    "            \n",
    "        return CVScores, CVParams\n",
    "    \n",
    "    def fitWithoutGridSearch(self, XTrain, yTrain): #Fits the models without performing grid search CV\n",
    "        for i, model in enumerate(self.models):\n",
    "            labels = yTrain[label_type[i]]\n",
    "\n",
    "            self.models[i].fit(XTrain, labels)\n",
    "            print(\"Fitted Model \"+str(i))\n",
    "    \n",
    "    def predict(self, X): #Returns the binary classification results for each of the models\n",
    "        predictions = {}\n",
    "        for i, model in enumerate(self.models):\n",
    "            predictions[label_type[i]] = model.predict(X)\n",
    "        return predictions\n",
    "    \n",
    "    def predictProbabilities(self, X): #Returns the predicted class probabilities for each of the classes\n",
    "        probabilities = {}\n",
    "        for i, model in enumerate(self.models):\n",
    "            classes = model.classes_\n",
    "            index = np.where(classes == 1)[0][0] #Getting the index of class 1\n",
    "            probabilities[label_type[i]] = model.predict_proba(X)[:, index] #Extracting the probabilities of class 1\n",
    "        return probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now testing out the Random forest classifier on various data matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted Model 0\n",
      "Fitted Model 1\n",
      "Fitted Model 2\n",
      "Fitted Model 3\n",
      "Fitted Model 4\n",
      "Fitted Model 5\n",
      "Final class probabilities for first 20 samples of training data:\n",
      "       harsh  extremely_harsh    vulgar  threatening  disrespect  \\\n",
      "0   0.016194         0.000759  0.008839     0.001289    0.006417   \n",
      "1   0.025383         0.000833  0.009698     0.000680    0.008090   \n",
      "2   0.132829         0.007805  0.067742     0.001863    0.055493   \n",
      "3   0.295715         0.020514  0.300716     0.025500    0.225198   \n",
      "4   0.044347         0.002165  0.021662     0.002235    0.017245   \n",
      "5   0.054994         0.002134  0.017480     0.002391    0.019593   \n",
      "6   0.075361         0.001995  0.033869     0.003731    0.024901   \n",
      "7   0.203485         0.024459  0.090113     0.011652    0.121085   \n",
      "8   0.026184         0.001594  0.013796     0.001256    0.010722   \n",
      "9   0.064305         0.002615  0.030291     0.013831    0.032870   \n",
      "10  0.030578         0.002283  0.010665     0.005448    0.012921   \n",
      "11  0.075991         0.007471  0.043828     0.004393    0.034987   \n",
      "12  0.046881         0.001668  0.020582     0.000558    0.021477   \n",
      "13  0.028967         0.001109  0.012248     0.002133    0.011061   \n",
      "14  0.190413         0.019307  0.113968     0.003487    0.111771   \n",
      "15  0.036309         0.000149  0.012132     0.000481    0.012217   \n",
      "16  0.083514         0.003529  0.037688     0.002830    0.044372   \n",
      "17  0.078074         0.009667  0.031447     0.003063    0.035239   \n",
      "18  0.064266         0.005765  0.032240     0.003950    0.027926   \n",
      "19  0.012882         0.001963  0.009625     0.004557    0.009075   \n",
      "20  0.005537         0.001392  0.005661     0.000064    0.005517   \n",
      "\n",
      "    targeted_hate  \n",
      "0        0.000857  \n",
      "1        0.001579  \n",
      "2        0.009363  \n",
      "3        0.032708  \n",
      "4        0.002715  \n",
      "5        0.001604  \n",
      "6        0.003379  \n",
      "7        0.018242  \n",
      "8        0.001374  \n",
      "9        0.004336  \n",
      "10       0.001088  \n",
      "11       0.004780  \n",
      "12       0.002760  \n",
      "13       0.003604  \n",
      "14       0.016655  \n",
      "15       0.001451  \n",
      "16       0.008853  \n",
      "17       0.004537  \n",
      "18       0.002319  \n",
      "19       0.000603  \n",
      "20       0.000341  \n",
      "Final class probabilities for first 20 samples of test data:\n",
      "                      id     harsh  extremely_harsh    vulgar  threatening  \\\n",
      "0   e0ae9d9474a5689a5791  0.104864         0.015581  0.060747     0.002958   \n",
      "1   b64a191301cad4f11287  0.224156         0.007160  0.084789     0.004064   \n",
      "2   5e1953d9ae04bdc66408  0.229342         0.037704  0.169231     0.007002   \n",
      "3   23128f98196c8e8f7b90  0.029199         0.001052  0.011225     0.000152   \n",
      "4   2d3f1254f71472bf2b78  0.067878         0.005643  0.027221     0.000576   \n",
      "5   21f4f0f4812a08ea6c28  0.047756         0.002545  0.019989     0.000874   \n",
      "6   733b43d534c67c1be948  0.069829         0.001933  0.027274     0.000130   \n",
      "7   aad47a397f7ddc629d5d  0.059180         0.002935  0.029433     0.000580   \n",
      "8   d19fcde8a3af2e472d74  0.064211         0.002016  0.029168     0.001808   \n",
      "9   7d4de482c60f1c8a79c6  0.262265         0.032701  0.146842     0.006578   \n",
      "10  f81afe094bcd161ea6f8  0.038064         0.002303  0.015282     0.001138   \n",
      "11  132973e40fa63e0d07f1  0.020909         0.000797  0.006228     0.000474   \n",
      "12  37195759bf9104c3b488  0.035469         0.002517  0.014455     0.000959   \n",
      "13  0c46ec0e711469190af8  0.095603         0.006498  0.051875     0.000653   \n",
      "14  ecaeb00c751a61c1370e  0.055042         0.006502  0.034935     0.002285   \n",
      "15  dfaa434123c910477026  0.051874         0.004187  0.027035     0.005118   \n",
      "16  f0e8517a1d9e1394696b  0.032013         0.000891  0.012590     0.000292   \n",
      "17  db952db7fb786093a8f7  0.092549         0.008156  0.045226     0.005600   \n",
      "18  132ad2e7621d9c132c3f  0.036899         0.001554  0.022075     0.001390   \n",
      "19  38eef2c8994e8a980bb1  0.184869         0.013560  0.090095     0.001223   \n",
      "20  160a621ad6ee20736755  0.019055         0.000961  0.008902     0.000261   \n",
      "\n",
      "    disrespect  targeted_hate  \n",
      "0     0.060832       0.009103  \n",
      "1     0.101581       0.025238  \n",
      "2     0.161632       0.023899  \n",
      "3     0.007772       0.000820  \n",
      "4     0.022085       0.002293  \n",
      "5     0.026550       0.001743  \n",
      "6     0.031914       0.002563  \n",
      "7     0.021652       0.003256  \n",
      "8     0.029624       0.003593  \n",
      "9     0.150023       0.024411  \n",
      "10    0.015991       0.003692  \n",
      "11    0.005459       0.001740  \n",
      "12    0.013431       0.003216  \n",
      "13    0.040471       0.006893  \n",
      "14    0.033965       0.020995  \n",
      "15    0.021785       0.003272  \n",
      "16    0.011589       0.001696  \n",
      "17    0.044940       0.009355  \n",
      "18    0.017118       0.001937  \n",
      "19    0.079518       0.008443  \n",
      "20    0.006891       0.001894  \n"
     ]
    }
   ],
   "source": [
    "train_data = X_train_tfidf\n",
    "test_data = X_test_tfidf\n",
    "clf = RFClassifier()\n",
    "csv_filename = \"RFC_tfidf_Prob.csv\"\n",
    "model_name = \"Random Forest Classifier\"\n",
    "data_matrix_name = \"TF-IDF\"\n",
    "execute(clf, train_data, test_data, model_name, data_matrix_name, csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted Model 0\n",
      "Fitted Model 1\n",
      "Fitted Model 2\n",
      "Fitted Model 3\n",
      "Fitted Model 4\n",
      "Fitted Model 5\n",
      "Final class probabilities for first 20 samples of training data:\n",
      "       harsh  extremely_harsh    vulgar  threatening  disrespect  \\\n",
      "0   0.023074         0.001147  0.009783     0.001451    0.005691   \n",
      "1   0.024338         0.000953  0.013179     0.000614    0.010863   \n",
      "2   0.130678         0.008229  0.062590     0.001551    0.056230   \n",
      "3   0.332694         0.057548  0.208173     0.019898    0.195574   \n",
      "4   0.046343         0.003230  0.018780     0.001500    0.019077   \n",
      "5   0.059486         0.002348  0.016149     0.001419    0.016796   \n",
      "6   0.072818         0.003116  0.039424     0.002427    0.024770   \n",
      "7   0.181348         0.018748  0.091742     0.007164    0.085700   \n",
      "8   0.025166         0.001718  0.010624     0.000885    0.009073   \n",
      "9   0.087524         0.003281  0.043526     0.010557    0.037947   \n",
      "10  0.038426         0.001407  0.013914     0.005634    0.013096   \n",
      "11  0.064843         0.006212  0.037388     0.001866    0.032416   \n",
      "12  0.046598         0.001382  0.012389     0.000473    0.017864   \n",
      "13  0.044766         0.002296  0.020089     0.002375    0.014467   \n",
      "14  0.193682         0.017426  0.101001     0.004752    0.103781   \n",
      "15  0.040306         0.000161  0.013069     0.000031    0.010301   \n",
      "16  0.089592         0.003992  0.035714     0.002476    0.037815   \n",
      "17  0.086531         0.008655  0.037804     0.002676    0.037526   \n",
      "18  0.063721         0.004298  0.026576     0.003026    0.020682   \n",
      "19  0.007865         0.001274  0.008139     0.000497    0.005926   \n",
      "20  0.003610         0.000077  0.002296     0.000926    0.002570   \n",
      "\n",
      "    targeted_hate  \n",
      "0        0.000604  \n",
      "1        0.001708  \n",
      "2        0.010531  \n",
      "3        0.037689  \n",
      "4        0.002452  \n",
      "5        0.002435  \n",
      "6        0.004076  \n",
      "7        0.013775  \n",
      "8        0.002117  \n",
      "9        0.005702  \n",
      "10       0.001604  \n",
      "11       0.004289  \n",
      "12       0.003492  \n",
      "13       0.004108  \n",
      "14       0.014310  \n",
      "15       0.001855  \n",
      "16       0.005934  \n",
      "17       0.004921  \n",
      "18       0.002137  \n",
      "19       0.001088  \n",
      "20       0.000571  \n",
      "Final class probabilities for first 20 samples of test data:\n",
      "                      id     harsh  extremely_harsh    vulgar  threatening  \\\n",
      "0   e0ae9d9474a5689a5791  0.115803         0.013199  0.051733     0.001850   \n",
      "1   b64a191301cad4f11287  0.208405         0.007252  0.092596     0.004906   \n",
      "2   5e1953d9ae04bdc66408  0.216089         0.039921  0.136851     0.007248   \n",
      "3   23128f98196c8e8f7b90  0.034238         0.000675  0.011459     0.000016   \n",
      "4   2d3f1254f71472bf2b78  0.071798         0.005995  0.038371     0.001203   \n",
      "5   21f4f0f4812a08ea6c28  0.078036         0.005003  0.041255     0.003716   \n",
      "6   733b43d534c67c1be948  0.063721         0.002508  0.023345     0.000715   \n",
      "7   aad47a397f7ddc629d5d  0.061864         0.003311  0.033401     0.000348   \n",
      "8   d19fcde8a3af2e472d74  0.060186         0.002539  0.027888     0.001311   \n",
      "9   7d4de482c60f1c8a79c6  0.236074         0.025230  0.127230     0.004008   \n",
      "10  f81afe094bcd161ea6f8  0.042379         0.000492  0.015483     0.001452   \n",
      "11  132973e40fa63e0d07f1  0.022640         0.000892  0.009431     0.000234   \n",
      "12  37195759bf9104c3b488  0.035935         0.001477  0.014590     0.004160   \n",
      "13  0c46ec0e711469190af8  0.091635         0.007402  0.048697     0.001027   \n",
      "14  ecaeb00c751a61c1370e  0.065060         0.005232  0.030981     0.003421   \n",
      "15  dfaa434123c910477026  0.066388         0.003744  0.031811     0.002555   \n",
      "16  f0e8517a1d9e1394696b  0.038992         0.000877  0.011154     0.000474   \n",
      "17  db952db7fb786093a8f7  0.095025         0.007965  0.048164     0.002384   \n",
      "18  132ad2e7621d9c132c3f  0.033216         0.001949  0.013727     0.000750   \n",
      "19  38eef2c8994e8a980bb1  0.148290         0.008537  0.082296     0.001634   \n",
      "20  160a621ad6ee20736755  0.025942         0.000678  0.008272     0.000487   \n",
      "\n",
      "    disrespect  targeted_hate  \n",
      "0     0.055941       0.012065  \n",
      "1     0.115519       0.018023  \n",
      "2     0.144161       0.023185  \n",
      "3     0.013184       0.001778  \n",
      "4     0.031460       0.003551  \n",
      "5     0.087265       0.004619  \n",
      "6     0.031189       0.003004  \n",
      "7     0.025752       0.005229  \n",
      "8     0.027419       0.004299  \n",
      "9     0.132909       0.017896  \n",
      "10    0.014488       0.002237  \n",
      "11    0.007115       0.002241  \n",
      "12    0.011392       0.002262  \n",
      "13    0.042692       0.008484  \n",
      "14    0.025686       0.010030  \n",
      "15    0.025393       0.004962  \n",
      "16    0.012386       0.002173  \n",
      "17    0.044808       0.008035  \n",
      "18    0.019191       0.002197  \n",
      "19    0.072704       0.008019  \n",
      "20    0.006978       0.000970  \n"
     ]
    }
   ],
   "source": [
    "train_data = X_train_bow\n",
    "test_data = X_test_bow\n",
    "clf = RFClassifier()\n",
    "csv_filename = \"RFC_bow_Prob.csv\"\n",
    "model_name = \"Random Forest Classifier\"\n",
    "data_matrix_name = \"Bag of Words\"\n",
    "execute(clf, train_data, test_data, model_name, data_matrix_name, csv_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.06 Bagging Classifier (base estimator -> Logistic Regression):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "class BGClassifier(object): #Pack of 6 binary Bagging classifiers\n",
    "    def __init__(self, base_params):\n",
    "        self.param_grid = {\n",
    "            'n_estimators' : [10, 20, 30], #default=10\n",
    "        }\n",
    "        self.models = []\n",
    "        self.base_params = base_params\n",
    "        \n",
    "        for i in range(6): #Appending 6 binary Bagging classifiers (one for each class)\n",
    "            self.models.append(BaggingClassifier(base_estimator=LogisticRegression(max_iter=self.base_params[i]['max_iter'], class_weight=self.base_params[i]['class_weight'], penalty=self.base_params[i]['penalty'], solver=self.base_params[i]['solver'], C=self.base_params[i]['C'], tol=1e-9), max_samples=1.0, n_estimators = 50, oob_score=True))\n",
    "    \n",
    "    def fit(self, XTrain, yTrain, CVScoreType, numFolds): #Fits the models and returns the best CVScores and CVParams after performing grid search CV\n",
    "        CVScores = {}\n",
    "        CVParams = {}\n",
    "        \n",
    "        for i, model in enumerate(self.models):\n",
    "            labels = yTrain[label_type[i]]\n",
    "            \n",
    "            bestParams, bestScore = performGridSearchCV(model, self.param_grid, XTrain, labels, numFolds, CVScoreType)\n",
    "            CVParams[label_type[i]] = bestParams\n",
    "            CVScores[label_type[i]] = bestScore\n",
    "            \n",
    "            self.models[i] = BaggingClassifier(base_estimator=LogisticRegression(max_iter=self.base_params[i]['max_iter'], class_weight=self.base_params[i]['class_weight'], penalty=self.base_params[i]['penalty'], solver=self.base_params[i]['solver'], C=self.base_params[i]['C'], tol=1e-9), max_samples=1.0, n_estimators = bestParams['n_estimators'])\n",
    "            self.models[i].fit(XTrain, labels)\n",
    "            print(\"Fitted Model \"+str(i))\n",
    "            \n",
    "        return CVScores, CVParams\n",
    "    \n",
    "    def fitWithoutGridSearch(self, XTrain, yTrain): #Fits the models without performing grid search CV\n",
    "        for i, model in enumerate(self.models):\n",
    "            labels = yTrain[label_type[i]]\n",
    "            self.models[i].fit(XTrain, labels) \n",
    "            print(\"Fitted Model \"+str(i))\n",
    "    \n",
    "    def predict(self, X): #Returns the binary classification results for each of the models\n",
    "        predictions = {}\n",
    "        for i, model in enumerate(self.models):\n",
    "            predictions[label_type[i]] = model.predict(X)\n",
    "        return predictions\n",
    "    \n",
    "    def predictProbabilities(self, X): #Returns the predicted class probabilities for each of the classes\n",
    "        probabilities = {}\n",
    "        for i, model in enumerate(self.models):\n",
    "            classes = model.classes_\n",
    "            index = np.where(classes == 1)[0][0] #Getting the index of class 1\n",
    "            probabilities[label_type[i]] = model.predict_proba(X)[:, index] #Extracting the probabilities of class 1\n",
    "        return probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now testing out the Bagging classifier on various data matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BP = {\n",
    "            0 : {'max_iter':100000, 'class_weight':'balanced', 'penalty':'l2', 'solver':'lbfgs', 'C':1.9},\n",
    "            1 : {'max_iter':100000, 'class_weight':'balanced', 'penalty':'l2', 'solver':'liblinear', 'C':0.175},\n",
    "            2 : {'max_iter':100000, 'class_weight':'balanced', 'penalty':'l2', 'solver':'liblinear', 'C':1.8},\n",
    "            3 : {'max_iter':100000, 'class_weight':'balanced', 'penalty':'l2', 'solver':'liblinear', 'C':1.6},\n",
    "            4 : {'max_iter':100000, 'class_weight':'balanced', 'penalty':'l2', 'solver':'liblinear', 'C':1.55},\n",
    "            5 : {'max_iter':100000, 'class_weight':'balanced', 'penalty':'l2', 'solver':'liblinear', 'C':2}\n",
    "        } #Parameters for the base estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted Model 0\n",
      "Fitted Model 1\n",
      "Fitted Model 2\n",
      "Fitted Model 3\n",
      "Fitted Model 4\n",
      "Fitted Model 5\n",
      "Final class probabilities for first 20 samples of training data:\n",
      "       harsh  extremely_harsh    vulgar  threatening  disrespect  \\\n",
      "0   0.001298         0.005266  0.003161     0.003070    0.000805   \n",
      "1   0.000401         0.007107  0.001940     0.000666    0.000708   \n",
      "2   0.377775         0.170654  0.037222     0.004246    0.035514   \n",
      "3   0.998096         0.330927  0.983612     0.821934    0.947852   \n",
      "4   0.014563         0.014437  0.015975     0.001037    0.005475   \n",
      "5   0.065050         0.015761  0.023220     0.007002    0.020980   \n",
      "6   0.156611         0.068169  0.089450     0.021285    0.066310   \n",
      "7   0.510772         0.385891  0.221020     0.123913    0.216587   \n",
      "8   0.002165         0.002872  0.005297     0.000470    0.001427   \n",
      "9   0.102574         0.062546  0.023521     0.048813    0.084221   \n",
      "10  0.007415         0.015833  0.003577     0.001774    0.001637   \n",
      "11  0.029497         0.021742  0.018718     0.001231    0.022390   \n",
      "12  0.012734         0.020211  0.009785     0.001928    0.013525   \n",
      "13  0.043828         0.028166  0.029142     0.004413    0.024479   \n",
      "14  0.814178         0.142403  0.148080     0.011993    0.233000   \n",
      "15  0.042881         0.028275  0.022131     0.000910    0.029827   \n",
      "16  0.184991         0.054418  0.116904     0.017901    0.073231   \n",
      "17  0.309415         0.069026  0.049145     0.019325    0.095297   \n",
      "18  0.126754         0.016627  0.039348     0.003812    0.021654   \n",
      "19  0.020206         0.018024  0.023031     0.011956    0.037405   \n",
      "20  0.000966         0.001861  0.001922     0.000131    0.001176   \n",
      "\n",
      "    targeted_hate  \n",
      "0        0.000326  \n",
      "1        0.000385  \n",
      "2        0.008545  \n",
      "3        0.143155  \n",
      "4        0.000863  \n",
      "5        0.006684  \n",
      "6        0.010057  \n",
      "7        0.060568  \n",
      "8        0.000738  \n",
      "9        0.013339  \n",
      "10       0.000932  \n",
      "11       0.008379  \n",
      "12       0.004964  \n",
      "13       0.006086  \n",
      "14       0.016591  \n",
      "15       0.003683  \n",
      "16       0.022109  \n",
      "17       0.010173  \n",
      "18       0.003538  \n",
      "19       0.003883  \n",
      "20       0.000132  \n",
      "Final class probabilities for first 20 samples of test data:\n",
      "                      id     harsh  extremely_harsh    vulgar  threatening  \\\n",
      "0   e0ae9d9474a5689a5791  0.119173         0.100050  0.030183     0.009914   \n",
      "1   b64a191301cad4f11287  0.833178         0.019748  0.137193     0.001814   \n",
      "2   5e1953d9ae04bdc66408  0.664307         0.165075  0.246178     0.048000   \n",
      "3   23128f98196c8e8f7b90  0.032119         0.024432  0.030786     0.000925   \n",
      "4   2d3f1254f71472bf2b78  0.108151         0.045299  0.038161     0.001896   \n",
      "5   21f4f0f4812a08ea6c28  0.492544         0.007668  0.032818     0.001088   \n",
      "6   733b43d534c67c1be948  0.108963         0.018536  0.032305     0.000528   \n",
      "7   aad47a397f7ddc629d5d  0.019761         0.026216  0.016736     0.002541   \n",
      "8   d19fcde8a3af2e472d74  0.330847         0.060605  0.092714     0.003597   \n",
      "9   7d4de482c60f1c8a79c6  0.630236         0.432614  0.305744     0.042993   \n",
      "10  f81afe094bcd161ea6f8  0.039984         0.009495  0.014146     0.003013   \n",
      "11  132973e40fa63e0d07f1  0.012272         0.003417  0.005160     0.000466   \n",
      "12  37195759bf9104c3b488  0.016014         0.008395  0.014671     0.001734   \n",
      "13  0c46ec0e711469190af8  0.122311         0.079606  0.031340     0.005513   \n",
      "14  ecaeb00c751a61c1370e  0.227624         0.076736  0.452457     0.015428   \n",
      "15  dfaa434123c910477026  0.092198         0.033804  0.018376     0.005169   \n",
      "16  f0e8517a1d9e1394696b  0.067206         0.009710  0.037334     0.000695   \n",
      "17  db952db7fb786093a8f7  0.034304         0.047696  0.022834     0.006015   \n",
      "18  132ad2e7621d9c132c3f  0.004405         0.009818  0.005856     0.000992   \n",
      "19  38eef2c8994e8a980bb1  0.085473         0.080935  0.046209     0.002426   \n",
      "20  160a621ad6ee20736755  0.015240         0.005748  0.008200     0.000946   \n",
      "\n",
      "    disrespect  targeted_hate  \n",
      "0     0.046739       0.007583  \n",
      "1     0.287045       0.034455  \n",
      "2     0.416076       0.032993  \n",
      "3     0.017530       0.007590  \n",
      "4     0.027497       0.002984  \n",
      "5     0.057981       0.002775  \n",
      "6     0.051623       0.004085  \n",
      "7     0.010136       0.006109  \n",
      "8     0.075485       0.004611  \n",
      "9     0.302077       0.139225  \n",
      "10    0.029819       0.007147  \n",
      "11    0.005493       0.001204  \n",
      "12    0.012865       0.002094  \n",
      "13    0.076564       0.012461  \n",
      "14    0.094139       0.150485  \n",
      "15    0.010704       0.005138  \n",
      "16    0.016661       0.006372  \n",
      "17    0.025223       0.008929  \n",
      "18    0.006820       0.000792  \n",
      "19    0.063984       0.008427  \n",
      "20    0.008610       0.002438  \n"
     ]
    }
   ],
   "source": [
    "train_data = X_train_tfidf\n",
    "test_data = X_test_tfidf\n",
    "clf = BGClassifier(BP)\n",
    "csv_filename = \"BGC_tfidf_Prob.csv\"\n",
    "model_name = \"Bagging Classifier\"\n",
    "data_matrix_name = \"TF-IDF\"\n",
    "#execute(clf, train_data, test_data, model_name, data_matrix_name, csv_filename, \"WGS\")\n",
    "execute(clf, train_data, test_data, model_name, data_matrix_name, csv_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.07 AdaBoost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "class ADBClassifier(object): #Pack of 6 binary AdaBoost classifiers\n",
    "    def __init__(self):\n",
    "        self.param_grid = {\n",
    "            'n_estimators' : [5] #default=50\n",
    "        }\n",
    "        self.models = []\n",
    "        \n",
    "        for i in range(6):\n",
    "            self.models.append(AdaBoostClassifier(base_estimator=LogisticRegression(max_iter=100000, class_weight='balanced', penalty='l2')))\n",
    "    \n",
    "    def fit(self, XTrain, yTrain, CVScoreType, numFolds): #Fits the models and returns the CVScores and CVParams after performing cross validation\n",
    "        CVScores = {}\n",
    "        CVParams = {}\n",
    "        \n",
    "        for i, model in enumerate(self.models):\n",
    "            labels = yTrain[label_type[i]]\n",
    "            \n",
    "            bestParams, bestScore = performGridSearchCV(model, self.param_grid, XTrain, labels, numFolds, CVScoreType)\n",
    "            CVParams[label_type[i]] = bestParams\n",
    "            CVScores[label_type[i]] = bestScore\n",
    "            \n",
    "            self.models[i] = AdaBoostClassifier(n_estimators = bestParams['n_estimators'], base_estimator=LogisticRegression(max_iter=100000, class_weight='balanced', penalty='l2'))\n",
    "            self.models[i].fit(XTrain, labels)\n",
    "            print(\"Fitted Model \"+str(i))\n",
    "            \n",
    "        return CVScores, CVParams\n",
    "    \n",
    "    def fitWithoutGridSearch(self, XTrain, yTrain):\n",
    "        for i, model in enumerate(self.models):\n",
    "            labels = yTrain[label_type[i]]\n",
    "            self.models[i].fit(XTrain, labels) \n",
    "            print(\"Fitted Model \"+str(i))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = {}\n",
    "        for i, model in enumerate(self.models):\n",
    "            predictions[label_type[i]] = model.predict(X)\n",
    "        return predictions\n",
    "    \n",
    "    def predictProbabilities(self, X):\n",
    "        probabilities = {}\n",
    "        for i, model in enumerate(self.models):\n",
    "            classes = model.classes_\n",
    "            index = np.where(classes == 1)[0][0] #Getting the index of class 1\n",
    "            probabilities[label_type[i]] = model.predict_proba(X)[:, index] #Extracting the probabilities of class 1\n",
    "        return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = X_train_tfidf\n",
    "test_data = X_test_tfidf\n",
    "clf = ADBClassifier()\n",
    "csv_filename = \"ADBC_tfidf_Prob.csv\"\n",
    "model_name = \"AdaBoost Classifier\"\n",
    "data_matrix_name = \"TF-IDF\"\n",
    "execute(clf, train_data, test_data, model_name, data_matrix_name, csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = X_train_bow\n",
    "test_data = X_test_bow\n",
    "clf = ADBClassifier()\n",
    "csv_filename = \"ADBC_bow_Prob.csv\"\n",
    "model_name = \"AdaBoost Classifier\"\n",
    "data_matrix_name = \"Bag of Words\"\n",
    "execute(clf, train_data, test_data, model_name, data_matrix_name, csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = X_train_PV_DM \n",
    "test_data = X_test_PV_DM \n",
    "clf = ADBClassifier()\n",
    "csv_filename = \"ADBC_PV_DM_Prob.csv\"\n",
    "model_name = \"AdaBoost Classifier\"\n",
    "data_matrix_name = \"Paragraph Vector - Distributed Memory\"\n",
    "execute(clf, train_data, test_data, model_name, data_matrix_name, csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = X_train_PV_DBOW\n",
    "test_data = X_test_PV_DBOW \n",
    "clf = ADBClassifier()\n",
    "csv_filename = \"ADBC_PV_DBOW_Prob.csv\"\n",
    "model_name = \"AdaBoost Classifier\"\n",
    "data_matrix_name = \"Paragraph Vector - Distributed Bag of Words\"\n",
    "execute(clf, train_data, test_data, model_name, data_matrix_name, csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = X_train_W2V_method_1\n",
    "test_data = X_test_W2V_method_1\n",
    "clf = ADBClassifier()\n",
    "csv_filename = \"ADBC_W2V_method_1_Prob.csv\"\n",
    "model_name = \"AdaBoost Classifier\"\n",
    "data_matrix_name = \"Word2Vec Method_1\"\n",
    "execute(clf, train_data, test_data, model_name, data_matrix_name, csv_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.08 K-Nearest Neighbors Classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "class KNNClassifier(object): #Pack of 6 binary K-Nearest Neighbors classifiers\n",
    "    def __init__(self, n_neighbors=5, algorithm='auto'):\n",
    "        self.param_grid = {\n",
    "            'n_neighbors' : [5] #default=5\n",
    "        }\n",
    "        self.models = []\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.algorithm = algorithm\n",
    "        \n",
    "        for i in range(6):\n",
    "            self.models.append(KNeighborsClassifier(n_neighbors=self.n_neighbors, algorithm=self.algorithm))\n",
    "    \n",
    "    def fit(self, XTrain, yTrain, CVScoreType, numFolds): #Fits the models and returns the CVScores and CVParams after performing cross validation\n",
    "        CVScores = {}\n",
    "        CVParams = {}\n",
    "        \n",
    "        for i, model in enumerate(self.models):\n",
    "            labels = yTrain[label_type[i]]\n",
    "            \n",
    "            bestParams, bestScore = performGridSearchCV(model, self.param_grid, XTrain, labels, numFolds, CVScoreType)\n",
    "            CVParams[label_type[i]] = bestParams\n",
    "            CVScores[label_type[i]] = bestScore\n",
    "            \n",
    "            self.models[i] = KNeighborsClassifier(n_neighbors=bestParams['n_neighbors'], algorithm=self.algorithm)\n",
    "            self.models[i].fit(XTrain, labels)\n",
    "            print(\"Fitted Model \"+str(i))\n",
    "            \n",
    "        return CVScores, CVParams\n",
    "    \n",
    "    def fitWithoutGridSearch(self, XTrain, yTrain):\n",
    "        for i, model in enumerate(self.models):\n",
    "            labels = yTrain[label_type[i]]\n",
    "            self.models[i].fit(XTrain, labels) \n",
    "            print(\"Fitted Model \"+str(i))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = {}\n",
    "        for i, model in enumerate(self.models):\n",
    "            predictions[label_type[i]] = model.predict(X)\n",
    "        return predictions\n",
    "    \n",
    "    def predictProbabilities(self, X):\n",
    "        probabilities = {}\n",
    "        for i, model in enumerate(self.models):\n",
    "            classes = model.classes_\n",
    "            index = np.where(classes == 1)[0][0] #Getting the index of class 1\n",
    "            probabilities[label_type[i]] = model.predict_proba(X)[:, index] #Extracting the probabilities of class 1\n",
    "        return probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now testing out the KNN Classifier on various data matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = X_train_PV_DBOW\n",
    "test_data = X_test_PV_DBOW\n",
    "clf = KNNClassifier()\n",
    "csv_filename = \"KNNC_PV_DBOW_Prob.csv\"\n",
    "model_name = \"KNN Classifier\"\n",
    "data_matrix_name = \"Paragraph Vector - Distributed Bag Of Words\"\n",
    "execute(clf, train_data, test_data, model_name, data_matrix_name, csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = X_train_PV_DM\n",
    "test_data = X_test_PV_DM\n",
    "clf = KNNClassifier()\n",
    "csv_filename = \"KNNC_PV_DM_Prob.csv\"\n",
    "model_name = \"KNN Classifier\"\n",
    "data_matrix_name = \"Paragraph Vector - Distributed Memory\"\n",
    "execute(clf, train_data, test_data, model_name, data_matrix_name, csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = X_train_W2V_method_1\n",
    "test_data = X_test_W2V_method_1\n",
    "clf = KNNClassifier()\n",
    "csv_filename = \"KNNC_W2V_method_1_Prob.csv\"\n",
    "model_name = \"KNN Classifier\"\n",
    "data_matrix_name = \"Word2Vec Method_1\"\n",
    "execute(clf, train_data, test_data, model_name, data_matrix_name, csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = X_train_tfidf\n",
    "test_data = X_test_tfidf\n",
    "clf = KNNClassifier()\n",
    "csv_filename = \"KNNC_tfidf_Prob.csv\"\n",
    "model_name = \"KNN Classifier\"\n",
    "data_matrix_name = \"TF-IDF\"\n",
    "# execute(clf, train_data, test_data, model_name, data_matrix_name, csv_filename, \"WGS\")\n",
    "execute(clf, train_data, test_data, model_name, data_matrix_name, csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = X_train_bow\n",
    "test_data = X_test_bow\n",
    "clf = KNNClassifier()\n",
    "csv_filename = \"KNNC_bow_Prob.csv\"\n",
    "model_name = \"KNN Classifier\"\n",
    "data_matrix_name = \"Bag of Words\"\n",
    "execute(clf, train_data, test_data, model_name, data_matrix_name, csv_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.09 Stochastic Gradient Descent Classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "class StochasticGDClassifier(object): #Pack of 6 binary Stochastic Gradient Descent classifiers\n",
    "    def __init__(self):\n",
    "        self.param_grid = {\n",
    "            'loss' : ['modified_huber']\n",
    "        }\n",
    "        self.models = []\n",
    "        \n",
    "        for i in range(6):\n",
    "            self.models.append(SGDClassifier(loss='modified_huber'))\n",
    "    \n",
    "    def fit(self, XTrain, yTrain, CVScoreType, numFolds): #Fits the models and returns the CVScores and CVParams after performing cross validation\n",
    "        CVScores = {}\n",
    "        CVParams = {}\n",
    "        \n",
    "        for i, model in enumerate(self.models):\n",
    "            labels = yTrain[label_type[i]]\n",
    "            \n",
    "            bestParams, bestScore = performGridSearchCV(model, self.param_grid, XTrain, labels, numFolds, CVScoreType)\n",
    "            CVParams[label_type[i]] = bestParams\n",
    "            CVScores[label_type[i]] = bestScore\n",
    "            \n",
    "            self.models[i] = SGDClassifier(loss=bestParams['loss'])\n",
    "            self.models[i].fit(XTrain, labels)\n",
    "            print(\"Fitted Model \"+str(i))\n",
    "            \n",
    "        return CVScores, CVParams\n",
    "    \n",
    "    def fitWithoutGridSearch(self, XTrain, yTrain):\n",
    "        for i, model in enumerate(self.models):\n",
    "            labels = yTrain[label_type[i]]\n",
    "            self.models[i].fit(XTrain, labels) \n",
    "            print(\"Fitted Model \"+str(i))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = {}\n",
    "        for i, model in enumerate(self.models):\n",
    "            predictions[label_type[i]] = model.predict(X)\n",
    "        return predictions\n",
    "    \n",
    "    def predictProbabilities(self, X):\n",
    "        probabilities = {}\n",
    "        for i, model in enumerate(self.models):\n",
    "            classes = model.classes_\n",
    "            index = np.where(classes == 1)[0][0] #Getting the index of class 1\n",
    "            probabilities[label_type[i]] = model.predict_proba(X)[:, index] #Extracting the probabilities of class 1\n",
    "        return probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now testing out the Stochastic Gradient Descent classifier on various data matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted Model 0\n",
      "Fitted Model 1\n",
      "Fitted Model 2\n",
      "Fitted Model 3\n",
      "Fitted Model 4\n",
      "Fitted Model 5\n",
      "Final class probabilities for first 20 samples of training data:\n",
      "       harsh  extremely_harsh    vulgar  threatening  disrespect  \\\n",
      "0   0.000000         0.000000  0.000000     0.000000    0.000000   \n",
      "1   0.000000         0.000000  0.000000     0.000000    0.000000   \n",
      "2   0.179472         0.078281  0.026050     0.000000    0.010527   \n",
      "3   0.985254         0.115066  0.671012     0.336576    0.549389   \n",
      "4   0.000000         0.000000  0.000000     0.000000    0.000000   \n",
      "5   0.024415         0.000000  0.013129     0.000000    0.000000   \n",
      "6   0.079299         0.000000  0.048603     0.000000    0.056246   \n",
      "7   0.225894         0.036536  0.101335     0.044807    0.057467   \n",
      "8   0.000000         0.000000  0.000000     0.000000    0.000000   \n",
      "9   0.028663         0.000000  0.000000     0.026210    0.042728   \n",
      "10  0.000000         0.000000  0.000000     0.000000    0.000000   \n",
      "11  0.000000         0.000000  0.000000     0.000000    0.000000   \n",
      "12  0.000000         0.000000  0.000000     0.000000    0.000000   \n",
      "13  0.000000         0.000000  0.000000     0.000000    0.000000   \n",
      "14  0.436539         0.000000  0.066783     0.000000    0.114808   \n",
      "15  0.000000         0.000000  0.000000     0.000000    0.000000   \n",
      "16  0.061329         0.000000  0.033606     0.000000    0.000000   \n",
      "17  0.134765         0.000000  0.000000     0.000000    0.000000   \n",
      "18  0.020595         0.000000  0.000000     0.000000    0.000000   \n",
      "19  0.000000         0.000000  0.000000     0.002722    0.000000   \n",
      "20  0.000000         0.000000  0.000000     0.000000    0.000000   \n",
      "\n",
      "    targeted_hate  \n",
      "0        0.000000  \n",
      "1        0.000000  \n",
      "2        0.000000  \n",
      "3        0.012995  \n",
      "4        0.000000  \n",
      "5        0.000000  \n",
      "6        0.000000  \n",
      "7        0.000000  \n",
      "8        0.000000  \n",
      "9        0.000000  \n",
      "10       0.000000  \n",
      "11       0.000000  \n",
      "12       0.000000  \n",
      "13       0.000000  \n",
      "14       0.000000  \n",
      "15       0.000000  \n",
      "16       0.000000  \n",
      "17       0.000000  \n",
      "18       0.000000  \n",
      "19       0.000000  \n",
      "20       0.000000  \n",
      "Final class probabilities for first 20 samples of test data:\n",
      "                      id     harsh  extremely_harsh    vulgar  threatening  \\\n",
      "0   e0ae9d9474a5689a5791  0.124318         0.000000  0.000000          0.0   \n",
      "1   b64a191301cad4f11287  0.390832         0.000000  0.078350          0.0   \n",
      "2   5e1953d9ae04bdc66408  0.334158         0.000000  0.112690          0.0   \n",
      "3   23128f98196c8e8f7b90  0.000000         0.000000  0.000000          0.0   \n",
      "4   2d3f1254f71472bf2b78  0.000000         0.000000  0.007954          0.0   \n",
      "5   21f4f0f4812a08ea6c28  0.264667         0.000000  0.003019          0.0   \n",
      "6   733b43d534c67c1be948  0.022690         0.000000  0.000000          0.0   \n",
      "7   aad47a397f7ddc629d5d  0.000000         0.000000  0.000000          0.0   \n",
      "8   d19fcde8a3af2e472d74  0.187847         0.000000  0.086214          0.0   \n",
      "9   7d4de482c60f1c8a79c6  0.296478         0.081303  0.093215          0.0   \n",
      "10  f81afe094bcd161ea6f8  0.000000         0.000000  0.000000          0.0   \n",
      "11  132973e40fa63e0d07f1  0.000000         0.000000  0.000000          0.0   \n",
      "12  37195759bf9104c3b488  0.000000         0.000000  0.000000          0.0   \n",
      "13  0c46ec0e711469190af8  0.025227         0.000000  0.000000          0.0   \n",
      "14  ecaeb00c751a61c1370e  0.165940         0.000000  0.230375          0.0   \n",
      "15  dfaa434123c910477026  0.003180         0.000000  0.000000          0.0   \n",
      "16  f0e8517a1d9e1394696b  0.000000         0.000000  0.005313          0.0   \n",
      "17  db952db7fb786093a8f7  0.000000         0.000000  0.000000          0.0   \n",
      "18  132ad2e7621d9c132c3f  0.000000         0.000000  0.000000          0.0   \n",
      "19  38eef2c8994e8a980bb1  0.008971         0.000000  0.000000          0.0   \n",
      "20  160a621ad6ee20736755  0.000000         0.000000  0.000000          0.0   \n",
      "\n",
      "    disrespect  targeted_hate  \n",
      "0     0.000000       0.000000  \n",
      "1     0.139008       0.000000  \n",
      "2     0.207020       0.000000  \n",
      "3     0.000000       0.000000  \n",
      "4     0.000000       0.000000  \n",
      "5     0.024234       0.000000  \n",
      "6     0.000000       0.000000  \n",
      "7     0.000000       0.000000  \n",
      "8     0.044132       0.000000  \n",
      "9     0.146039       0.003917  \n",
      "10    0.000000       0.000000  \n",
      "11    0.000000       0.000000  \n",
      "12    0.000000       0.000000  \n",
      "13    0.000000       0.000000  \n",
      "14    0.000000       0.033668  \n",
      "15    0.000000       0.000000  \n",
      "16    0.000000       0.000000  \n",
      "17    0.000000       0.000000  \n",
      "18    0.000000       0.000000  \n",
      "19    0.000000       0.000000  \n",
      "20    0.000000       0.000000  \n"
     ]
    }
   ],
   "source": [
    "train_data = X_train_tfidf\n",
    "test_data = X_test_tfidf\n",
    "clf = StochasticGDClassifier()\n",
    "csv_filename = \"SGDC_tfidf_Prob.csv\"\n",
    "model_name = \"SGD Classifier\"\n",
    "data_matrix_name = \"TF-IDF\"\n",
    "# execute(clf, train_data, test_data, model_name, data_matrix_name, csv_filename, \"WGS\")\n",
    "execute(clf, train_data, test_data, model_name, data_matrix_name, csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted Model 0\n",
      "Fitted Model 1\n",
      "Fitted Model 2\n",
      "Fitted Model 3\n",
      "Fitted Model 4\n",
      "Fitted Model 5\n",
      "Final class probabilities for first 20 samples of training data:\n",
      "       harsh  extremely_harsh  vulgar  threatening  disrespect  targeted_hate\n",
      "0   0.000000         0.000000     0.0          0.0         0.0            0.0\n",
      "1   0.000000         0.000000     0.0          0.0         0.0            0.0\n",
      "2   0.000000         0.000000     0.0          0.0         0.0            0.0\n",
      "3   1.000000         0.367315     1.0          1.0         1.0            0.0\n",
      "4   0.000000         0.000000     0.0          0.0         0.0            0.0\n",
      "5   0.000000         0.000000     0.0          0.0         0.0            0.0\n",
      "6   0.000000         0.000000     0.0          0.0         0.0            0.0\n",
      "7   0.000000         0.000000     0.0          0.0         0.0            0.0\n",
      "8   0.000000         0.000000     0.0          0.0         0.0            0.0\n",
      "9   0.000000         0.000000     0.0          0.0         0.0            0.0\n",
      "10  0.000000         0.000000     0.0          0.0         0.0            0.0\n",
      "11  0.000000         0.000000     0.0          0.0         0.0            0.0\n",
      "12  0.000000         0.000000     0.0          0.0         0.0            0.0\n",
      "13  0.000000         0.000000     0.0          0.0         0.0            0.0\n",
      "14  0.717282         0.000000     0.0          0.0         0.0            0.0\n",
      "15  0.000000         0.000000     0.0          0.0         0.0            0.0\n",
      "16  0.000000         0.000000     0.0          0.0         0.0            0.0\n",
      "17  0.000000         0.000000     0.0          0.0         0.0            0.0\n",
      "18  0.000000         0.000000     0.0          0.0         0.0            0.0\n",
      "19  0.000000         0.000000     0.0          0.0         0.0            0.0\n",
      "20  0.000000         0.000000     0.0          0.0         0.0            0.0\n",
      "Final class probabilities for first 20 samples of test data:\n",
      "                      id     harsh  extremely_harsh  vulgar  threatening  \\\n",
      "0   e0ae9d9474a5689a5791  0.000000              0.0     0.0          0.0   \n",
      "1   b64a191301cad4f11287  1.000000              0.0     0.0          0.0   \n",
      "2   5e1953d9ae04bdc66408  0.000000              0.0     0.0          0.0   \n",
      "3   23128f98196c8e8f7b90  0.000000              0.0     0.0          0.0   \n",
      "4   2d3f1254f71472bf2b78  0.000000              0.0     0.0          0.0   \n",
      "5   21f4f0f4812a08ea6c28  0.247799              0.0     0.0          0.0   \n",
      "6   733b43d534c67c1be948  0.000000              0.0     0.0          0.0   \n",
      "7   aad47a397f7ddc629d5d  0.000000              0.0     0.0          0.0   \n",
      "8   d19fcde8a3af2e472d74  0.447996              0.0     0.0          0.0   \n",
      "9   7d4de482c60f1c8a79c6  0.000000              0.0     0.0          0.0   \n",
      "10  f81afe094bcd161ea6f8  0.000000              0.0     0.0          0.0   \n",
      "11  132973e40fa63e0d07f1  0.000000              0.0     0.0          0.0   \n",
      "12  37195759bf9104c3b488  0.000000              0.0     0.0          0.0   \n",
      "13  0c46ec0e711469190af8  0.000000              0.0     0.0          0.0   \n",
      "14  ecaeb00c751a61c1370e  0.000000              0.0     0.0          0.0   \n",
      "15  dfaa434123c910477026  0.000000              0.0     0.0          0.0   \n",
      "16  f0e8517a1d9e1394696b  0.000000              0.0     0.0          0.0   \n",
      "17  db952db7fb786093a8f7  0.000000              0.0     0.0          0.0   \n",
      "18  132ad2e7621d9c132c3f  0.000000              0.0     0.0          0.0   \n",
      "19  38eef2c8994e8a980bb1  0.000000              0.0     0.0          0.0   \n",
      "20  160a621ad6ee20736755  0.000000              0.0     0.0          0.0   \n",
      "\n",
      "    disrespect  targeted_hate  \n",
      "0          0.0            0.0  \n",
      "1          0.0            0.0  \n",
      "2          0.0            0.0  \n",
      "3          0.0            0.0  \n",
      "4          0.0            0.0  \n",
      "5          0.0            0.0  \n",
      "6          0.0            0.0  \n",
      "7          0.0            0.0  \n",
      "8          0.0            0.0  \n",
      "9          0.0            0.0  \n",
      "10         0.0            0.0  \n",
      "11         0.0            0.0  \n",
      "12         0.0            0.0  \n",
      "13         0.0            0.0  \n",
      "14         0.0            0.0  \n",
      "15         0.0            0.0  \n",
      "16         0.0            0.0  \n",
      "17         0.0            0.0  \n",
      "18         0.0            0.0  \n",
      "19         0.0            0.0  \n",
      "20         0.0            0.0  \n"
     ]
    }
   ],
   "source": [
    "train_data = X_train_bow\n",
    "test_data = X_test_bow\n",
    "clf = StochasticGDClassifier()\n",
    "csv_filename = \"SGDC_bow_Prob.csv\"\n",
    "model_name = \"SGD Classifier\"\n",
    "data_matrix_name = \"Bag of Words\"\n",
    "execute(clf, train_data, test_data, model_name, data_matrix_name, csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted Model 0\n",
      "Fitted Model 1\n",
      "Fitted Model 2\n",
      "Fitted Model 3\n",
      "Fitted Model 4\n",
      "Fitted Model 5\n",
      "Final class probabilities for first 20 samples of training data:\n",
      "    harsh  extremely_harsh  vulgar  threatening  disrespect  targeted_hate\n",
      "0     0.0              0.0     0.0          0.0         0.0            0.0\n",
      "1     0.0              0.0     0.0          0.0         0.0            0.0\n",
      "2     0.0              0.0     0.0          0.0         0.0            0.0\n",
      "3     1.0              0.0     0.0          1.0         1.0            0.0\n",
      "4     0.0              0.0     0.0          0.0         0.0            0.0\n",
      "5     0.0              0.0     0.0          0.0         0.0            0.0\n",
      "6     0.0              0.0     0.0          0.0         0.0            0.0\n",
      "7     1.0              0.0     0.0          0.0         0.0            0.0\n",
      "8     0.0              0.0     0.0          0.0         0.0            0.0\n",
      "9     0.0              0.0     0.0          0.0         0.0            0.0\n",
      "10    0.0              0.0     0.0          0.0         0.0            0.0\n",
      "11    0.0              0.0     0.0          0.0         0.0            0.0\n",
      "12    0.0              0.0     0.0          0.0         0.0            0.0\n",
      "13    1.0              0.0     0.0          0.0         0.0            0.0\n",
      "14    0.0              0.0     0.0          0.0         0.0            0.0\n",
      "15    0.0              0.0     0.0          0.0         0.0            0.0\n",
      "16    0.0              0.0     0.0          0.0         0.0            0.0\n",
      "17    1.0              0.0     0.0          0.0         0.0            0.0\n",
      "18    0.0              0.0     0.0          0.0         0.0            0.0\n",
      "19    0.0              0.0     0.0          0.0         0.0            0.0\n",
      "20    0.0              0.0     0.0          0.0         0.0            0.0\n",
      "Final class probabilities for first 20 samples of test data:\n",
      "                      id  harsh  extremely_harsh  vulgar  threatening  \\\n",
      "0   e0ae9d9474a5689a5791    0.0              0.0     0.0          0.0   \n",
      "1   b64a191301cad4f11287    0.0              0.0     0.0          0.0   \n",
      "2   5e1953d9ae04bdc66408    0.0              0.0     0.0          0.0   \n",
      "3   23128f98196c8e8f7b90    0.0              0.0     0.0          0.0   \n",
      "4   2d3f1254f71472bf2b78    0.0              0.0     0.0          0.0   \n",
      "5   21f4f0f4812a08ea6c28    0.0              0.0     0.0          0.0   \n",
      "6   733b43d534c67c1be948    0.0              0.0     0.0          0.0   \n",
      "7   aad47a397f7ddc629d5d    0.0              0.0     0.0          0.0   \n",
      "8   d19fcde8a3af2e472d74    0.0              0.0     0.0          0.0   \n",
      "9   7d4de482c60f1c8a79c6    0.0              0.0     0.0          0.0   \n",
      "10  f81afe094bcd161ea6f8    0.0              0.0     0.0          0.0   \n",
      "11  132973e40fa63e0d07f1    0.0              0.0     0.0          0.0   \n",
      "12  37195759bf9104c3b488    0.0              0.0     0.0          0.0   \n",
      "13  0c46ec0e711469190af8    0.0              0.0     0.0          0.0   \n",
      "14  ecaeb00c751a61c1370e    0.0              0.0     0.0          0.0   \n",
      "15  dfaa434123c910477026    0.0              0.0     0.0          0.0   \n",
      "16  f0e8517a1d9e1394696b    0.0              0.0     0.0          0.0   \n",
      "17  db952db7fb786093a8f7    0.0              0.0     0.0          0.0   \n",
      "18  132ad2e7621d9c132c3f    0.0              0.0     0.0          0.0   \n",
      "19  38eef2c8994e8a980bb1    0.0              0.0     0.0          0.0   \n",
      "20  160a621ad6ee20736755    0.0              0.0     0.0          0.0   \n",
      "\n",
      "    disrespect  targeted_hate  \n",
      "0          0.0            0.0  \n",
      "1          0.0            0.0  \n",
      "2          0.0            0.0  \n",
      "3          0.0            0.0  \n",
      "4          0.0            0.0  \n",
      "5          0.0            0.0  \n",
      "6          0.0            0.0  \n",
      "7          0.0            0.0  \n",
      "8          0.0            0.0  \n",
      "9          0.0            0.0  \n",
      "10         0.0            0.0  \n",
      "11         0.0            0.0  \n",
      "12         0.0            0.0  \n",
      "13         0.0            0.0  \n",
      "14         0.0            0.0  \n",
      "15         0.0            0.0  \n",
      "16         0.0            0.0  \n",
      "17         0.0            0.0  \n",
      "18         0.0            0.0  \n",
      "19         0.0            0.0  \n",
      "20         0.0            0.0  \n"
     ]
    }
   ],
   "source": [
    "train_data = X_train_PV_DM\n",
    "test_data = X_test_PV_DM\n",
    "clf = StochasticGDClassifier()\n",
    "csv_filename = \"SGDC_PV_DM_Prob.csv\"\n",
    "model_name = \"SGD Classifier\"\n",
    "data_matrix_name = \"Paragraph Vector - Distributed Memory\"\n",
    "execute(clf, train_data, test_data, model_name, data_matrix_name, csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted Model 0\n",
      "Fitted Model 1\n",
      "Fitted Model 2\n",
      "Fitted Model 3\n",
      "Fitted Model 4\n",
      "Fitted Model 5\n",
      "Final class probabilities for first 20 samples of training data:\n",
      "    harsh  extremely_harsh  vulgar  threatening  disrespect  targeted_hate\n",
      "0     0.0              0.0     0.0          0.0         0.0            0.0\n",
      "1     0.0              0.0     0.0          0.0         0.0            0.0\n",
      "2     0.0              0.0     0.0          0.0         0.0            0.0\n",
      "3     1.0              0.0     1.0          1.0         1.0            0.0\n",
      "4     0.0              0.0     0.0          0.0         0.0            0.0\n",
      "5     0.0              0.0     0.0          0.0         0.0            0.0\n",
      "6     0.0              0.0     0.0          0.0         0.0            0.0\n",
      "7     0.0              0.0     0.0          0.0         0.0            0.0\n",
      "8     0.0              0.0     0.0          0.0         0.0            0.0\n",
      "9     0.0              0.0     0.0          0.0         0.0            0.0\n",
      "10    0.0              0.0     0.0          0.0         0.0            0.0\n",
      "11    0.0              0.0     0.0          0.0         0.0            0.0\n",
      "12    0.0              0.0     0.0          0.0         0.0            0.0\n",
      "13    0.0              0.0     0.0          0.0         0.0            0.0\n",
      "14    0.0              0.0     0.0          0.0         0.0            0.0\n",
      "15    0.0              0.0     0.0          0.0         0.0            0.0\n",
      "16    0.0              0.0     0.0          0.0         0.0            0.0\n",
      "17    1.0              0.0     0.0          0.0         0.0            0.0\n",
      "18    0.0              0.0     0.0          0.0         0.0            0.0\n",
      "19    0.0              0.0     0.0          0.0         0.0            0.0\n",
      "20    0.0              0.0     0.0          0.0         0.0            0.0\n",
      "Final class probabilities for first 20 samples of test data:\n",
      "                      id  harsh  extremely_harsh  vulgar  threatening  \\\n",
      "0   e0ae9d9474a5689a5791    0.0              0.0     0.0          0.0   \n",
      "1   b64a191301cad4f11287    0.0              0.0     0.0          0.0   \n",
      "2   5e1953d9ae04bdc66408    0.0              0.0     0.0          0.0   \n",
      "3   23128f98196c8e8f7b90    0.0              0.0     0.0          0.0   \n",
      "4   2d3f1254f71472bf2b78    0.0              0.0     0.0          0.0   \n",
      "5   21f4f0f4812a08ea6c28    0.0              0.0     0.0          0.0   \n",
      "6   733b43d534c67c1be948    0.0              0.0     0.0          0.0   \n",
      "7   aad47a397f7ddc629d5d    0.0              0.0     0.0          0.0   \n",
      "8   d19fcde8a3af2e472d74    0.0              0.0     0.0          0.0   \n",
      "9   7d4de482c60f1c8a79c6    0.0              0.0     0.0          0.0   \n",
      "10  f81afe094bcd161ea6f8    0.0              0.0     0.0          0.0   \n",
      "11  132973e40fa63e0d07f1    0.0              0.0     0.0          0.0   \n",
      "12  37195759bf9104c3b488    0.0              0.0     0.0          0.0   \n",
      "13  0c46ec0e711469190af8    0.0              0.0     0.0          0.0   \n",
      "14  ecaeb00c751a61c1370e    1.0              0.0     0.0          0.0   \n",
      "15  dfaa434123c910477026    0.0              0.0     0.0          0.0   \n",
      "16  f0e8517a1d9e1394696b    0.0              0.0     0.0          0.0   \n",
      "17  db952db7fb786093a8f7    0.0              0.0     0.0          0.0   \n",
      "18  132ad2e7621d9c132c3f    0.0              0.0     0.0          0.0   \n",
      "19  38eef2c8994e8a980bb1    0.0              0.0     0.0          0.0   \n",
      "20  160a621ad6ee20736755    0.0              0.0     0.0          0.0   \n",
      "\n",
      "    disrespect  targeted_hate  \n",
      "0          0.0            0.0  \n",
      "1          0.0            0.0  \n",
      "2          0.0            0.0  \n",
      "3          0.0            0.0  \n",
      "4          0.0            0.0  \n",
      "5          0.0            0.0  \n",
      "6          0.0            0.0  \n",
      "7          0.0            0.0  \n",
      "8          0.0            0.0  \n",
      "9          0.0            0.0  \n",
      "10         0.0            0.0  \n",
      "11         0.0            0.0  \n",
      "12         0.0            0.0  \n",
      "13         0.0            0.0  \n",
      "14         0.0            0.0  \n",
      "15         0.0            0.0  \n",
      "16         0.0            0.0  \n",
      "17         0.0            0.0  \n",
      "18         0.0            0.0  \n",
      "19         0.0            0.0  \n",
      "20         0.0            0.0  \n"
     ]
    }
   ],
   "source": [
    "train_data = X_train_PV_DBOW\n",
    "test_data = X_test_PV_DBOW\n",
    "clf = StochasticGDClassifier()\n",
    "csv_filename = \"SGDC_PV_DBOW_Prob.csv\"\n",
    "model_name = \"SGD Classifier\"\n",
    "data_matrix_name = \"Paragraph Vector - Distributed Bag Of Words\"\n",
    "execute(clf, train_data, test_data, model_name, data_matrix_name, csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted Model 0\n",
      "Fitted Model 1\n",
      "Fitted Model 2\n",
      "Fitted Model 3\n",
      "Fitted Model 4\n",
      "Fitted Model 5\n",
      "Final class probabilities for first 20 samples of training data:\n",
      "    harsh  extremely_harsh  vulgar  threatening  disrespect  targeted_hate\n",
      "0     0.0              0.0     0.0          0.0         0.0            0.0\n",
      "1     0.0              0.0     0.0          0.0         0.0            0.0\n",
      "2     0.0              0.0     0.0          0.0         0.0            0.0\n",
      "3     0.0              0.0     0.0          0.0         0.0            0.0\n",
      "4     0.0              0.0     0.0          0.0         0.0            0.0\n",
      "5     0.0              0.0     0.0          0.0         0.0            0.0\n",
      "6     0.0              0.0     0.0          0.0         0.0            0.0\n",
      "7     0.0              0.0     0.0          0.0         0.0            0.0\n",
      "8     0.0              0.0     0.0          0.0         0.0            0.0\n",
      "9     0.0              0.0     0.0          0.0         0.0            0.0\n",
      "10    0.0              0.0     0.0          0.0         0.0            0.0\n",
      "11    0.0              0.0     0.0          0.0         0.0            0.0\n",
      "12    0.0              0.0     0.0          0.0         0.0            0.0\n",
      "13    0.0              0.0     0.0          0.0         0.0            0.0\n",
      "14    0.0              0.0     0.0          0.0         0.0            0.0\n",
      "15    0.0              0.0     0.0          0.0         0.0            0.0\n",
      "16    0.0              0.0     0.0          0.0         0.0            0.0\n",
      "17    1.0              0.0     0.0          0.0         0.0            0.0\n",
      "18    0.0              0.0     0.0          0.0         0.0            0.0\n",
      "19    0.0              0.0     0.0          0.0         0.0            0.0\n",
      "20    0.0              0.0     0.0          0.0         0.0            0.0\n",
      "Final class probabilities for first 20 samples of test data:\n",
      "                      id  harsh  extremely_harsh  vulgar  threatening  \\\n",
      "0   e0ae9d9474a5689a5791    0.0              0.0     0.0          0.0   \n",
      "1   b64a191301cad4f11287    1.0              0.0     0.0          0.0   \n",
      "2   5e1953d9ae04bdc66408    0.0              0.0     0.0          0.0   \n",
      "3   23128f98196c8e8f7b90    0.0              0.0     0.0          0.0   \n",
      "4   2d3f1254f71472bf2b78    0.0              0.0     0.0          0.0   \n",
      "5   21f4f0f4812a08ea6c28    0.0              0.0     0.0          0.0   \n",
      "6   733b43d534c67c1be948    0.0              0.0     0.0          0.0   \n",
      "7   aad47a397f7ddc629d5d    0.0              0.0     0.0          0.0   \n",
      "8   d19fcde8a3af2e472d74    0.0              0.0     0.0          0.0   \n",
      "9   7d4de482c60f1c8a79c6    0.0              1.0     0.0          0.0   \n",
      "10  f81afe094bcd161ea6f8    0.0              0.0     0.0          0.0   \n",
      "11  132973e40fa63e0d07f1    0.0              0.0     0.0          0.0   \n",
      "12  37195759bf9104c3b488    0.0              0.0     0.0          0.0   \n",
      "13  0c46ec0e711469190af8    0.0              0.0     0.0          0.0   \n",
      "14  ecaeb00c751a61c1370e    1.0              0.0     0.0          0.0   \n",
      "15  dfaa434123c910477026    0.0              0.0     0.0          0.0   \n",
      "16  f0e8517a1d9e1394696b    0.0              0.0     0.0          0.0   \n",
      "17  db952db7fb786093a8f7    0.0              0.0     0.0          0.0   \n",
      "18  132ad2e7621d9c132c3f    0.0              0.0     0.0          0.0   \n",
      "19  38eef2c8994e8a980bb1    0.0              0.0     0.0          0.0   \n",
      "20  160a621ad6ee20736755    0.0              0.0     0.0          0.0   \n",
      "\n",
      "    disrespect  targeted_hate  \n",
      "0          0.0            0.0  \n",
      "1          0.0            0.0  \n",
      "2          1.0            0.0  \n",
      "3          0.0            0.0  \n",
      "4          0.0            0.0  \n",
      "5          0.0            0.0  \n",
      "6          0.0            0.0  \n",
      "7          0.0            0.0  \n",
      "8          0.0            0.0  \n",
      "9          0.0            1.0  \n",
      "10         0.0            0.0  \n",
      "11         0.0            0.0  \n",
      "12         0.0            0.0  \n",
      "13         0.0            0.0  \n",
      "14         1.0            0.0  \n",
      "15         0.0            0.0  \n",
      "16         0.0            0.0  \n",
      "17         0.0            0.0  \n",
      "18         0.0            0.0  \n",
      "19         0.0            0.0  \n",
      "20         0.0            0.0  \n"
     ]
    }
   ],
   "source": [
    "train_data = X_train_W2V_method_1\n",
    "test_data = X_test_W2V_method_1\n",
    "clf = StochasticGDClassifier()\n",
    "csv_filename = \"SGDC_W2V_method_1_Prob.csv\"\n",
    "model_name = \"SGD Classifier\"\n",
    "data_matrix_name = \"Word2Vec Method_1\"\n",
    "execute(clf, train_data, test_data, model_name, data_matrix_name, csv_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.10 Gradient Boosting Classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "class GBClassifier(object): #Pack of 6 binary Gradient Boosting classifiers\n",
    "    def __init__(self):\n",
    "        self.param_grid = {\n",
    "            'loss' : ['log_loss'] #default='log_loss'\n",
    "        }\n",
    "        self.models = []\n",
    "        \n",
    "        for i in range(6):\n",
    "            self.models.append(GradientBoostingClassifier())\n",
    "    \n",
    "    def fit(self, XTrain, yTrain, CVScoreType, numFolds): #Fits the models and returns the CVScores and CVParams after performing cross validation\n",
    "        CVScores = {}\n",
    "        CVParams = {}\n",
    "        \n",
    "        for i, model in enumerate(self.models):\n",
    "            labels = yTrain[label_type[i]]\n",
    "            \n",
    "            bestParams, bestScore = performGridSearchCV(model, self.param_grid, XTrain, labels, numFolds, CVScoreType)\n",
    "            CVParams[label_type[i]] = bestParams\n",
    "            CVScores[label_type[i]] = bestScore\n",
    "            \n",
    "            self.models[i] = GradientBoostingClassifier(loss=bestParams['loss'])\n",
    "            self.models[i].fit(XTrain, labels)\n",
    "            print(\"Fitted Model \"+str(i))\n",
    "            \n",
    "        return CVScores, CVParams\n",
    "    \n",
    "    def fitWithoutGridSearch(self, XTrain, yTrain):\n",
    "        for i, model in enumerate(self.models):\n",
    "            labels = yTrain[label_type[i]]\n",
    "            self.models[i].fit(XTrain, labels) \n",
    "            print(\"Fitted Model \"+str(i))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = {}\n",
    "        for i, model in enumerate(self.models):\n",
    "            predictions[label_type[i]] = model.predict(X)\n",
    "        return predictions\n",
    "    \n",
    "    def predictProbabilities(self, X):\n",
    "        probabilities = {}\n",
    "        for i, model in enumerate(self.models):\n",
    "            classes = model.classes_\n",
    "            index = np.where(classes == 1)[0][0] #Getting the index of class 1\n",
    "            probabilities[label_type[i]] = model.predict_proba(X)[:, index] #Extracting the probabilities of class 1\n",
    "        return probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now testing out the Gradient Boosting classifier on various data matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = X_train_tfidf\n",
    "test_data = X_test_tfidf\n",
    "clf = GBClassifier()\n",
    "csv_filename = \"GBC_tfidf_Prob.csv\"\n",
    "model_name = \"GB Classifier\"\n",
    "data_matrix_name = \"TF-IDF\"\n",
    "# execute(clf, train_data, test_data, model_name, data_matrix_name, csv_filename, \"WGS\")\n",
    "execute(clf, train_data, test_data, model_name, data_matrix_name, csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = X_train_bow\n",
    "test_data = X_test_bow\n",
    "clf = GBClassifier()\n",
    "csv_filename = \"GBC_bow_Prob.csv\"\n",
    "model_name = \"GB Classifier\"\n",
    "data_matrix_name = \"Bag of Words\"\n",
    "execute(clf, train_data, test_data, model_name, data_matrix_name, csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = X_train_PV_DM\n",
    "test_data = X_test_PV_DM\n",
    "clf = GBClassifier()\n",
    "csv_filename = \"GBC_PV_DM_Prob.csv\"\n",
    "model_name = \"GB Classifier\"\n",
    "data_matrix_name = \"Paragraph Vector - Distributed Memory\"\n",
    "execute(clf, train_data, test_data, model_name, data_matrix_name, csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = X_train_PV_DBOW\n",
    "test_data = X_test_PV_DBOW\n",
    "clf = GBClassifier()\n",
    "csv_filename = \"GBC_PV_DBOW_Prob.csv\"\n",
    "model_name = \"GB Classifier\"\n",
    "data_matrix_name = \"Paragraph Vector - Distributed Bag Of Words\"\n",
    "execute(clf, train_data, test_data, model_name, data_matrix_name, csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = X_train_W2V_method_1\n",
    "test_data = X_test_W2V_method_1\n",
    "clf = GBClassifier()\n",
    "csv_filename = \"GBC_W2V_method_1_Prob.csv\"\n",
    "model_name = \"GB Classifier\"\n",
    "data_matrix_name = \"Word2Vec Method_1\"\n",
    "execute(clf, train_data, test_data, model_name, data_matrix_name, csv_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.11 LGBM Classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm.sklearn import LGBMClassifier\n",
    "class LightGBMClassifier(object): #Pack of 6 LGBM classifiers\n",
    "    def __init__(self):\n",
    "        self.param_grid = {\n",
    "            'n_estimators' : [100, 200] #default=100\n",
    "        }\n",
    "        self.models = []\n",
    "        \n",
    "        for i in range(6):\n",
    "            self.models.append(LGBMClassifier(n_estimators = 100))\n",
    "    \n",
    "    def fit(self, XTrain, yTrain, CVScoreType, numFolds): #Fits the models and returns the CVScores and CVParams after performing cross validation\n",
    "        CVScores = {}\n",
    "        CVParams = {}\n",
    "        \n",
    "        for i, model in enumerate(self.models):\n",
    "            labels = yTrain[label_type[i]]\n",
    "            \n",
    "            bestParams, bestScore = performGridSearchCV(model, self.param_grid, XTrain, labels, numFolds, CVScoreType)\n",
    "            CVParams[label_type[i]] = bestParams\n",
    "            CVScores[label_type[i]] = bestScore\n",
    "            \n",
    "            self.models[i] = LGBMClassifier(n_estimators=bestParams['n_estimators'])\n",
    "            self.models[i].fit(XTrain, labels)\n",
    "            print(\"Fitted Model \"+str(i))\n",
    "            \n",
    "        return CVScores, CVParams\n",
    "    \n",
    "    def fitWithoutGridSearch(self, XTrain, yTrain):\n",
    "        for i, model in enumerate(self.models):\n",
    "            labels = yTrain[label_type[i]]\n",
    "            self.models[i].fit(XTrain, labels) \n",
    "            print(\"Fitted Model \"+str(i))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = {}\n",
    "        for i, model in enumerate(self.models):\n",
    "            predictions[label_type[i]] = model.predict(X)\n",
    "        return predictions\n",
    "    \n",
    "    def predictProbabilities(self, X):\n",
    "        probabilities = {}\n",
    "        for i, model in enumerate(self.models):\n",
    "            classes = model.classes_\n",
    "            index = np.where(classes == 1)[0][0] #Getting the index of class 1\n",
    "            probabilities[label_type[i]] = model.predict_proba(X)[:, index] #Extracting the probabilities of class 1\n",
    "        return probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now testing out the LightGBM classifier on various data matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = X_train_tfidf\n",
    "test_data = X_test_tfidf\n",
    "clf = LightGBMClassifier()\n",
    "csv_filename = \"LGBMC_tfidf_Prob.csv\"\n",
    "model_name = \"LightGBM Classifier\"\n",
    "data_matrix_name = \"TF-IDF\"\n",
    "# execute(clf, train_data, test_data, model_name, data_matrix_name, csv_filename, \"WGS\")\n",
    "execute(clf, train_data, test_data, model_name, data_matrix_name, csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = X_train_bow\n",
    "test_data = X_test_bow\n",
    "clf = LightGBMClassifier()\n",
    "csv_filename = \"LGBMC_bow_Prob.csv\"\n",
    "model_name = \"LightGBM Classifier\"\n",
    "data_matrix_name = \"Bag of Words\"\n",
    "execute(clf, train_data, test_data, model_name, data_matrix_name, csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = X_train_PV_DM\n",
    "test_data = X_test_PV_DM\n",
    "clf = LightGBMClassifier()\n",
    "csv_filename = \"LGBMC_PV_DM_Prob.csv\"\n",
    "model_name = \"LightGBM Classifier\"\n",
    "data_matrix_name = \"Paragraph Vector - Distributed Memory\"\n",
    "execute(clf, train_data, test_data, model_name, data_matrix_name, csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = X_train_PV_DBOW\n",
    "test_data = X_test_PV_DBOW\n",
    "clf = LightGBMClassifier()\n",
    "csv_filename = \"LGBMC_PV_DBOW_Prob.csv\"\n",
    "model_name = \"LightGBM Classifier\"\n",
    "data_matrix_name = \"Paragraph Vector - Distributed Bag Of Words\"\n",
    "execute(clf, train_data, test_data, model_name, data_matrix_name, csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = X_train_W2V_method_1\n",
    "test_data = X_test_W2V_method_1\n",
    "clf = LightGBMClassifier()\n",
    "csv_filename = \"LGBMC_W2V_method_1_Prob.csv\"\n",
    "model_name = \"LightGBM Classifier\"\n",
    "data_matrix_name = \"Word2Vec Method_1\"\n",
    "execute(clf, train_data, test_data, model_name, data_matrix_name, csv_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.12 XGBoost Classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost.sklearn import XGBClassifier\n",
    "class XGBoostClassifier(object): #Pack of 6 binary xgboost classifiers\n",
    "    def __init__(self):\n",
    "        self.param_grid = {\n",
    "            'n_estimators' : [100, 200]\n",
    "        }\n",
    "        self.models = []\n",
    "        \n",
    "        for i in range(6):\n",
    "            self.models.append(XGBClassifier(n_estimators = 200))\n",
    "    \n",
    "    def fit(self, XTrain, yTrain, CVScoreType, numFolds): #Fits the models and returns the CVScores and CVParams after performing cross validation\n",
    "        CVScores = {}\n",
    "        CVParams = {}\n",
    "        \n",
    "        for i, model in enumerate(self.models):\n",
    "            labels = yTrain[label_type[i]]\n",
    "            \n",
    "            bestParams, bestScore = performGridSearchCV(model, self.param_grid, XTrain, labels, numFolds, CVScoreType)\n",
    "            CVParams[label_type[i]] = bestParams\n",
    "            CVScores[label_type[i]] = bestScore\n",
    "            \n",
    "            self.models[i] = XGBClassifier(n_estimators=bestParams['n_estimators'])\n",
    "            self.models[i].fit(XTrain, labels)\n",
    "            print(\"Fitted Model \"+str(i))\n",
    "            \n",
    "        return CVScores, CVParams\n",
    "    \n",
    "    def fitWithoutGridSearch(self, XTrain, yTrain):\n",
    "        for i, model in enumerate(self.models):\n",
    "            labels = yTrain[label_type[i]]\n",
    "            self.models[i].fit(XTrain, labels) \n",
    "            print(\"Fitted Model \"+str(i))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = {}\n",
    "        for i, model in enumerate(self.models):\n",
    "            predictions[label_type[i]] = model.predict(X)\n",
    "        return predictions\n",
    "    \n",
    "    def predictProbabilities(self, X):\n",
    "        probabilities = {}\n",
    "        for i, model in enumerate(self.models):\n",
    "            classes = model.classes_\n",
    "            index = np.where(classes == 1)[0][0] #Getting the index of class 1\n",
    "            probabilities[label_type[i]] = model.predict_proba(X)[:, index] #Extracting the probabilities of class 1\n",
    "        return probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now testing out the xgboost classifier on various data matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted Model 0\n",
      "Fitted Model 1\n",
      "Fitted Model 2\n",
      "Fitted Model 3\n",
      "Fitted Model 4\n",
      "Fitted Model 5\n",
      "Final class probabilities for first 20 samples of training data:\n",
      "       harsh  extremely_harsh    vulgar   threatening  disrespect  \\\n",
      "0   0.000282     1.070759e-05  0.000214  8.365677e-07    0.000078   \n",
      "1   0.000232     7.752950e-06  0.000074  4.040901e-06    0.000088   \n",
      "2   0.053848     1.300273e-03  0.011630  3.143892e-05    0.018620   \n",
      "3   0.997861     8.039668e-04  0.996234  9.842815e-01    0.799310   \n",
      "4   0.002502     1.342239e-05  0.000374  2.235838e-05    0.000400   \n",
      "5   0.004766     2.124028e-05  0.000656  2.667590e-06    0.002541   \n",
      "6   0.011963     2.366339e-04  0.006231  1.050652e-04    0.004850   \n",
      "7   0.031890     4.986063e-03  0.009718  6.764389e-05    0.006038   \n",
      "8   0.000707     8.723255e-08  0.000051  4.327815e-08    0.000142   \n",
      "9   0.039160     1.694841e-04  0.002270  6.181579e-05    0.008213   \n",
      "10  0.000928     8.315992e-05  0.000070  3.704565e-05    0.000297   \n",
      "11  0.008202     9.216019e-05  0.009162  1.632872e-06    0.003831   \n",
      "12  0.005131     9.822165e-05  0.001008  7.510549e-06    0.001912   \n",
      "13  0.004436     4.768765e-05  0.001736  2.674866e-06    0.003755   \n",
      "14  0.495317     4.128319e-04  0.023990  2.547056e-03    0.038049   \n",
      "15  0.008768     1.347817e-04  0.001532  1.620789e-07    0.004441   \n",
      "16  0.070537     4.925721e-04  0.012666  5.449196e-05    0.012389   \n",
      "17  0.034319     4.796124e-04  0.008339  1.615349e-04    0.009380   \n",
      "18  0.009561     9.116671e-05  0.001292  5.480177e-05    0.000759   \n",
      "19  0.000882     3.539288e-06  0.000398  1.814671e-06    0.001193   \n",
      "20  0.000753     3.836770e-06  0.000381  2.261356e-07    0.000323   \n",
      "\n",
      "    targeted_hate  \n",
      "0        0.000002  \n",
      "1        0.000105  \n",
      "2        0.001872  \n",
      "3        0.002264  \n",
      "4        0.000037  \n",
      "5        0.000186  \n",
      "6        0.000645  \n",
      "7        0.000361  \n",
      "8        0.000002  \n",
      "9        0.000374  \n",
      "10       0.000007  \n",
      "11       0.000026  \n",
      "12       0.000137  \n",
      "13       0.000173  \n",
      "14       0.011740  \n",
      "15       0.000246  \n",
      "16       0.001558  \n",
      "17       0.000289  \n",
      "18       0.000132  \n",
      "19       0.000015  \n",
      "20       0.000001  \n",
      "Final class probabilities for first 20 samples of test data:\n",
      "                      id     harsh  extremely_harsh    vulgar   threatening  \\\n",
      "0   e0ae9d9474a5689a5791  0.032730     1.098771e-02  0.011500  1.302380e-03   \n",
      "1   b64a191301cad4f11287  0.187633     5.443162e-05  0.003298  1.410231e-06   \n",
      "2   5e1953d9ae04bdc66408  0.289283     1.342462e-03  0.080826  9.849594e-04   \n",
      "3   23128f98196c8e8f7b90  0.008798     2.143280e-05  0.002454  1.866338e-06   \n",
      "4   2d3f1254f71472bf2b78  0.020686     9.890551e-05  0.004667  6.097494e-06   \n",
      "5   21f4f0f4812a08ea6c28  0.199035     4.135803e-05  0.001971  2.973753e-07   \n",
      "6   733b43d534c67c1be948  0.008998     5.435241e-05  0.000809  7.851381e-07   \n",
      "7   aad47a397f7ddc629d5d  0.010450     1.206738e-04  0.002578  7.206779e-05   \n",
      "8   d19fcde8a3af2e472d74  0.017733     1.521807e-04  0.003348  6.754458e-06   \n",
      "9   7d4de482c60f1c8a79c6  0.199194     2.477636e-02  0.067574  5.534574e-03   \n",
      "10  f81afe094bcd161ea6f8  0.001777     6.026527e-07  0.001557  8.178264e-07   \n",
      "11  132973e40fa63e0d07f1  0.001781     1.534843e-06  0.000653  3.706154e-07   \n",
      "12  37195759bf9104c3b488  0.007077     4.086226e-06  0.001171  1.722308e-05   \n",
      "13  0c46ec0e711469190af8  0.045461     8.923162e-04  0.007674  1.110192e-05   \n",
      "14  ecaeb00c751a61c1370e  0.014302     1.080344e-04  0.006699  8.586233e-05   \n",
      "15  dfaa434123c910477026  0.021828     1.279670e-05  0.003364  7.465896e-05   \n",
      "16  f0e8517a1d9e1394696b  0.004818     6.013023e-06  0.001547  2.032043e-07   \n",
      "17  db952db7fb786093a8f7  0.017272     2.454111e-04  0.001247  6.049515e-05   \n",
      "18  132ad2e7621d9c132c3f  0.002366     6.119499e-07  0.000312  1.489321e-07   \n",
      "19  38eef2c8994e8a980bb1  0.045656     3.264817e-05  0.004868  5.207556e-06   \n",
      "20  160a621ad6ee20736755  0.002671     7.801351e-07  0.000349  1.248316e-05   \n",
      "\n",
      "    disrespect  targeted_hate  \n",
      "0     0.014680       0.001888  \n",
      "1     0.003898       0.000184  \n",
      "2     0.115028       0.002000  \n",
      "3     0.000966       0.000372  \n",
      "4     0.003895       0.000041  \n",
      "5     0.012881       0.000021  \n",
      "6     0.001533       0.000061  \n",
      "7     0.001271       0.000166  \n",
      "8     0.001261       0.000043  \n",
      "9     0.106185       0.007720  \n",
      "10    0.002717       0.000064  \n",
      "11    0.000841       0.000032  \n",
      "12    0.000488       0.000030  \n",
      "13    0.013361       0.001604  \n",
      "14    0.035130       0.000949  \n",
      "15    0.001190       0.000075  \n",
      "16    0.001464       0.000053  \n",
      "17    0.002540       0.000289  \n",
      "18    0.000469       0.000011  \n",
      "19    0.006944       0.000718  \n",
      "20    0.000524       0.000198  \n"
     ]
    }
   ],
   "source": [
    "train_data = X_train_tfidf\n",
    "test_data = X_test_tfidf\n",
    "clf = XGBoostClassifier()\n",
    "csv_filename = \"XGBC_tfidf_Prob.csv\"\n",
    "model_name = \"XGBoost Classifier\"\n",
    "data_matrix_name = \"TF-IDF\"\n",
    "# execute(clf, train_data, test_data, model_name, data_matrix_name, csv_filename, \"WGS\")\n",
    "execute(clf, train_data, test_data, model_name, data_matrix_name, csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted Model 0\n",
      "Fitted Model 1\n",
      "Fitted Model 2\n",
      "Fitted Model 3\n",
      "Fitted Model 4\n",
      "Fitted Model 5\n",
      "Final class probabilities for first 20 samples of training data:\n",
      "       harsh  extremely_harsh    vulgar   threatening  disrespect  \\\n",
      "0   0.000048     6.407876e-06  0.000274  6.122703e-07    0.000030   \n",
      "1   0.000030     1.441784e-06  0.000065  1.319673e-05    0.000032   \n",
      "2   0.055309     2.212258e-03  0.013130  1.840673e-04    0.017298   \n",
      "3   0.998773     1.055603e-02  0.974737  9.933352e-01    0.974876   \n",
      "4   0.008283     2.321046e-05  0.003062  1.907640e-05    0.001408   \n",
      "5   0.003081     1.753062e-04  0.002157  1.297463e-05    0.004178   \n",
      "6   0.011574     6.722349e-04  0.008621  2.123451e-04    0.002807   \n",
      "7   0.092265     8.587264e-03  0.013904  4.260239e-05    0.013896   \n",
      "8   0.000259     3.274173e-08  0.000086  6.650868e-07    0.000142   \n",
      "9   0.029691     1.560676e-04  0.001803  1.059955e-04    0.008684   \n",
      "10  0.001820     8.284796e-06  0.000073  3.017419e-04    0.000134   \n",
      "11  0.022629     3.044498e-04  0.011459  1.531351e-05    0.006295   \n",
      "12  0.004271     2.283967e-05  0.000761  4.766385e-06    0.000647   \n",
      "13  0.002220     1.195425e-04  0.001618  7.011426e-05    0.000971   \n",
      "14  0.352134     1.505649e-03  0.026977  7.150048e-04    0.056356   \n",
      "15  0.006580     5.926865e-05  0.000968  8.593060e-08    0.001582   \n",
      "16  0.037420     5.332535e-04  0.007963  4.815983e-05    0.008863   \n",
      "17  0.057031     7.971712e-04  0.003370  1.136977e-03    0.014501   \n",
      "18  0.012789     1.617672e-04  0.002979  3.327219e-04    0.003917   \n",
      "19  0.001150     2.258185e-05  0.000270  4.857747e-06    0.001809   \n",
      "20  0.000656     7.506992e-06  0.000292  1.366591e-07    0.000189   \n",
      "\n",
      "    targeted_hate  \n",
      "0    1.127115e-06  \n",
      "1    8.955064e-06  \n",
      "2    2.145410e-03  \n",
      "3    7.905088e-03  \n",
      "4    1.583377e-05  \n",
      "5    3.000337e-04  \n",
      "6    1.317804e-03  \n",
      "7    5.310529e-04  \n",
      "8    1.156506e-05  \n",
      "9    9.213423e-04  \n",
      "10   3.406490e-06  \n",
      "11   2.571729e-04  \n",
      "12   1.272586e-05  \n",
      "13   1.314528e-04  \n",
      "14   7.118162e-03  \n",
      "15   1.493481e-04  \n",
      "16   1.460764e-03  \n",
      "17   1.408791e-03  \n",
      "18   3.100841e-04  \n",
      "19   1.652291e-04  \n",
      "20   6.282843e-07  \n",
      "Final class probabilities for first 20 samples of test data:\n",
      "                      id     harsh  extremely_harsh    vulgar   threatening  \\\n",
      "0   e0ae9d9474a5689a5791  0.047956     3.924641e-03  0.015632  2.679551e-05   \n",
      "1   b64a191301cad4f11287  0.188100     2.599012e-05  0.004928  1.274972e-06   \n",
      "2   5e1953d9ae04bdc66408  0.285317     6.859471e-04  0.043165  7.898717e-04   \n",
      "3   23128f98196c8e8f7b90  0.002973     6.006301e-06  0.001876  2.098663e-07   \n",
      "4   2d3f1254f71472bf2b78  0.029020     2.577406e-04  0.007896  2.463484e-05   \n",
      "5   21f4f0f4812a08ea6c28  0.030944     6.178132e-07  0.006283  5.723208e-08   \n",
      "6   733b43d534c67c1be948  0.004411     3.121938e-06  0.002223  9.452444e-07   \n",
      "7   aad47a397f7ddc629d5d  0.015034     8.434615e-05  0.006512  2.069235e-05   \n",
      "8   d19fcde8a3af2e472d74  0.030370     7.700888e-05  0.005065  7.790875e-07   \n",
      "9   7d4de482c60f1c8a79c6  0.152005     9.554114e-03  0.036755  4.680147e-03   \n",
      "10  f81afe094bcd161ea6f8  0.003207     1.328800e-06  0.001125  1.164980e-05   \n",
      "11  132973e40fa63e0d07f1  0.003497     4.993137e-07  0.000438  9.778473e-08   \n",
      "12  37195759bf9104c3b488  0.001560     2.912171e-06  0.000346  5.060460e-05   \n",
      "13  0c46ec0e711469190af8  0.030206     3.861921e-04  0.006985  5.155148e-05   \n",
      "14  ecaeb00c751a61c1370e  0.014447     1.681282e-04  0.004624  1.022529e-04   \n",
      "15  dfaa434123c910477026  0.021498     4.237528e-04  0.009445  1.386367e-04   \n",
      "16  f0e8517a1d9e1394696b  0.005925     5.124680e-05  0.003480  5.058648e-08   \n",
      "17  db952db7fb786093a8f7  0.020702     1.204412e-03  0.008193  8.555095e-05   \n",
      "18  132ad2e7621d9c132c3f  0.000642     5.136678e-07  0.000524  6.636295e-07   \n",
      "19  38eef2c8994e8a980bb1  0.074812     1.092667e-03  0.014423  4.269597e-05   \n",
      "20  160a621ad6ee20736755  0.003454     5.573601e-06  0.000456  7.382404e-07   \n",
      "\n",
      "    disrespect  targeted_hate  \n",
      "0     0.023781       0.001614  \n",
      "1     0.076555       0.002556  \n",
      "2     0.083761       0.001064  \n",
      "3     0.000410       0.000012  \n",
      "4     0.005832       0.000150  \n",
      "5     0.000997       0.000003  \n",
      "6     0.001599       0.000028  \n",
      "7     0.002717       0.000296  \n",
      "8     0.002538       0.000230  \n",
      "9     0.067601       0.005102  \n",
      "10    0.010064       0.001380  \n",
      "11    0.000278       0.000024  \n",
      "12    0.000909       0.000006  \n",
      "13    0.009925       0.000685  \n",
      "14    0.008611       0.000701  \n",
      "15    0.006192       0.000174  \n",
      "16    0.000677       0.000075  \n",
      "17    0.004046       0.000963  \n",
      "18    0.001251       0.000005  \n",
      "19    0.019494       0.000947  \n",
      "20    0.001812       0.000587  \n"
     ]
    }
   ],
   "source": [
    "train_data = X_train_bow\n",
    "test_data = X_test_bow\n",
    "clf = XGBoostClassifier()\n",
    "csv_filename = \"XGBC_bow_Prob.csv\"\n",
    "model_name = \"XGBoost Classifier\"\n",
    "data_matrix_name = \"Bag of Words\"\n",
    "execute(clf, train_data, test_data, model_name, data_matrix_name, csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = X_train_PV_DBOW\n",
    "test_data = X_test_PV_DBOW\n",
    "clf = XGBoostClassifier()\n",
    "csv_filename = \"XGBC_PV_DBOW_Prob.csv\"\n",
    "model_name = \"XGBoost Classifier\"\n",
    "data_matrix_name = \"Paragraph Vector - Distributed Bag of Words\"\n",
    "execute(clf, train_data, test_data, model_name, data_matrix_name, csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = X_train_PV_DM\n",
    "test_data = X_test_PV_DM\n",
    "clf = XGBoostClassifier()\n",
    "csv_filename = \"XGBC_PV_DM_Prob.csv\"\n",
    "model_name = \"XGBoost Classifier\"\n",
    "data_matrix_name = \"Paragraph Vector - Distributed Memory\"\n",
    "execute(clf, train_data, test_data, model_name, data_matrix_name, csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted Model 0\n",
      "Fitted Model 1\n",
      "Fitted Model 2\n",
      "Fitted Model 3\n",
      "Fitted Model 4\n",
      "Fitted Model 5\n",
      "Final class probabilities for first 20 samples of training data:\n",
      "       harsh  extremely_harsh    vulgar   threatening    disrespect  \\\n",
      "0   0.000025     8.518708e-07  0.000043  4.396744e-07  2.153356e-06   \n",
      "1   0.000023     6.088360e-06  0.000010  3.073925e-07  3.345883e-06   \n",
      "2   0.922770     5.003320e-05  0.001595  6.964872e-05  8.665733e-05   \n",
      "3   0.962770     1.495434e-04  0.988023  9.879891e-01  9.754642e-01   \n",
      "4   0.001224     2.849664e-07  0.000028  2.720159e-07  2.097956e-06   \n",
      "5   0.001169     2.118120e-06  0.000016  1.266965e-06  7.089826e-04   \n",
      "6   0.037869     5.575115e-07  0.000476  4.817862e-05  3.178253e-04   \n",
      "7   0.001479     2.758503e-04  0.000214  4.507098e-05  8.246690e-06   \n",
      "8   0.000220     1.878986e-06  0.000053  1.879815e-07  3.129140e-04   \n",
      "9   0.001741     2.367449e-06  0.000012  5.101483e-06  1.679393e-03   \n",
      "10  0.000102     5.604509e-07  0.000051  2.493803e-07  2.426861e-06   \n",
      "11  0.000595     8.946698e-07  0.000051  3.818089e-07  2.156303e-06   \n",
      "12  0.000376     1.668314e-07  0.000022  1.290114e-06  3.192022e-06   \n",
      "13  0.000179     4.310059e-07  0.000012  1.041278e-06  5.345624e-06   \n",
      "14  0.925878     1.309842e-05  0.002051  9.736154e-06  1.361952e-03   \n",
      "15  0.000004     2.814324e-07  0.000011  7.454695e-08  7.399793e-07   \n",
      "16  0.011358     4.417075e-06  0.001444  8.443236e-06  7.885409e-04   \n",
      "17  0.069243     3.540311e-05  0.007693  5.573807e-04  4.241925e-03   \n",
      "18  0.000847     8.798897e-05  0.000009  1.107384e-05  4.325268e-06   \n",
      "19  0.000157     1.554262e-05  0.000170  9.692556e-06  1.880309e-04   \n",
      "20  0.000015     9.340194e-08  0.000012  9.934619e-07  4.293450e-06   \n",
      "\n",
      "    targeted_hate  \n",
      "0    2.264069e-07  \n",
      "1    1.310030e-07  \n",
      "2    1.505073e-05  \n",
      "3    1.953967e-04  \n",
      "4    1.464915e-07  \n",
      "5    5.050072e-06  \n",
      "6    1.437346e-06  \n",
      "7    1.061508e-06  \n",
      "8    4.418431e-06  \n",
      "9    5.652121e-05  \n",
      "10   1.663132e-06  \n",
      "11   1.791367e-06  \n",
      "12   2.125957e-07  \n",
      "13   4.679278e-08  \n",
      "14   6.474033e-05  \n",
      "15   6.352927e-07  \n",
      "16   1.115572e-06  \n",
      "17   3.172716e-04  \n",
      "18   3.852100e-07  \n",
      "19   7.521155e-07  \n",
      "20   2.798125e-06  \n",
      "Final class probabilities for first 20 samples of test data:\n",
      "                      id     harsh  extremely_harsh    vulgar   threatening  \\\n",
      "0   e0ae9d9474a5689a5791  0.000355     9.391227e-07  0.000023  5.225316e-06   \n",
      "1   b64a191301cad4f11287  0.015879     1.757026e-07  0.000002  2.034030e-05   \n",
      "2   5e1953d9ae04bdc66408  0.351669     1.226842e-05  0.002626  6.235648e-06   \n",
      "3   23128f98196c8e8f7b90  0.000136     3.954804e-08  0.000046  3.618240e-07   \n",
      "4   2d3f1254f71472bf2b78  0.002847     1.171237e-06  0.000064  8.918033e-07   \n",
      "5   21f4f0f4812a08ea6c28  0.001309     5.913312e-06  0.000047  2.491897e-05   \n",
      "6   733b43d534c67c1be948  0.004128     9.705523e-08  0.000566  7.805950e-07   \n",
      "7   aad47a397f7ddc629d5d  0.000427     8.800698e-08  0.000041  3.966340e-07   \n",
      "8   d19fcde8a3af2e472d74  0.004416     5.253739e-07  0.000116  6.927637e-07   \n",
      "9   7d4de482c60f1c8a79c6  0.168529     1.170672e-04  0.000980  3.486227e-05   \n",
      "10  f81afe094bcd161ea6f8  0.006156     8.612493e-07  0.000030  5.093992e-06   \n",
      "11  132973e40fa63e0d07f1  0.002512     2.332668e-07  0.000029  7.870565e-07   \n",
      "12  37195759bf9104c3b488  0.000226     1.601167e-07  0.000031  8.345246e-07   \n",
      "13  0c46ec0e711469190af8  0.012477     9.207617e-07  0.001135  1.573287e-06   \n",
      "14  ecaeb00c751a61c1370e  0.066370     7.131664e-06  0.000533  5.925650e-05   \n",
      "15  dfaa434123c910477026  0.000280     2.610638e-07  0.000078  9.586754e-06   \n",
      "16  f0e8517a1d9e1394696b  0.132888     4.445142e-06  0.001832  2.170783e-06   \n",
      "17  db952db7fb786093a8f7  0.000036     2.516068e-06  0.000004  1.259105e-06   \n",
      "18  132ad2e7621d9c132c3f  0.000045     6.355903e-07  0.000009  1.680970e-07   \n",
      "19  38eef2c8994e8a980bb1  0.005366     1.295340e-05  0.001958  2.745049e-06   \n",
      "20  160a621ad6ee20736755  0.000466     3.412094e-07  0.000041  3.298765e-07   \n",
      "\n",
      "      disrespect  targeted_hate  \n",
      "0   1.526479e-04   1.528121e-06  \n",
      "1   5.800029e-03   1.169852e-05  \n",
      "2   2.090027e-03   1.358735e-06  \n",
      "3   9.066422e-06   1.167481e-05  \n",
      "4   8.820446e-05   9.323157e-07  \n",
      "5   5.780078e-05   1.503305e-05  \n",
      "6   3.750431e-03   6.693602e-07  \n",
      "7   2.589055e-07   1.959388e-07  \n",
      "8   1.087148e-05   1.555741e-06  \n",
      "9   4.679126e-02   1.300731e-03  \n",
      "10  1.639194e-04   1.419107e-05  \n",
      "11  6.221895e-05   1.529941e-05  \n",
      "12  1.561041e-04   2.437494e-06  \n",
      "13  1.856204e-03   6.397904e-06  \n",
      "14  1.148084e-01   4.708169e-03  \n",
      "15  2.533366e-05   2.751150e-07  \n",
      "16  1.950805e-02   3.847907e-05  \n",
      "17  7.755936e-07   1.088271e-06  \n",
      "18  6.725579e-05   6.171260e-07  \n",
      "19  3.462691e-04   9.422653e-07  \n",
      "20  5.239008e-05   3.192256e-06  \n"
     ]
    }
   ],
   "source": [
    "train_data = X_train_W2V_method_1\n",
    "test_data = X_test_W2V_method_1\n",
    "clf = XGBoostClassifier()\n",
    "csv_filename = \"XGBC_W2V_method_1_Prob.csv\"\n",
    "model_name = \"XGBoost Classifier\"\n",
    "data_matrix_name = \"Word2Vec Method_1\"\n",
    "execute(clf, train_data, test_data, model_name, data_matrix_name, csv_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.13 Gaussian Process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "\n",
    "class GProcessClassifier(object): #Pack of 6 binary Gaussian Process classifiers\n",
    "    def __init__(self):\n",
    "        self.param_grid = {\n",
    "            'n_restarts_optimizer' : [0]\n",
    "        }\n",
    "        self.models = []\n",
    "        \n",
    "        for i in range(6):\n",
    "            self.models.append(GaussianProcessClassifier(1.0 * RBF(1.0)))\n",
    "    \n",
    "    def fit(self, XTrain, yTrain, CVScoreType, numFolds): #Fits the models and returns the CVScores and CVParams after performing cross validation\n",
    "        CVScores = {}\n",
    "        CVParams = {}\n",
    "        \n",
    "        for i, model in enumerate(self.models):\n",
    "            labels = yTrain[label_type[i]]\n",
    "            \n",
    "            bestParams, bestScore = performGridSearchCV(model, self.param_grid, XTrain, labels, numFolds, CVScoreType)\n",
    "            CVParams[label_type[i]] = bestParams\n",
    "            CVScores[label_type[i]] = bestScore\n",
    "            \n",
    "            self.models[i] = GaussianProcessClassifier(1.0 * RBF(1.0), n_restarts_optimizer = bestParams['n_restarts_optimizer'])\n",
    "            self.models[i].fit(XTrain, labels)\n",
    "            print(\"Fitted Model \"+str(i))\n",
    "            \n",
    "        return CVScores, CVParams\n",
    "    \n",
    "    def fitWithoutGridSearch(self, XTrain, yTrain, CVScoreType, numFolds):\n",
    "        CVScores = {}\n",
    "        CVParams = {}\n",
    "\n",
    "        for i, model in enumerate(self.models):\n",
    "            labels = yTrain[label_type[i]]\n",
    "\n",
    "            CVParams[label_type[i]] = {}\n",
    "            CVScores[label_type[i]] = 0.0\n",
    "\n",
    "            self.models[i].fit(XTrain, labels)\n",
    "            print(\"Fitted Model \"+str(i))\n",
    "\n",
    "        return CVScores, CVParams\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = {}\n",
    "        for i, model in enumerate(self.models):\n",
    "            predictions[label_type[i]] = model.predict(X)\n",
    "        return predictions\n",
    "    \n",
    "    def predictProbabilities(self, X):\n",
    "        probabilities = {}\n",
    "        for i, model in enumerate(self.models):\n",
    "            classes = model.classes_\n",
    "            index = np.where(classes == 1)[0][0] #Getting the index of class 1\n",
    "            probabilities[label_type[i]] = model.predict_proba(X)[:, index] #Extracting the probabilities of class 1\n",
    "        return probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now trying out the Gaussian Process classifier on various data matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = X_train_tfidf.toarray()\n",
    "test_data = X_test_tfidf.toarray()\n",
    "clf = GProcessClassifier()\n",
    "csv_filename = \"GPC_tfidf_Prob.csv\"\n",
    "model_name = \"Gaussian Process Classifier\"\n",
    "data_matrix_name = \"TF-IDF\"\n",
    "execute(clf, train_data, test_data, model_name, data_matrix_name, csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = X_train_bow.toarray()\n",
    "test_data = X_test_bow.toarray()\n",
    "clf = GProcessClassifier()\n",
    "csv_filename = \"GPC_bow_Prob.csv\"\n",
    "model_name = \"Gaussian Process Classifier\"\n",
    "data_matrix_name = \"Bag of Words\"\n",
    "execute(clf, train_data, test_data, model_name, data_matrix_name, csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = X_train_PV_DM.toarray()\n",
    "test_data = X_test_PV_DM.toarray()\n",
    "clf = GProcessClassifier()\n",
    "csv_filename = \"GPC_PV_DM_Prob.csv\"\n",
    "model_name = \"Gaussian Process Classifier\"\n",
    "data_matrix_name = \"Paragraph Vector - Distributed Memory\"\n",
    "execute(clf, train_data, test_data, model_name, data_matrix_name, csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = X_train_PV_DBOW.toarray()\n",
    "test_data = X_test_PV_DBOW.toarray()\n",
    "clf = GProcessClassifier()\n",
    "csv_filename = \"GPC_PV_DBOW_Prob.csv\"\n",
    "model_name = \"Gaussian Process Classifier\"\n",
    "data_matrix_name = \"Paragraph Vector - Distributed Bag of Words\"\n",
    "execute(clf, train_data, test_data, model_name, data_matrix_name, csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = X_train_W2V_method_1.toarray()\n",
    "test_data = X_test_W2V_method_1.toarray()\n",
    "clf = GProcessClassifier()\n",
    "csv_filename = \"GPC_W2V_method_1_Prob.csv\"\n",
    "model_name = \"Gaussian Process Classifier\"\n",
    "data_matrix_name = \"Word2Vec Method_1\"\n",
    "execute(clf, train_data, test_data, model_name, data_matrix_name, csv_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Selecting best class probabilities from various combinations of model and data matrix type:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far it has been seen that \"Logistic regression classifier with TF-IDF data matrix\" and \"Bagging classifier (with logistic regression as base estimator) with TF-IDF data matrix\" are giving best roc_auc scores on unseen data.\n",
    "\n",
    "We can further improve the results by selecting the best class probabilities from the above mentioned Model-DataMatrix combinations..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = {\n",
    "    'harsh': \"VOTC_tfidf_Prob.csv\",\n",
    "    'extremely_harsh' : \"VOTC_tfidf_Prob.csv\",\n",
    "    'vulgar': \"VOTC_tfidf_Prob.csv\",\n",
    "    'threatening': \"VOTC_tfidf_Prob.csv\",\n",
    "    'disrespect': \"VOTC_tfidf_Prob.csv\",\n",
    "    'targeted_hate' : \"VOTC_tfidf_Prob.csv\"\n",
    "    }\n",
    "\n",
    "submission_df = pd.DataFrame([])\n",
    "submission_df['id'] = test_df['id']\n",
    "for i in range(6):\n",
    "    submission_df[label_type[i]] = pd.read_csv(\"TestClassProbs/\"+csv_file[label_type[i]])[label_type[i]]\n",
    "submission_df.to_csv(\"submission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. What's new in Version 2?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following 7 new models were built:\n",
    "1) AdaBoost Classifier\n",
    "2) K-Nearest Neighbors Classifier\n",
    "3) Stochastic Gradient Descent Classifier\n",
    "4) Gradient Boosting Classifier\n",
    "5) LGBM Classifier\n",
    "6) XGBoost Classifier\n",
    "7) Gaussian Process Classifier\n",
    "\n",
    "The above models were tried out with the different types of data matrices that we had built earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Future Scope:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Currently, we are mainly focussed in the following aspects:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) A Word2Vec Method 2 would be developed based on the technique mentioned in \"Weighted_word_embedding_aggregation.pdf\".\n",
    "\n",
    "2) Further refinements of our models and data matrices would be done and more combinations would be tried out.\n",
    "\n",
    "3) Multi-layer Perceptron and RNNs(specially LSTMs) would be tried out later if allowed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d8ca940d3f560627c9207d96bd179a9b9267790497e4dfec8bf824c14aa499b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
